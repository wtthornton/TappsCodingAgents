# Story 16.3: Add Business Logic Validation to Scoring Tests

<!-- Source: implementation/cursor/EPIC_16_Unit_Test_Weak_Assertions.md -->
<!-- Context: Add comprehensive business logic validation to scoring tests to ensure algorithms produce correct outcomes -->

## Status

**Deferred** - Testing infrastructure improvement, lower priority

## Story

**As a** developer/maintainer,
**I want** scoring tests to validate that the scoring algorithms produce correct relative outcomes,
**so that** I can trust that simple code scores better than complex code, secure code scores higher than insecure code, and the overall score formula is correct.

## Acceptance Criteria

1. Tests verify that simple code scores better (higher overall score) than complex code when all other factors are equal.
2. Tests verify that insecure code scores lower (lower security score and overall score) than secure code when all other factors are equal.
3. Tests verify that maintainable code scores higher (higher maintainability score and overall score) than unmaintainable code when all other factors are equal.
4. Tests validate that the overall score calculation formula is correct using known inputs and expected outputs.
5. Tests validate weighted average calculations for overall score using different weight configurations.
6. All new tests fail when scoring logic is intentionally broken, demonstrating they catch real issues.

## Tasks / Subtasks

- [ ] Task 1: Add complexity comparison tests (AC: 1)
  - [ ] Create test with simple code (low cyclomatic complexity, few branches)
  - [ ] Create test with complex code (high cyclomatic complexity, many branches)
  - [ ] Verify simple code has higher overall score than complex code
  - [ ] Verify complexity metric is lower (better) for simple code
  - [ ] Test with various complexity levels to validate relative scoring

- [ ] Task 2: Add security comparison tests (AC: 2)
  - [ ] Create test with secure code (no known vulnerabilities, safe practices)
  - [ ] Create test with insecure code (known vulnerabilities, unsafe practices)
  - [ ] Verify secure code has higher security score than insecure code
  - [ ] Verify secure code has higher overall score than insecure code
  - [ ] Test with various security issues to validate relative scoring

- [ ] Task 3: Add maintainability comparison tests (AC: 3)
  - [ ] Create test with maintainable code (good structure, clear naming, low duplication)
  - [ ] Create test with unmaintainable code (poor structure, unclear naming, high duplication)
  - [ ] Verify maintainable code has higher maintainability score
  - [ ] Verify maintainable code has higher overall score
  - [ ] Test with various maintainability factors to validate relative scoring

- [ ] Task 4: Add overall score formula validation tests (AC: 4)
  - [ ] Create test with known metric values and expected overall score
  - [ ] Verify overall score calculation matches documented formula
  - [ ] Test with edge cases (all metrics at 0, all at 10, mixed values)
  - [ ] Validate formula handles complexity inversion correctly (lower complexity is better)
  - [ ] Document the exact formula being tested

- [ ] Task 5: Add weighted average calculation tests (AC: 5)
  - [ ] Test with default weight configuration
  - [ ] Test with custom weight configurations
  - [ ] Verify weights sum to approximately 1.0
  - [ ] Verify weighted average calculation is correct
  - [ ] Test that changing weights produces expected changes in overall score

- [ ] Task 6: Update existing scoring tests (AC: 1, 2, 3, 4, 5)
  - [ ] Review existing tests in `test_scoring.py`
  - [ ] Add business logic validation to existing tests where appropriate
  - [ ] Ensure existing tests validate relative correctness, not just ranges
  - [ ] Remove or update permissive assertions that don't validate logic

- [ ] Task 7: Validation testing (AC: 6)
  - [ ] Intentionally break scoring logic in production code
  - [ ] Verify that new tests fail appropriately
  - [ ] Document that tests catch real issues
  - [ ] Ensure tests provide clear failure messages

## Dev Notes

### Existing System Context

- Scoring system exists in `tapps_agents/agents/reviewer/scoring.py`
- Current tests in `test_scoring.py` use permissive assertions (`>= 0`, `<= 10`) that don't validate business logic
- Scoring uses five metrics: complexity, security, maintainability, coverage, performance
- Overall score is calculated using weighted average with configurable weights
- Complexity metric is inverted (lower complexity is better) in overall score calculation

### Integration Approach

- Add new test cases to `test_scoring.py`
- Use existing test fixtures and setup
- Create test code samples with known characteristics (simple vs complex, secure vs insecure, etc.)
- No changes to production code required (unless bugs are discovered)

### Risk Assessment

- **Primary risk**: Tests may reveal that scoring logic doesn't produce expected relative outcomes
- **Mitigation**: 
  - Document expected behavior clearly
  - Fix scoring logic if tests reveal bugs
  - Ensure test code samples are representative and valid
- **Secondary risk**: Test code samples may not accurately represent real-world scenarios
- **Mitigation**: Use realistic code examples, test with multiple scenarios, validate with known good/bad code patterns

### Rollback Plan

- Test changes can be reverted independently
- No impact on production code (unless bugs are fixed)
- Can add tests incrementally

### Testing

- Use `pytest` to run new and modified tests
- Verify tests fail when scoring logic is intentionally broken
- Ensure test execution time doesn't significantly increase
- Run full test suite to ensure no regressions
- Validate that tests provide clear, actionable failure messages

### Test Code Samples Needed

- Simple code: minimal complexity, clear structure
- Complex code: high cyclomatic complexity, nested branches
- Secure code: no vulnerabilities, safe practices (e.g., parameterized queries, input validation)
- Insecure code: known vulnerabilities (e.g., SQL injection, XSS, hardcoded secrets)
- Maintainable code: good structure, clear naming, low duplication, good documentation
- Unmaintainable code: poor structure, unclear naming, high duplication, no documentation

## Change Log

| Date       | Version | Description                | Author      |
| ---------- | ------- | -------------------------- | ----------- |
| 2025-01-13 | 0.1     | Initial draft for Epic 16.3 | bmad-master |

## Dev Agent Record

### Agent Model Used

_TBD_

### Debug Log References

_TBD_

### Completion Notes List

- _TBD_

### File List

**Files to Modify:**
- `tests/unit/test_scoring.py` - Add business logic validation tests

**Files to Reference:**
- `tapps_agents/agents/reviewer/scoring.py` - Scoring implementation
- `tapps_agents/core/config.py` - ScoringWeightsConfig

## QA Results

_TBD_

