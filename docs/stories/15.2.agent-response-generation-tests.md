# Story 15.2: Agent Response Generation Tests

<!-- Source: implementation/cursor/EPIC_15_E2E_Agent_Behavior_Testing.md -->
<!-- Context: Add E2E tests for agent response generation and formatting -->

## Status

Complete

## Story

**As a** QA engineer and developer,
**I want** comprehensive E2E tests that validate agent response generation, formatting, and content appropriateness,
**so that** I can ensure agents generate appropriate, well-formatted responses that contain expected information and are contextually appropriate.

## Acceptance Criteria

1. E2E tests exist in `tests/e2e/agents/test_agent_response_generation.py` that validate response generation for all agent types (planner, implementer, reviewer, tester).
2. Tests validate that agents generate appropriate responses based on input:
   - Responses match the command type and context
   - Responses contain relevant information for the command
   - Responses are contextually appropriate (e.g., planner generates plans, implementer generates code)
3. Tests validate that responses are properly formatted:
   - Response structure matches expected format (dict with status, content, etc.)
   - Response content is well-structured (e.g., JSON, markdown, code blocks)
   - Response formatting is consistent across similar commands
4. Tests validate that responses contain expected information:
   - Required fields are present in response structure
   - Response content includes expected data (e.g., file paths, status codes, results)
   - Response metadata is present (e.g., agent_id, timestamp, correlation_id)
5. Tests validate that responses are contextually appropriate:
   - Responses match the agent's role and capabilities
   - Responses are appropriate for the command context
   - Responses provide actionable information when applicable
6. Response validation utilities exist in `tests/e2e/fixtures/agent_test_helpers.py` for:
   - Validating response structure (required fields, types)
   - Validating response content (expected data, formatting)
   - Validating response context (appropriateness, completeness)
   - Asserting response quality metrics
7. Tests use behavioral mocks that simulate realistic agent response generation (not just "Mock LLM response").
8. Tests validate response generation for various command scenarios:
   - Simple commands (e.g., help, status)
   - Commands with parameters (e.g., review file.py)
   - Commands that require context (e.g., plan with project context)
   - Commands that produce artifacts (e.g., generate code, create plan)
9. All tests pass consistently and are Windows-compatible.

## Tasks / Subtasks

- [x] Task 1: Create response validation utilities (AC: 6)
  - [x] Extend `tests/e2e/fixtures/agent_test_helpers.py`
  - [x] Implement `validate_response_structure(response, required_fields)` helper
  - [x] Implement `validate_response_content(response, expected_data)` helper
  - [x] Implement `validate_response_context(response, command, agent_type)` helper
  - [x] Implement `assert_response_quality(response, metrics)` helper
  - [x] Add helpers for validating formatted content (JSON, markdown, code)

- [x] Task 2: Implement response appropriateness tests (AC: 2)
  - [x] Create `tests/e2e/agents/test_agent_response_generation.py`
  - [x] Test responses match command type and context
  - [x] Test responses contain relevant information
  - [x] Test responses are contextually appropriate for agent type
  - [x] Test for each agent type (planner, implementer, reviewer, tester)

- [x] Task 3: Implement response formatting tests (AC: 3)
  - [x] Test response structure matches expected format
  - [x] Test response content is well-structured (JSON, markdown, code blocks)
  - [x] Test response formatting consistency
  - [x] Test for each agent type

- [x] Task 4: Implement response content validation tests (AC: 4)
  - [x] Test required fields are present
  - [x] Test response content includes expected data
  - [x] Test response metadata is present
  - [x] Test for each agent type

- [x] Task 5: Implement contextual appropriateness tests (AC: 5)
  - [x] Test responses match agent role and capabilities
  - [x] Test responses are appropriate for command context
  - [x] Test responses provide actionable information
  - [x] Test for each agent type

- [x] Task 6: Create behavioral mocks for response generation (AC: 7)
  - [x] Enhance `mock_mal` or create agent-specific mocks
  - [x] Mock realistic response generation behavior
  - [x] Mock responses that match agent type and command context
  - [x] Ensure mocks simulate actual agent response logic

- [x] Task 7: Implement scenario-based response tests (AC: 8)
  - [x] Test simple command responses (help, status)
  - [x] Test parameterized command responses (review file.py)
  - [x] Test context-dependent command responses (plan with project context)
  - [x] Test artifact-generating command responses (generate code, create plan)
  - [x] Test for each agent type

- [x] Task 8: Documentation and test execution (AC: 9)
  - [x] Document response generation test patterns in `tests/e2e/agents/README.md`
  - [x] Ensure all tests are Windows-compatible
  - [x] Verify tests pass consistently
  - [x] Add test markers appropriately (`@pytest.mark.e2e`)

## Dev Notes

### Existing System Context

- Agent base class: `tapps_agents/core/agent_base.py`
  - `BaseAgent.run()` method returns `dict[str, Any]` response
  - Response structure varies by agent type and command
- Agent implementations:
  - `tapps_agents/agents/planner/agent.py` - generates plans, structured planning responses
  - `tapps_agents/agents/implementer/agent.py` - generates code, implementation responses
  - `tapps_agents/agents/reviewer/agent.py` - generates reviews, feedback responses
  - `tapps_agents/agents/tester/agent.py` - generates tests, test results (if exists)
- Existing E2E tests:
  - `tests/e2e/smoke/test_agent_lifecycle.py` - basic lifecycle tests
  - `tests/conftest.py` - `mock_mal` fixture (returns "Mock LLM response")
- E2E foundation (from Epic 8):
  - `tests/e2e/fixtures/e2e_harness.py` - E2E harness utilities
  - `tests/e2e/fixtures/project_templates.py` - project templates
  - `tests/e2e/conftest.py` - E2E fixtures

### Integration Approach

- Build on existing agent response patterns
- Extend `mock_mal` fixture or create agent-specific behavioral mocks
- Use E2E harness utilities from Epic 8 for project setup
- Follow existing E2E test patterns
- Create reusable test helpers for response validation

### Technology Research

- Use `pytest` with `pytest-asyncio` for async agent execution
- Use `unittest.mock` for creating behavioral mocks
- Use `jsonschema` or similar for response structure validation (if needed)
- Use `pytest.mark.parametrize` for testing multiple agent types and scenarios
- Reference `pytest` documentation for test organization patterns

### Risk Assessment

- **Primary risk**: Response validation tests may be too strict or too loose
- **Mitigation**: Use contract-based validation (validate structure and key content, not exact text)
- **Rollback plan**: Response generation tests are additive; can be disabled via markers if needed

### Testing

- Test file location: `tests/e2e/agents/test_agent_response_generation.py`
- Test standards: Use `pytest`, follow existing E2E test patterns
- Testing frameworks: `pytest`, `pytest-asyncio` for async tests
- Specific requirements:
  - Test response generation for all agent types
  - Test response formatting and structure
  - Test response content and context
  - Use behavioral mocks (not generic "Mock LLM response")
  - Test various command scenarios
  - Windows-compatible (use `pathlib.Path`, no shell commands)

## Change Log

| Date       | Version | Description                       | Author      |
| ---------- | ------- | --------------------------------- | ----------- |
| 2025-01-XX | 0.1     | Initial draft for Epic 15.2      | bmad-master |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_

