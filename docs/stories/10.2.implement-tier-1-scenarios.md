# Story 10.2: Implement 2â€“3 Tier-1 Scenarios

<!-- Source: implementation/cursor/EPIC_10_E2E_Scenario_Tests.md -->
<!-- Context: Implement E2E tests for 2-3 tier-1 user journey scenarios (feature implementation, bug fix, refactor) -->

## Status

Draft

## Story

**As a** QA engineer and developer,
**I want** E2E tests for 2-3 tier-1 user journey scenarios (feature implementation, bug fix, refactor) that validate complete end-to-end workflows,
**so that** I have high confidence that multi-agent orchestration, artifact handling, quality gates, and reporting work together in realistic scenarios.

## Acceptance Criteria

1. E2E test exists for Scenario A: "Implement a small feature end-to-end"
   - Test: Start with small scenario template (feature request defined)
   - Test: Execute appropriate workflow (e.g., `full-sdlc.yaml` or `feature-implementation.yaml`)
   - Test: Validate feature is implemented (new files, modified files)
   - Test: Validate artifacts are created (planning, design, code, review artifacts)
   - Test: Validate tests are created and passing
   - Test: Validate quality gates pass
   - Test: Validate workflow completes successfully
2. E2E test exists for Scenario B: "Fix a bug with tests + review gate"
   - Test: Start with small scenario template (bug present, reproducible)
   - Test: Execute appropriate workflow (e.g., `quick-fix.yaml` or `quality.yaml`)
   - Test: Validate bug is fixed (modified files, bug fix logic)
   - Test: Validate bug reproduction test is created
   - Test: Validate fix verification test passes
   - Test: Validate review gate passes
   - Test: Validate workflow completes successfully
3. E2E test exists for Scenario C: "Refactor + quality gate + docs update"
   - Test: Start with medium scenario template (legacy code, refactor needed)
   - Test: Execute appropriate workflow (e.g., `full-sdlc.yaml` or `multi-agent-refactor.yaml`)
   - Test: Validate refactor is completed (refactored files, improved structure)
   - Test: Validate all tests still pass (regression validation)
   - Test: Validate quality gates pass (maintained/improved quality)
   - Test: Validate documentation is updated
   - Test: Validate workflow completes successfully
4. All scenario tests use scenario templates from Story 10.1:
   - Use `create_small_scenario_template()` or `create_medium_scenario_template()`
   - Use scenario-specific starting state
   - Use expected outputs for validation
5. All scenario tests use workflow runner from Epic 9:
   - Use `run_workflow_step_by_step()` for step-level validation
   - Use `capture_workflow_state()` for state snapshots
   - Use `assert_workflow_artifacts()` for artifact validation
   - Use `control_gate_outcome()` for deterministic gate routing (if needed)
6. All scenario tests validate outcome contracts:
   - File changes (existence, minimal content validation)
   - Artifacts (JSON shape, required keys, file existence)
   - Test outcomes (pass/fail counts, exit codes)
   - Quality signals (gate states, quality scores)
   - Report summaries (workflow completion, step summaries)
   - Avoid free-form text assertions
7. All scenario tests run in **mocked mode by default**:
   - No real LLM calls (use `mock_mal` fixture)
   - No real Context7 calls (mocked if needed)
   - Deterministic outcomes (controlled via mocks)
   - Tests produce identical results on repeated runs
8. All scenario tests support **real mode** behind markers:
   - Real mode enabled via `requires_llm` marker
   - Real mode can be enabled via environment variable
   - Real mode tests are clearly marked and documented
   - Real mode tests are designed for scheduled execution (nightly/pre-release)
9. All scenario tests include explicit timeouts and produce actionable failure artifacts:
   - Scenario-level timeouts (per-scenario timeout caps)
   - Step-level timeouts
   - Failure artifacts include: logs, state snapshots, produced artifacts, step timeline
10. All scenario tests use isolated project environments:
    - Use scenario templates for isolated test projects
    - Clean state before each test
    - Cleanup after test completion
11. All scenario tests are marked with `e2e_scenario` marker.
12. Documentation exists for running scenario E2E tests (mocked and real modes).

## Tasks / Subtasks

- [ ] Task 1: Create Scenario A test - Feature Implementation (AC: 1, 4, 5, 6, 7, 8, 9, 10, 11)
  - [ ] Create `tests/e2e/scenarios/test_feature_implementation.py`
  - [ ] Use small scenario template with feature request
  - [ ] Execute appropriate workflow (e.g., `full-sdlc.yaml` or `feature-implementation.yaml`)
  - [ ] Use workflow runner for step-by-step execution
  - [ ] Validate feature implementation:
    - [ ] New feature files exist
    - [ ] Modified files (tests, imports, etc.)
    - [ ] Feature logic is correct (minimal content validation)
  - [ ] Validate artifacts (planning, design, code, review)
  - [ ] Validate tests created and passing
  - [ ] Validate quality gates pass
  - [ ] Validate workflow completion
  - [ ] Use mocked mode by default
  - [ ] Support real mode behind `requires_llm` marker
  - [ ] Mark with `e2e_scenario`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 2: Create Scenario B test - Bug Fix (AC: 2, 4, 5, 6, 7, 8, 9, 10, 11)
  - [ ] Create `tests/e2e/scenarios/test_bug_fix.py`
  - [ ] Use small scenario template with bug present
  - [ ] Execute appropriate workflow (e.g., `quick-fix.yaml` or `quality.yaml`)
  - [ ] Use workflow runner for step-by-step execution
  - [ ] Validate bug fix:
    - [ ] Bug fix files modified
    - [ ] Bug fix logic is correct (minimal content validation)
  - [ ] Validate bug reproduction test created
  - [ ] Validate fix verification test passes
  - [ ] Validate review gate passes
  - [ ] Validate workflow completion
  - [ ] Use mocked mode by default
  - [ ] Support real mode behind `requires_llm` marker
  - [ ] Mark with `e2e_scenario`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 3: Create Scenario C test - Refactor (AC: 3, 4, 5, 6, 7, 8, 9, 10, 11)
  - [ ] Create `tests/e2e/scenarios/test_refactor.py`
  - [ ] Use medium scenario template with refactor needed
  - [ ] Execute appropriate workflow (e.g., `full-sdlc.yaml` or `multi-agent-refactor.yaml`)
  - [ ] Use workflow runner for step-by-step execution
  - [ ] Validate refactor:
    - [ ] Refactored files exist
    - [ ] Improved structure (file organization, code organization)
    - [ ] Refactor logic is correct (minimal content validation)
  - [ ] Validate all tests still pass (regression validation)
  - [ ] Validate quality gates pass (maintained/improved quality)
  - [ ] Validate documentation updated
  - [ ] Validate workflow completion
  - [ ] Use mocked mode by default
  - [ ] Support real mode behind `requires_llm` marker
  - [ ] Mark with `e2e_scenario`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 4: Implement outcome contract validation (AC: 6)
  - [ ] Use scenario validators from Story 10.1
  - [ ] Validate file changes (existence, minimal content)
  - [ ] Validate artifacts (JSON shape, required keys)
  - [ ] Validate test outcomes (pass/fail counts, exit codes)
  - [ ] Validate quality signals (gate states, quality scores)
  - [ ] Validate report summaries (workflow completion, step summaries)
  - [ ] Provide clear validation error messages

- [ ] Task 5: Optimize test execution and reliability (AC: 9)
  - [ ] Review test execution time
  - [ ] Optimize slow tests (reduce fixture complexity, use faster mocks)
  - [ ] Ensure scenario-level and step-level timeouts are appropriate
  - [ ] Validate failure artifact capture works correctly
  - [ ] Test artifact capture in failing scenarios

- [ ] Task 6: Documentation and validation (AC: 12)
  - [ ] Document scenario E2E test coverage in `tests/e2e/scenarios/README.md`
  - [ ] Document how to run scenario E2E tests locally (mocked mode)
  - [ ] Document how to run scenario E2E tests with real services (real mode)
  - [ ] Document outcome contract validation approach
  - [ ] Document scenario-specific requirements and setup
  - [ ] Verify all tests are deterministic in mocked mode (run twice, compare results)

## Dev Notes

### Existing System Context

- Scenario templates (from Story 10.1):
  - `tests/e2e/fixtures/scenario_templates.py` - scenario template creation
  - `tests/e2e/fixtures/scenario_validators.py` - expected output validation
- Workflow runner (from Epic 9):
  - `tests/e2e/fixtures/workflow_runner.py` - workflow runner harness
- Workflow presets:
  - `workflows/presets/full-sdlc.yaml` - full SDLC workflow
  - `workflows/presets/feature-implementation.yaml` - feature implementation workflow
  - `workflows/presets/quick-fix.yaml` - quick fix workflow
  - `workflows/presets/quality.yaml` - quality-focused workflow
  - `workflows/multi-agent-refactor.yaml` - refactor workflow
- E2E foundation (from Epic 8):
  - `tests/e2e/fixtures/e2e_harness.py` - E2E harness utilities
  - `tests/e2e/fixtures/project_templates.py` - base project templates
  - `tests/e2e/conftest.py` - E2E fixtures
- Existing test patterns:
  - `tests/conftest.py` - `mock_mal` fixture
  - `tests/integration/test_e2e_workflow_real.py` - real E2E workflow test (reference)

### Integration Approach

- Use scenario templates from Story 10.1
- Use workflow runner harness from Epic 9
- Build on existing workflow executor and presets
- Reuse E2E foundation fixtures and utilities
- Follow patterns from workflow E2E tests (Epic 9) but enhance for scenario testing
- Use `mock_mal` fixture for mocked agent execution
- Use scenario validators for outcome contract validation

### Technology Research

- Use `pytest` with `pytest-asyncio` for async workflow execution
- Use `pytest-timeout` for explicit timeouts
- Use `unittest.mock` for mocking agents
- Use `pathlib.Path` for filesystem operations
- Use `yaml` library for workflow loading (already in use)

### Risk Assessment

- **Primary risk**: Scenario tests become too slow or flaky
- **Mitigation**: Use mocked mode by default; optimize test execution; use small/medium project templates; add explicit timeouts; design for scheduled execution
- **Rollback plan**: Scenario tests are additive; can be disabled via markers without affecting other tests; can be moved to nightly-only

### Testing

- Test file location: `tests/e2e/scenarios/` (multiple test files)
- Test standards: Use `pytest`, follow E2E conventions from Epic 8
- Testing frameworks: `pytest`, `pytest-asyncio`, `pytest-timeout`
- Specific requirements:
  - All tests must use `e2e_scenario` marker
  - All tests must use scenario templates
  - All tests must use workflow runner harness
  - All tests must be deterministic in mocked mode
  - All tests must validate outcome contracts
  - All tests must capture failure artifacts
  - Tests should run in reasonable time (optimize as needed)
  - Tests are designed for scheduled execution (not PR blockers)

## Change Log

| Date       | Version | Description                       | Author      |
| ---------- | ------- | --------------------------------- | ----------- |
| 2025-01-XX | 0.1     | Initial draft for Epic 10.2       | bmad-master |

## Dev Agent Record

### Agent Model Used

_TBD_

### Debug Log References

_TBD_

### Completion Notes List

_TBD_

### File List

_TBD_

## QA Results

_TBD_
