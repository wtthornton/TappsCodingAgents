# Story 3.6: Retrieval Quality Evaluation & Safety Hardening

<!-- Source: implementation/cursor/EPIC_03_Expert_System.md -->
<!-- Context: Add measurable retrieval quality + prompt-injection defenses for RAG -->

## Status

Draft

## Story

**As a** maintainer of the expert/RAG system,
**I want** automated retrieval evaluation and safety hardening,
**so that** RAG remains fast, accurate, and resilient to prompt-injection and untrusted content.

## Acceptance Criteria

1. A small evaluation set exists for expert knowledge retrieval (questions + expected sources/snippets) and runs in CI.
2. Retrieval quality metrics are recorded and reported (at minimum: latency, hit rate, and a basic relevance proxy), and CI fails on major regressions.
3. Retrieved content is treated as **untrusted input**:
   - prompt templates clearly label retrieved text as reference material
   - retrieved text cannot override system/agent/tool instructions
   - sources/citations are preserved and displayed
4. Indexing/retrieval has explicit allowlists and “do-not-index” patterns to reduce risk of secrets/PII inclusion.
5. The system degrades gracefully (expert-free mode) on retrieval failures/timeouts and indicates this in outputs.

## Tasks / Subtasks

- [ ] Task 1: Define evaluation dataset format and location (AC: 1)
  - [ ] Create a minimal, deterministic test set mapped to built-in knowledge files
  - [ ] Store expected sources (file path) and/or expected key phrases

- [ ] Task 2: Implement evaluation runner + CI gate (AC: 1–2)
  - [ ] Add a test/harness that runs retrieval for the dataset
  - [ ] Record latency and hit-rate metrics
  - [ ] Define regression thresholds and make failures actionable

- [ ] Task 3: Safety hardening in prompts and context formatting (AC: 3)
  - [ ] Ensure all prompts wrap retrieved text with clear delimiters and “untrusted” labeling
  - [ ] Ensure citations are always surfaced in outputs when RAG is used

- [ ] Task 4: Data governance controls (AC: 4)
  - [ ] Implement allowlist of directories/files eligible for indexing
  - [ ] Add do-not-index patterns (secrets, tokens, private keys) and documentation

- [ ] Task 5: Graceful degradation behavior (AC: 5)
  - [ ] Ensure retrieval failures/timeouts produce explicit status and do not crash agents
  - [ ] Ensure “expert-free mode” is user-visible in result payloads

## Dev Notes

### Existing System Context

- Current retrieval is `tapps_agents/experts/simple_rag.py` (keyword-based) with unit tests in `tests/unit/experts/test_simple_rag.py`.
- Expert consultation already supports sources via `BaseExpert._get_sources()`.
- Confidence/threshold gating exists via `tapps_agents/core/config.py::ExpertConfig`.

### Integration Approach

- Add evaluation as tests to keep it deterministic and CI-friendly.
- Centralize context formatting (with safety delimiters + citations) in the retrieval layer so all experts benefit.

### Risk Assessment

- **Risk**: evaluation becomes flaky due to non-deterministic retrieval.
- **Mitigation**: keep dataset small, keep retrieval deterministic (stable ordering), and measure regressions with wide-enough margins.

### Rollback Plan

- Keep evaluation as informational (non-gating) temporarily if it blocks delivery, but retain metrics output.

### Testing

- Implement evaluation as a unit/integration test module under `tests/unit/experts/` with deterministic fixtures.

## Change Log

| Date       | Version | Description                    | Author      |
| ---------- | ------- | ------------------------------ | ----------- |
| 2025-12-14 | 0.1     | Initial draft for Epic 3.6     | bmad-master |

## Dev Agent Record

### Agent Model Used

_TBD_

### Debug Log References

_TBD_

### Completion Notes List

- _TBD_

### File List

- _TBD_

## QA Results

_TBD_
