# Story 11.2: CLI Golden Paths

<!-- Source: implementation/cursor/EPIC_11_E2E_CLI_Tests.md -->
<!-- Context: Implement E2E tests for most-used CLI commands (review, score, workflow list/start/status) with JSON output validation -->

## Status

Draft

## Story

**As a** QA engineer and developer,
**I want** E2E tests for the most-used CLI commands that validate golden-path execution and JSON output contracts,
**so that** I have confidence that core CLI commands work correctly end-to-end and produce stable, predictable outputs.

## Acceptance Criteria

1. E2E test exists for `review` command:
   - Test: Execute `tapps-agents review <file>` with valid file
   - Test: Validate exit code is 0 (success)
   - Test: Validate JSON output shape (required keys: file, scoring, feedback, passed)
   - Test: Validate scoring keys exist (complexity_score, security_score, maintainability_score, overall_score)
   - Test: Validate file path in output matches input
   - Test: Validate output is valid JSON
2. E2E test exists for `score` command:
   - Test: Execute `tapps-agents score <file>` with valid file
   - Test: Validate exit code is 0 (success)
   - Test: Validate JSON output shape (required keys: file, scoring)
   - Test: Validate scoring keys exist
   - Test: Validate file path in output matches input
   - Test: Validate output is valid JSON
3. E2E test exists for `workflow list` command:
   - Test: Execute `tapps-agents workflow list`
   - Test: Validate exit code is 0 (success)
   - Test: Validate JSON output shape (required keys: workflows or similar)
   - Test: Validate workflow list contains expected workflows
   - Test: Validate output is valid JSON
4. E2E test exists for `workflow start` command:
   - Test: Execute `tapps-agents workflow start <workflow-name>` with valid workflow
   - Test: Validate exit code is 0 (success) or appropriate code
   - Test: Validate JSON output shape (if JSON mode)
   - Test: Validate workflow execution artifacts are created
   - Test: Validate workflow state is persisted
5. E2E test exists for `workflow status` command:
   - Test: Execute `tapps-agents workflow status <workflow-id>` (if supported)
   - Test: Validate exit code is 0 (success)
   - Test: Validate JSON output shape (required keys: status, workflow_id, etc.)
   - Test: Validate output is valid JSON
6. All tests use CLI harness from Story 11.1:
   - Use `run_cli_command()` or `capture_cli_output()` for command execution
   - Use `assert_cli_success()` for success validation
   - Use `assert_cli_output_contract()` for JSON output validation
   - Use isolated project environments
7. All tests validate output contracts (stable, non-brittle):
   - JSON shape validation (required keys, structure)
   - Essential key validation (file paths, scores, status)
   - Avoid free-form text matching
   - Prefer JSON mode for stable assertions
8. All tests use isolated project environments:
   - Use E2E harness project templates (small/medium as appropriate)
   - Each test uses its own temporary project directory
   - Clean state before each test
   - Cleanup after test completion
9. All tests include explicit timeouts and produce actionable failure artifacts:
   - Command-level timeouts
   - Failure artifacts include: stdout, stderr, exit code, command executed, project state
10. All tests are marked with `e2e_cli` marker.
11. All tests run in **mocked mode by default** (if applicable):
    - No real LLM calls for review/score commands (use mocked agents or test files)
    - Tests produce identical results on repeated runs
12. Documentation exists for running CLI golden path tests.

## Tasks / Subtasks

- [ ] Task 1: Create test for `review` command (AC: 1, 6, 7, 8, 9, 10, 11)
  - [ ] Create `tests/e2e/cli/test_review_command.py`
  - [ ] Use CLI harness to execute `tapps-agents review <file>`
  - [ ] Use isolated project with test file
  - [ ] Validate exit code is 0
  - [ ] Validate JSON output shape (required keys)
  - [ ] Validate scoring keys exist
  - [ ] Validate file path in output
  - [ ] Use mocked mode (test file, no real LLM)
  - [ ] Mark with `e2e_cli`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 2: Create test for `score` command (AC: 2, 6, 7, 8, 9, 10, 11)
  - [ ] Create `tests/e2e/cli/test_score_command.py`
  - [ ] Use CLI harness to execute `tapps-agents score <file>`
  - [ ] Use isolated project with test file
  - [ ] Validate exit code is 0
  - [ ] Validate JSON output shape (required keys)
  - [ ] Validate scoring keys exist
  - [ ] Validate file path in output
  - [ ] Use mocked mode (test file, no real LLM)
  - [ ] Mark with `e2e_cli`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 3: Create test for `workflow list` command (AC: 3, 6, 7, 8, 9, 10)
  - [ ] Create `tests/e2e/cli/test_workflow_list_command.py`
  - [ ] Use CLI harness to execute `tapps-agents workflow list`
  - [ ] Use isolated project (may need workflows directory)
  - [ ] Validate exit code is 0
  - [ ] Validate JSON output shape (workflows list)
  - [ ] Validate expected workflows are in list
  - [ ] Mark with `e2e_cli`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 4: Create test for `workflow start` command (AC: 4, 6, 7, 8, 9, 10, 11)
  - [ ] Create `tests/e2e/cli/test_workflow_start_command.py`
  - [ ] Use CLI harness to execute `tapps-agents workflow start <workflow-name>`
  - [ ] Use isolated project with workflow preset
  - [ ] Validate exit code (0 or appropriate)
  - [ ] Validate JSON output shape (if JSON mode)
  - [ ] Validate workflow execution artifacts are created
  - [ ] Validate workflow state is persisted
  - [ ] Use mocked mode (no real LLM, minimal workflow)
  - [ ] Mark with `e2e_cli`
  - [ ] Add timeouts and failure artifact capture

- [ ] Task 5: Create test for `workflow status` command (AC: 5, 6, 7, 8, 9, 10, 11)
  - [ ] Check if `workflow status` command exists in CLI
  - [ ] If exists, create `tests/e2e/cli/test_workflow_status_command.py`
  - [ ] Use CLI harness to execute `tapps-agents workflow status <workflow-id>`
  - [ ] Use isolated project with running/completed workflow
  - [ ] Validate exit code is 0
  - [ ] Validate JSON output shape (status, workflow_id, etc.)
  - [ ] Use mocked mode
  - [ ] Mark with `e2e_cli`
  - [ ] Add timeouts and failure artifact capture
  - [ ] If command doesn't exist, document as future enhancement

- [ ] Task 6: Implement output contract validation (AC: 7)
  - [ ] Use CLI harness `assert_cli_output_contract()` for JSON validation
  - [ ] Define expected JSON schemas for each command
  - [ ] Validate required keys and structure
  - [ ] Validate essential keys (file paths, scores, status)
  - [ ] Avoid free-form text matching
  - [ ] Provide clear validation error messages

- [ ] Task 7: Optimize test execution and reliability (AC: 9)
  - [ ] Review test execution time
  - [ ] Optimize slow tests (reduce fixture complexity, use faster mocks)
  - [ ] Ensure command-level timeouts are appropriate
  - [ ] Validate failure artifact capture works correctly
  - [ ] Test artifact capture in failing scenarios

- [ ] Task 8: Documentation and validation (AC: 12)
  - [ ] Document CLI golden path test coverage in `tests/e2e/cli/README.md`
  - [ ] Document how to run CLI E2E tests locally
  - [ ] Document output contract validation approach
  - [ ] Document command-specific requirements and setup
  - [ ] Verify all tests are deterministic in mocked mode (run twice, compare results)

## Dev Notes

### Existing System Context

- CLI commands:
  - `tapps_agents/cli.py` - CLI entry point with commands
  - Commands: `review`, `score`, `workflow`, `init`, `doctor`, `plan`, etc.
  - Execution: `python -m tapps_agents.cli <command> <args>`
- CLI harness (from Story 11.1):
  - `tests/e2e/fixtures/cli_runner.py` - CLI runner harness
- Workflow system:
  - `tapps_agents/workflow/executor.py` - workflow execution
  - Workflows in `workflows/` and `workflows/presets/`
- E2E foundation (from Epic 8):
  - `tests/e2e/fixtures/e2e_harness.py` - E2E harness utilities
  - `tests/e2e/fixtures/project_templates.py` - project templates
  - `tests/e2e/conftest.py` - E2E fixtures
- Existing integration tests:
  - `tests/integration/test_cli_create_timeline_verification.py` - CLI create command test
  - `tests/integration/test_cli_create_cursor_ide_timeline.py` - CLI create command test

### Integration Approach

- Use CLI harness from Story 11.1
- Build on existing CLI entry point
- Reuse E2E foundation fixtures and utilities
- Follow patterns from existing CLI integration tests but enhance for E2E testing
- Use isolated project environments for each test

### Technology Research

- Use `pytest` for test framework
- Use `subprocess` for CLI command execution (via CLI harness)
- Use `json` for JSON output parsing and validation
- Use `pathlib.Path` for filesystem operations
- Use `pytest-timeout` for timeout handling (if needed)

### Risk Assessment

- **Primary risk**: CLI output assertions become brittle over time
- **Mitigation**: Use JSON output contracts; validate stable keys; avoid free-form text matching
- **Rollback plan**: CLI E2E tests are additive; can be disabled via markers without affecting other tests

### Testing

- Test file location: `tests/e2e/cli/` (multiple test files)
- Test standards: Use `pytest`, follow E2E conventions from Epic 8
- Testing frameworks: `pytest`, `pytest-timeout`
- Specific requirements:
  - All tests must use `e2e_cli` marker
  - All tests must use CLI harness
  - All tests must validate output contracts (JSON shape, required keys)
  - All tests must use isolated project environments
  - All tests must capture failure artifacts
  - Tests should run in reasonable time (optimize as needed)
  - Tests should be deterministic in mocked mode

## Change Log

| Date       | Version | Description                       | Author      |
| ---------- | ------- | --------------------------------- | ----------- |
| 2025-01-XX | 0.1     | Initial draft for Epic 11.2       | bmad-master |

## Dev Agent Record

### Agent Model Used

_TBD_

### Debug Log References

_TBD_

### Completion Notes List

_TBD_

### File List

_TBD_

## QA Results

_TBD_
