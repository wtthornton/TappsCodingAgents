# Story 15.1: Agent Command Processing Tests

<!-- Source: implementation/cursor/EPIC_15_E2E_Agent_Behavior_Testing.md -->
<!-- Context: Add E2E tests for agent command parsing and validation -->

## Status

Complete

## Story

**As a** QA engineer and developer,
**I want** comprehensive E2E tests that validate agent command parsing, validation, and help output,
**so that** I can ensure agents correctly process user commands and provide clear feedback for invalid inputs.

## Acceptance Criteria

1. E2E tests exist in `tests/e2e/agents/test_agent_command_processing.py` that validate command parsing for all agent types (planner, implementer, reviewer, tester).
2. Tests validate that valid commands are accepted and correctly parsed:
   - Star-prefixed commands (`*review file.py`) are parsed correctly
   - Numbered commands (`1`, `2`, etc.) map to correct commands from command list
   - Space-separated commands (`review file.py`) are parsed correctly
   - Command arguments are extracted correctly (e.g., file paths, parameters)
3. Tests validate that invalid commands are rejected with clear error messages:
   - Unknown commands produce appropriate error responses
   - Invalid command formats produce helpful error messages
   - Out-of-range numbered commands produce clear errors
   - Missing required arguments produce actionable error messages
4. Tests validate command parameter validation:
   - Required parameters are validated (e.g., file path required for review command)
   - Parameter types are validated (e.g., file existence, path validity)
   - Invalid parameter values produce clear error messages
5. Tests validate command help/usage output:
   - Help commands return structured command lists
   - Command descriptions are present and informative
   - Usage examples are provided where applicable
6. Test utilities exist in `tests/e2e/fixtures/agent_test_helpers.py` for:
   - Creating test agents with mocked MAL
   - Executing commands and capturing responses
   - Validating command parsing results
   - Asserting error message quality
7. Tests use behavioral mocks (not just "Mock LLM response") that simulate realistic agent behavior for command processing.
8. All tests pass consistently and are Windows-compatible.

## Tasks / Subtasks

- [x] Task 1: Create agent test helper utilities (AC: 6)
  - [x] Create `tests/e2e/fixtures/agent_test_helpers.py`
  - [x] Implement `create_test_agent(agent_type, mock_mal)` helper
  - [x] Implement `execute_command(agent, command)` helper
  - [x] Implement `assert_command_parsed(parsed_result, expected_command, expected_args)` helper
  - [x] Implement `assert_error_message(error_response, expected_keywords)` helper
  - [x] Add helpers for validating help output structure

- [x] Task 2: Implement command parsing validation tests (AC: 2)
  - [x] Create `tests/e2e/agents/test_agent_command_processing.py`
  - [x] Test star-prefixed command parsing (`*review file.py`)
  - [x] Test numbered command parsing (`1`, `2`, etc.)
  - [x] Test space-separated command parsing (`review file.py`)
  - [x] Test argument extraction (file paths, parameters)
  - [x] Test for each agent type (planner, implementer, reviewer, tester)

- [x] Task 3: Implement invalid command rejection tests (AC: 3)
  - [x] Test unknown command handling
  - [x] Test invalid command format handling
  - [x] Test out-of-range numbered commands
  - [x] Test missing required arguments
  - [x] Validate error message clarity and actionability
  - [x] Test for each agent type

- [x] Task 4: Implement parameter validation tests (AC: 4)
  - [x] Test required parameter validation
  - [x] Test parameter type validation (file existence, path validity)
  - [x] Test invalid parameter value handling
  - [x] Validate error message quality for parameter errors
  - [x] Test for each agent type

- [x] Task 5: Implement help/usage output tests (AC: 5)
  - [x] Test help command returns structured command list
  - [x] Test command descriptions are present
  - [x] Test usage examples are provided
  - [x] Validate help output structure and completeness
  - [x] Test for each agent type

- [x] Task 6: Create behavioral mocks for command processing (AC: 7)
  - [x] Enhance `mock_mal` or create agent-specific mocks
  - [x] Mock realistic command processing behavior
  - [x] Mock realistic error responses
  - [x] Ensure mocks simulate actual agent logic, not just generic responses

- [x] Task 7: Documentation and test execution (AC: 8)
  - [x] Document command processing test patterns in `tests/e2e/agents/README.md`
  - [x] Ensure all tests are Windows-compatible
  - [x] Verify tests pass consistently
  - [x] Add test markers appropriately (`@pytest.mark.e2e`)

## Dev Notes

### Existing System Context

- Agent base class: `tapps_agents/core/agent_base.py`
  - `BaseAgent` class with `parse_command()` method
  - `parse_command()` supports: star-prefixed commands, numbered commands, space-separated commands
  - `get_commands()` method returns available commands
  - `run()` method executes commands (abstract, implemented by each agent)
- Agent implementations:
  - `tapps_agents/agents/planner/agent.py` - Planner agent
  - `tapps_agents/agents/implementer/agent.py` - Implementer agent
  - `tapps_agents/agents/reviewer/agent.py` - Reviewer agent
  - `tapps_agents/agents/tester/agent.py` - Tester agent (if exists)
- Existing E2E tests:
  - `tests/e2e/smoke/test_agent_lifecycle.py` - basic lifecycle tests (activation, execution, cleanup)
  - `tests/conftest.py` - `mock_mal` fixture (returns "Mock LLM response")
- E2E foundation (from Epic 8):
  - `tests/e2e/fixtures/e2e_harness.py` - E2E harness utilities
  - `tests/e2e/fixtures/project_templates.py` - project templates
  - `tests/e2e/conftest.py` - E2E fixtures

### Integration Approach

- Build on existing `BaseAgent.parse_command()` method
- Extend `mock_mal` fixture or create agent-specific behavioral mocks
- Use E2E harness utilities from Epic 8 for project setup
- Follow existing E2E test patterns from `test_agent_lifecycle.py`
- Create reusable test helpers for command processing validation

### Technology Research

- Use `pytest` with `pytest-asyncio` for async agent execution
- Use `unittest.mock` for creating behavioral mocks
- Use `pathlib.Path` for file path validation
- Use `pytest.mark.parametrize` for testing multiple agent types
- Reference `pytest` documentation for test organization patterns

### Risk Assessment

- **Primary risk**: Command parsing tests may be brittle if parsing logic changes
- **Mitigation**: Test against documented command format contracts, not implementation details
- **Rollback plan**: Command processing tests are additive; can be disabled via markers if needed

### Testing

- Test file location: `tests/e2e/agents/test_agent_command_processing.py`
- Test standards: Use `pytest`, follow existing E2E test patterns
- Testing frameworks: `pytest`, `pytest-asyncio` for async tests
- Specific requirements:
  - Test command parsing for all agent types
  - Test invalid command handling
  - Test parameter validation
  - Test help output
  - Use behavioral mocks (not generic "Mock LLM response")
  - Windows-compatible (use `pathlib.Path`, no shell commands)

## Change Log

| Date       | Version | Description                       | Author      |
| ---------- | ------- | --------------------------------- | ----------- |
| 2025-01-XX | 0.1     | Initial draft for Epic 15.1      | bmad-master |

## Dev Agent Record

### Agent Model Used

_To be populated by dev agent_

### Debug Log References

_To be populated by dev agent_

### Completion Notes List

_To be populated by dev agent_

### File List

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_

