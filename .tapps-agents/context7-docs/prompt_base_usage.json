{
  "library": "prompt_base",
  "topic": "usage",
  "documentation": {
    "content": "## Update Token Count\n\n```Python\ndefupdate_token_count(self, inputs: List[str], outputs: List[str]):\n\"\"\"Update the token count based on the given inputs and outputs.\n\n            Uses the tokenizer to count the tokens.\n\n        Args:\n            inputs (List[str]): A list of input prompts.\n            outputs (List[str]): A list of generated responses.\n        \"\"\"\n        for input in inputs:\n            self.input_token_count += len(self.tokenizer.encode(input))\n\n        for output in outputs:\n            self.output_token_count += len(self.tokenizer.encode(output))\n```\n\n## Promptolution Base LLM Methods\n\n```Python\nfrom promptolution.llms.base_llm import BaseLLM\n\n# Assuming base_llm_instance is an instance of BaseLLM\n# response = base_llm_instance.get_response(\"Your prompt\")\n# token_count = base_llm_instance.get_token_count()\n# base_llm_instance.reset_token_count()\n# base_llm_instance.set_generation_seed(42)\n# base_llm_instance.update_token_count(100)\n```\n\n## Prepare Data for Experiment\n\n```python\ndf = pd.read_csv(\"hf://datasets/tasksource/subjectivity/train.csv\").sample(500)\ndf = df.rename(columns={\"Sentence\": \"x\", \"Label\": \"y\"})\ndf = df.replace({\"OBJ\": \"objective\", \"SUBJ\": \"subjective\"})\n\ntask_description = (\n    \"The dataset contains sentences labeled as either subjective or objective. \"\n    \"The task is to classify each sentence as either subjective or objective. \"\n    \"The class mentioned in between the answer tags <final_answer></final_answer> will be used as the prediction.\"\n)\n```\n\n## Python: Validate unused attributes in Config\n\n```Python\ndefvalidate(self):\n\"\"\"Check if any attributes were not used and run validation.\n\n    Does not raise an error, but logs a warning if any attributes are unused or validation fails.\n    \"\"\"\n    all_attributes = {k for k in self.__dict__ if not k.startswith(\"_\")}\n    unused_attributes = all_attributes - self._used_attributes\n    if unused_attributes:\n        logger.warning(f\"\u26a0\ufe0f Unused configuration attributes: {unused_attributes}\")\n```\n\n## Get LLM Token Counter\n\n```Python\ndef get_token_counter(llm):\n    \"\"\"Get a token counter function for the given LLM.\n\n    This function returns a callable that counts tokens based on the LLM's tokenizer\n    or a simple split method if no tokenizer is available.\n\n    Args:\n        llm: The language model object that may have a tokenizer.\n\n    Returns:\n        A callable that takes a text input and returns the token count.\n\n    \"\"\"\n    if hasattr(llm, \"tokenizer\"):\n        token_counter = lambda x: len(llm.tokenizer(x)(\"input_ids\"))\n    else:\n        logger.warning(\"\u26a0\ufe0f The LLM does not have a tokenizer. Using simple token count.\")\n        token_counter = lambda x: len(x.split())\n\n    return token_counter\n```\n\n## Generate Responses with LocalLLM\n\n```Python\n    def _get_response(self, prompts: list[str], system_prompts: list[str]) -> list[str]:\n        \"\"\"Generate responses for a list of prompts using the local language model.\n\n        Args:\n            prompts (list[str]): A list of input prompts.\n            system_prompts (list[str]): A list of system prompts to guide the model's behavior.\n\n        Returns:\n            list[str]: A list of generated responses corresponding to the input prompts.\n\n        Note:\n            This method uses torch.no_grad() for inference to reduce memory usage.\n            It handles both single and batch inputs, ensuring consistent output format.\n        \"\"\"\n        inputs = []\n        for prompt, sys_prompt in zip(prompts, system_prompts):\n            inputs.append([{\"role\": \"system\", \"prompt\": sys_prompt}, {\"role\": \"user\", \"prompt\": prompt}])\n\n        with torch.no_grad():\n            response = self.pipeline(inputs, pad_token_id=self.pipeline.tokenizer.eos_token_id)\n\n        if len(response) != 1:\n            response = [r[0] if isinstance(r, list) else r for r in response]\n\n        response = [r[\"generated_text\"] for r in response]\n        return response\n```\n\n## Classify Sentence Tone (Objective/Subjective)\n\n```text\nClassify each sentence as either objective or subjective by examining its linguistic tone, underlying intent, and purpose. Consider whether the text presents a neutral, factual account or expresses a personal opinion or emotional bias. Evaluate whether the text is neutral and provides mere reportage, such as a factual report on congressional Democrats' actions and labor union negotiations, or if it reveals an evaluative tone, offering a positive or negative appraisal of a nation's past performance. Outputs will include classifications like objective or subjective. The class mentioned first in the response will serve as the prediction, with the class label extracted from the text between the markers <final_answer> and </final_answer>.\n\nInput:\nOver several decades, Prime Central London \u2013 or PCL \u2013 had become a repository for cash from wealthy foreigners, whether they actually wanted to live there or not.\n\nOutput:\n<final_answer>objective</final_answer>\n\nInput:\nFaced with a tighter labor market, many districts are raising base salaries and offering signing and relocation bonuses \u2014 up to a whopping $25,000 in one New Mexico school district.\n\nOutput:\n<final_answer>objective</final_answer>\n\nInput:\nThat when liquidation of commodities and securities has gone too far it becomes the business of government to stop it, using public credit by such means as it may think fit.\n\nOutput:\n<final_answer>subjective</final_answer>\n```\n\n## Classify Sentence Tone (Objective/Subjective) - Nuanced\n\n```text\nGiven a sentence, classify it as either \"objective\" or \"subjective\" based on its tone and language, considering the presence of third-person pronouns, neutral language, and opinions. Classify the output as \"objective\" if the tone is neutral and detached, focusing on facts and data, or as \"subjective\" if the tone is evaluative, emotive, or biased.\n\nInput:\nTransportation Secretary Pete Buttigieg confirmed to The Associated Press on Thursday that $104.6 million in federal funds coming from last year\u2019s bipartisan infrastructure bill will go toward a plan to dismantle Interstate 375, a highway built to bisect Detroit\u2019s Black Bottom neighborhood and its epicenter of Black business, Paradise Valley.\n\nOutput:\n<final_answer>objective</final_answer>\n\nInput:\n\u201cThis latest rule will open our borders even more, and the Court seems to relish making arbitrary decisions without thinking about consequences.\n\nOutput:\n<final_answer>objective</final_answer>\n\nInput:\nHe is fairly secure.\n\nOutput:\n<final_answer>objective</final_answer>\n\nInput:\nIn a recent report on the \u201cnew poor,\u201d made by the Welfare Council of New York City, there is a reference to \u201cthe mental infection of dependency.\u201d This was upon the investigation of unemployment relief.\n\nOutput:\n<final_answer>objective</final_answer>\n```\n\n## Python: Initialize Config with keyword arguments\n\n```Python\ndef__init__(self, **kwargs):\n\"\"\"Initialize the configuration with the provided keyword arguments.\"\"\"\n    self._used_attributes: Set[str] = set()\n    for key, value in kwargs.items():\n        setattr(self, key, value)\n```\n\n## Sample Few-Shot Examples\n\n```Python\ndef _sample_examples(self) -> str:\n        \"\"\"Sample few-shot examples from the dataset.\n\n        Returns:\n            Formatted string of few-shot examples with inputs and expected outputs\n        \"\"\"\n        idx = np.random.choice(len(self.task.xs), self.num_few_shots)\n        sample_x = self.task.xs[idx]\n        sample_y = self.task.ys[idx]\n\n        return \"\\n\".join([f\"Input: {x}\\nOutput: {y}\" for x, y in zip(sample_x, sample_y)])\n```",
    "library": "prompt_quality",
    "topic": "usage",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:32:52.834872"
}