{
  "library": "prompt",
  "topic": null,
  "documentation": {
    "content": "## Select Best Prompt and Display Metrics (Python)\n\n```python\n# Find the best index based on highest test accuracy\nbest_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n\n# Retrieve values\nbest_prompt = result[\"prompts\"][best_idx - 1]\nbest_test_acc = result[\"test\"][best_idx]\nbest_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\ninitial_test_acc = result[\"test\"][0]\ninitial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n\n```\n\n## Optimize JSON Webpage Generation Prompts with Optimizer SDK\n\n```python\nimport pandas as pd\nfrom optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\nimport os\n\n# Initial prompt\nsystem_prompt = \"You are an expert in JSON webpage creation. Generate: {input}\"\n\n# Training data with feedback\ntrain_data = pd.DataFrame({\n    \"input\": [\n        \"Create a homepage for a tech startup\",\n        \"Build a product catalog page\",\n        \"Generate an about us page\"\n    ],\n    \"output\": [\n        '{\"title\": \"Homepage\"}',  # Missing structure\n        '{\"page\": {\"sections\": [{\"type\": \"catalog\"}], \"updatedAt\": \"2024-01-01\"}}',  # Good\n        '{\"content\": {\"title\": \"About\"}}'  # Wrong key\n    ],\n    \"correctness\": [\"incorrect\", \"correct\", \"incorrect\"],\n    \"explanation\": [\n        \"Missing 'page' wrapper, 'sections' array, and 'updatedAt' field\",\n        \"Correct structure with all required fields\",\n        \"Top-level key must be 'page', not 'content'\"\n    ],\n    \"rule_violations\": [\n        \"Rules violated: 1, 3, 5\",\n        \"No violations\",\n        \"Rules violated: 1\"\n    ]\n})\n\n# Initialize optimizer\noptimizer = PromptLearningOptimizer(\n    prompt=system_prompt,\n    model_choice=\"gpt-4o\",\n    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\n# Optimize with all feedback columns\noptimized_prompt = optimizer.optimize(\n    dataset=train_data,\n    output_column=\"output\",\n    feedback_columns=[\"correctness\", \"explanation\", \"rule_violations\"],\n    context_size_k=128000\n)\n\nprint(\"Original:\", system_prompt)\nprint(\"\\nOptimized:\", optimized_prompt)\n\n# Output:\n# Original: You are an expert in JSON webpage creation. Generate: {input}\n#\n# Optimized: You are an expert in JSON webpage creation. When generating webpages, follow these rules:\n# 1. Always use 'page' as the top-level key (never 'content', 'title', or other keys)\n# 2. Include a 'sections' array containing the page sections\n# 3. Each section must have a 'type' field using allowed vocabulary\n# 4. Always include an 'updatedAt' field with ISO 8601 timestamp\n# 5. Validate the complete structure before returning the JSON\n#\n# Generate: {input}\n```\n\n## Find Best Prompt and Display Metrics (Python)\n\n```python\n# Find the best index based on highest test accuracy\nbest_idx = max(range(len(result[\"test\"])), key=lambda i: result[\"test\"][i])\n\n# Retrieve values\nbest_prompt = result[\"prompt\"][best_idx - 1]\nbest_test_acc = result[\"test\"][best_idx]\nbest_train_acc = result[\"train\"][best_idx - 1] if (best_idx - 1) < len(result[\"train\"]) else None\ninitial_test_acc = result[\"test\"][0]\ninitial_train_acc = result[\"train\"][0] if result[\"train\"] else None\n\n# Print results\nprint(\"\\n\ud83d\udd0d Best Prompt Found:\")\nprint(best_prompt)\nprint(f\"\ud83e\uddea Initial Test Accuracy: {initial_test_acc}\")\nprint(f\"\ud83e\uddea Optimized Test Accuracy: {best_test_acc} (\u0394 {best_test_acc - initial_test_acc:.4f})\")\n```\n\n## Optimize Prompt and Run Experiments\n\n```python\nsystem_prompt = optimizer.optimize(\n            train_df,\n            \"output\",\n            feedback_columns=feedback_columns,\n            context_size_k=90000,\n            annotations=annotations,\n        )\n        train_metric_post_value = compute_metric(train_experiment, scorer)\n        train_metrics.append(train_metric_post_value)\n\n        test_experiment = await async_run_experiment(\n            dataset=test_dataset,\n            task=generate_task(system_prompt),\n            evaluators=[test_evaluator]\n        )\n        test_metric_post_value = compute_metric(test_experiment, scorer)\n        test_metrics.append(test_metric_post_value)\n\n        print(f\"\u2705 Train {scorer}: {train_metric_post_value}\")\n        print(f\"\u2705 Test {scorer}: {test_metric_post_value}\")\n\n        prompt_versions = upload_prompt_phoenix(system_prompt, prompt_name, curr_loop, prompt_versions, train_metric_post_value, test_metric_post_value)\n\n        if test_metric_post_value >= threshold:\n            print(\"\ud83c\udf89 Prompt optimization met threshold!\")\n            break\n\n        loops -= 1\n        curr_loop += 1\n\n    return {\n        \"train\": train_metrics,\n        \"test\": test_metrics,\n        \"prompt\": prompt_versions,\n        \"raw\": raw_dfs\n    }\n```\n\n## Log Experiments to Arize\n\n```python\nfrom arize.experimental.datasets.experiments.types import (\n    ExperimentTaskResultColumnNames,\n    EvaluationResultColumnNames,\n)\n\ntask_columns = ExperimentTaskResultColumnNames(\n    example_id=\"example_id\", result=\"final_plan\"\n)\nevaluator_columns = EvaluationResultColumnNames(\n    label=\"correctness\",\n    explanation=\"explanation\",\n    score=\"score\"\n)\n```\n\n## Extract Best Prompt from Optimization Results - Python\n\n```python\ndef get_best_prompt(results):\n    \"\"\"\n    Extract the prompt that achieved the best test accuracy.\n    \n    Args:\n        results: Results from optimize_loop or experiment\n        \n    Returns:\n        tuple: (best_prompt, best_accuracy, iteration_number)\n    \"\"\"\n    test_metrics = results['test']\n    prompts = results['prompt']\n    \n    # Find the iteration with highest test accuracy\n    best_iteration = test_metrics.index(max(test_metrics))\n    best_accuracy = test_metrics[best_iteration]\n    best_prompt = prompts[best_iteration]\n    \n    return best_prompt, best_accuracy, best_iteration\n\n# Usage example:\nbest_prompt, best_accuracy, best_iter = get_best_prompt(results)\nprint(f\"Original Prompt: {system_prompt}\")\nprint(f\"Best Optimized Prompt (iteration {best_iter}, accuracy: {best_accuracy:.3f}):\")\nprint(best_prompt)\n```\n\n## Execute Prompt Learning Workflow in Python\n\n```python\nfeedback_columns = [\"evaluator_correctness\", \"evaluator_explanation\"]\n\nresults = interactive_optimization_loop(train, test, feedback_columns, evaluator_prompt, loops=5)\n\n```\n\n## Prompt Learning Batch Processing Log\n\n```log\n\ud83d\udd27 Creating batches with 90,000 token limit\n\ud83d\udcca Processing 123 examples in 1 batches\n   \u2705 Batch 1/1: Optimized\n```\n\n## Optimize Prompt with Annotations and Rulesets\n\n```python\nfrom optimizer_sdk.optimizer import Optimizer\n\noptimizer = Optimizer()\n\n# Prepare annotations from previous analysis\nannotations = [\n    \"Rule 1: Always validate JSON structure before submission\",\n    \"Rule 2: Include required fields: page, updatedAt, sections\",\n    \"Rule 3: Use allowed section types: hero, content, footer\"\n]\n\n# Define dynamic ruleset for iterative improvement\ndynamic_ruleset = \"\"\"\n- Validate all JSON outputs against schema\n- Check for required fields before generating\n- Use proper error handling for edge cases\n\"\"\"\n\n# Optimize with annotations\n# If ruleset is provided, it optimizes the ruleset instead of the prompt\noptimized_prompt_or_ruleset = optimizer.optimize(\n    dataset=train_data, # Assuming train_data is loaded\n    output_column=\"output\",\n    feedback_columns=[\"correctness\", \"explanation\"],\n    annotations=annotations,\n    ruleset=dynamic_ruleset,\n    context_size_k=128000\n)\n\n# When ruleset is provided, returns optimized ruleset\nprint(optimized_prompt_or_ruleset)\n```\n\n## Optimize Prompt with English Feedback\n\n```python\nimport pandas as pd\nfrom optimizer_sdk.prompt_learning_optimizer import PromptLearningOptimizer\n\n# Create dataset with English feedback\ndataset = pd.DataFrame({\n    'input': [\"Generate a tech company's career page\"],\n    'output': [\"{incorrect JSON output}\"],\n    'feedback': [\"The generated JSON breaks several rules: missing 'updatedAt' field, top-level key should be 'page'\"]\n})\n\n# Initialize optimizer\noptimizer = PromptLearningOptimizer(\n    prompt=\"You are an expert in JSON webpage creation. Generate: {input}\",\n    model_choice=\"gpt-4\"\n)\n\n# Optimize the prompt using English feedback\noptimized_prompt = optimizer.optimize(\n    dataset=dataset,\n    output_column='output',\n    feedback_columns=['feedback']\n)\n```",
    "library": "Prompt",
    "topic": "best-practices",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-02-05T14:47:15.382089"
}