{
  "library": "score_validator",
  "topic": "examples",
  "documentation": {
    "content": "## Example Structure for few_shot_examples Parameter - TypeScript\n\n```TypeScript\nconst fewShotExamples = [\n  {\n    inputs: \"What color is the sky?\",\n    outputs: \"The sky is red.\",\n    reasoning: \"The sky is red because it is early evening.\",\n    score: 1,\n  }\n];\n```\n\n## Few-Shot Example Structure for LLM-as-Judge (Python)\n\n```Python\nfew_shot_examples = [\n    {\n        \"inputs\": \"What color is the sky?\",\n        \"outputs\": \"The sky is red.\",\n        \"reasoning\": \"The sky is red because it is early evening.\",\n        \"score\": 1,\n    }\n]\n```\n\n## Example LangGraph Trajectory Output (JSON) - TS Example\n\n```JSON\n{\n  'inputs': [{\n      '__start__': {\n          'messages': [\n              {'role': 'user', 'content': \"what's the weather in sf?\"}\n          ]}\n      }, \n      '__resuming__': {\n          'messages': [\n              {'role': 'user', 'content': 'It is rainy and 70 degrees!'}\n          ]}\n      ],\n      'outputs': {\n          'results': [\n            {},\n            {\n                'messages': [\n                    {'role': 'ai', 'content': 'The current weather in San Francisco is rainy, with a temperature of 70 degrees.'}\n                ]\n            }\n        ],\n        'steps': [\n            ['__start__', 'agent', 'tools', '__interrupt__'],\n            ['agent']\n        ]\n    }\n}\n```\n\n## Example Output (With Reference)\n\n```json\n{\n    'key': 'trajectory_accuracy',\n    'score': True,\n    'comment': 'The provided agent trajectory is consistent with the reference. Both trajectories start with the same user query and then correctly invoke a weather lookup through a tool call. Although the reference uses \"San Francisco\" while the provided trajectory uses \"SF\" and there is a minor formatting difference (degrees vs. \u02da), these differences do not affect the correctness or essential steps of the process. Thus, the score should be: true.'\n}\n```\n\n## Few-Shot Examples Parameter Structure\n\n```python\nfew_shot_examples = [\n    {\n        \"inputs\": \"What color is the sky?\",\n        \"outputs\": \"The sky is red.\",\n        \"reasoning\": \"The sky is red because it is early evening.\",\n        \"score\": 1,\n    }\n]\n```\n\n## Example Output (No Reference)\n\n```json\n{\n    'key': 'trajectory_accuracy',\n    'score': True,\n    'comment': 'The provided agent trajectory is reasonable...'\n}\n```\n\n## Example Graph Trajectory Evaluation Output (JSON) - TS Example\n\n```JSON\n{\n  'key': 'graph_trajectory_accuracy',\n  'score': True,\n  'comment': 'The overall process follows a logical progression: the conversation begins with the user\u2019s request, the agent then processes the request through its own internal steps (including calling tools), interrupts to obtain further input, and finally resumes to provide a natural language answer. Each step is consistent with the intended design in the rubric, and the overall path is relatively efficient and semantically aligns with a typical query resolution trajectory. Thus, the score should be: true.'\n}\n```\n\n## Few-Shot Example Structure for LLM-as-Judge (TypeScript)\n\n```TypeScript\nconst fewShotExamples = [\n  {\n    inputs: \"What color is the sky?\",\n    outputs: \"The sky is red.\",\n    reasoning: \"The sky is red because it is early evening.\",\n    score: 1,\n  }\n];\n```\n\n## Example LangGraph Trajectory Output (JSON)\n\n```JSON\n{\n  'inputs': [{\n      '__start__': {\n          'messages': [\n              {'role': 'user', 'content': \"what's the weather in sf?\"}\n          ]}\n      }, \n      '__resuming__': {\n          'messages': [\n              {'role': 'user', 'content': 'It is rainy and 70 degrees!'}\n          ]}\n      ],\n      'outputs': {\n          'results': [\n            {},\n            {\n                'messages': [\n                    {'role': 'ai', 'content': 'The current weather in San Francisco is rainy, with a temperature of 70 degrees.'}\n                ]\n            }\n        ],\n        'steps': [\n            ['__start__', 'agent', 'tools', '__interrupt__'],\n            ['agent']\n        ]\n    }\n}\n```\n\n## Example Graph Trajectory Evaluation Output (JSON)\n\n```JSON\n{\n  'key': 'graph_trajectory_accuracy',\n  'score': True,\n  'comment': 'The overall process follows a logical progression: the conversation begins with the user\u2019s request, the agent then processes the request through its own internal steps (including calling tools), interrupts to obtain further input, and finally resumes to provide a natural language answer. Each step is consistent with the intended design in the rubric, and the overall path is relatively efficient and semantically aligns with a typical query resolution trajectory. Thus, the score should be: true.'\n}\n```",
    "library": "evaluators",
    "topic": "examples",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-02-03T09:49:37.390446"
}