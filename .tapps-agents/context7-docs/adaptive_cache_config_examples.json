{
  "library": "adaptive_cache_config",
  "topic": "examples",
  "documentation": {
    "content": "## Adaptive Learner Core Methods\n\n```Python\ndef ask(self, n: int, tell_pending: bool = True) -> tuple[list[tuple[Int, Any]], list[float]]:\n\"\"\"Chose points for learners.\"\"\"\n        if n == 0:\n            return [], []\n\n        if not tell_pending:\n            with restore(*self.learners):\n                return self._ask_and_tell(n)\n        else:\n            return self._ask_and_tell(n)\n\ndef tell(self, x: tuple[Int, Any], y: Any) -> None:\n\"\"\"Tell the result of an evaluation.\"\"\"\n        index, x_ = x\n        self._ask_cache.pop(index, None)\n        self._loss.pop(index, None)\n        self._pending_loss.pop(index, None)\n        self.learners[index].tell(x_, y)\n\ndef tell_pending(self, x: tuple[Int, Any]) -> None:\n\"\"\"Tell pending results for an evaluation.\"\"\"\n        index, x_ = x\n        self._ask_cache.pop(index, None)\n        self._loss.pop(index, None)\n        self.learners[index].tell_pending(x_)\n```\n\n## API Documentation - Adaptive Utilities and Integrations\n\n```APIDOC\nadaptive.utils module\nadaptive.notebook_integration module\n```\n\n## Data Serialization and Management in Python\n\n```Python\nself.losses_combined = deepcopy(self.losses)\nself.neighbors_combined = deepcopy(self.neighbors)\n\n\ndef_get_data(self) -> dict[float, float]:\n    return self.data\n\ndef_set_data(self, data: dict[float, float]) -> None:\n    if data:\n        xs, ys = zip(*data.items())\n        self.tell_many(xs, ys)\n\ndef__getstate__(self):\n    return (\n        cloudpickle.dumps(self.function),\n        tuple(self.bounds),\n        self.loss_per_interval,\n        dict(self.losses),  # SortedDict cannot be pickled\n        dict(self.losses_combined),  # ItemSortedDict cannot be pickled\n        self._get_data(),\n    )\n\ndef__setstate__(self, state):\n    function, bounds, loss_per_interval, losses, losses_combined, data = state\n    function = cloudpickle.loads(function)\n    self.__init__(function, bounds, loss_per_interval)\n    self._set_data(data)\n    self.losses.update(losses)\n    self.losses_combined.update(losses_combined)\n```\n\n## Adaptive Learner Setup and Benchmarking Utilities\n\n```ipython3\nfrom __future__ import annotations\n\nimport itertools\n\nimport holoviews as hv\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\n\nimport adaptive\n\nadaptive.notebook_extension()\n\nbenchmarks = {}\nbenchmarks_2d = {}\n\n\ndef homogeneous_learner(learner):\n    if isinstance(learner, adaptive.Learner1D):\n        xs = np.linspace(*learner.bounds, learner.npoints)\n        homo_learner = adaptive.Learner1D(learner.function, learner.bounds)\n        homo_learner.tell_many(xs, learner.function(xs))\n    else:\n        homo_learner = adaptive.Learner2D(learner.function, bounds=learner.bounds)\n        n = int(learner.npoints**0.5)\n        xs, ys = (np.linspace(*bounds, n) for bounds in learner.bounds)\n        xys = list(itertools.product(xs, ys))\n        zs = map(homo_learner.function, xys)\n        homo_learner.tell_many(xys, zs)\n    return homo_learner\n\n\ndef plot(learner, other_learner):\n    if isinstance(learner, adaptive.Learner1D):\n        return learner.plot() + other_learner.plot()\n    else:\n        n = int(learner.npoints**0.5)\n        return (\n            (\n                other_learner.plot(n).relabel(\"Homogeneous grid\")\n                + learner.plot().relabel(\"With adaptive\")\n                + other_learner.plot(n, tri_alpha=0.4)\n                + learner.plot(tri_alpha=0.4)\n            )\n            .cols(2)\n            .options(hv.opts.EdgePaths(color=\"w\"))\n        )\n\n\ndef err(ys, ys_other):\n    abserr = np.abs(ys - ys_other)\n    return np.average(abserr**2) ** 0.5\n\ndef l1_norm_error(learner, other_learner):\n    if isinstance(learner, adaptive.Learner1D):\n        ys_interp = interp1d(*learner.to_numpy().T)\n        xs, _ = other_learner.to_numpy().T\n        ys = ys_interp(xs)  # interpolate the other learner's points\n        _, ys_other = other_learner.to_numpy().T\n        return err(ys, ys_other)\n    else:\n        xys = other_learner.to_numpy()[:, :2]\n        zs = learner.function(xys.T)\n        interpolator = learner.interpolator()\n        zs_interp = interpolator(xys)\n        # Compute the L1 norm error between the true function and the interpolator\n        return err(zs_interp, zs)\n\ndef run_and_plot(learner, **goal):\n    adaptive.runner.simple(learner, **goal)\n    homo_learner = homogeneous_learner(learner)\n    bms = benchmarks if isinstance(learner, adaptive.Learner1D) else benchmarks_2d\n    bm = {\n        \"npoints\": learner.npoints,\n        \"error\": l1_norm_error(learner, homo_learner),\n        \"uniform_error\": l1_norm_error(homo_learner, learner),\n    }\n    bm[\"error_ratio\"] = bm[\"uniform_error\"] / bm[\"error\"]\n    bms[learner.function.__name__] = bm\n    display(pd.DataFrame([bm]))  # noqa: F821\n    return plot(learner, homo_learner).relabel(\n        f\"{learner.function.__name__} function with {learner.npoints} points\"\n    )\n\ndef to_df(benchmarks):\n    df = pd.DataFrame(benchmarks).T\n    df.sort_values(\"error_ratio\", ascending=False, inplace=True)\n    return df\n\ndef plot_benchmarks(df, max_ratio: float = 1000, *, log_scale: bool = True):\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    df_hist = df.copy()\n\n    # Replace infinite values with 1000\n    df_hist.loc[np.isinf(df_hist.error_ratio), \"error_ratio\"] = max_ratio\n\n    # Convert the DataFrame index (function names) into a column\n    df_hist.reset_index(inplace=True)\n    df_hist.rename(columns={\"index\": \"function_name\"}, inplace=True)\n\n    # Create a list of colors based on the error_ratio values\n    bar_colors = [\"green\" if x > 1 else \"red\" for x in df_hist[\"error_ratio\"]]\n\n    # Create the bar chart\n    plt.figure(figsize=(12, 6))\n    plt.bar(df_hist[\"function_name\"], df_hist[\"error_ratio\"], color=bar_colors)\n\n    # Add a dashed horizontal line at 1\n    plt.axhline(y=1, linestyle=\"--\", color=\"gray\", linewidth=1)\n\n    if log_scale:\n        # Set the y-axis to log scale\n        plt.yscale(\"log\")\n\n    # Customize the plot\n    plt.xlabel(\"Function Name\")\n    plt.ylabel(\"Error Ratio (uniform Error / Learner Error)\")\n    plt.title(\"Error Ratio Comparison for Different Functions\")\n    plt.xticks(rotation=45)\n\n    # Show the plot\n    plt.show()\n```\n\n## Ask for Best Point\n\n```Python\ndef _ask_best_point(self):\n        assert self.tri is not None\n\n        loss, simplex, subsimplex = self._pop_highest_existing_simplex()\n\n        if subsimplex is None:\n            # We found a real simplex and want to subdivide it\n            points = self.tri.get_vertices(simplex)\n        else:\n            # We found a pending simplex and want to subdivide it\n            subtri = self._subtriangulations[simplex]\n            points = subtri.get_vertices(subsimplex)\n\n        point_new = tuple(choose_point_in_simplex(points, transform=self._transform))\n\n        self._pending_to_simplex[point_new] = simplex\n        self.tell_pending(point_new, simplex=simplex)  # O(??)\n\n        return point_new, loss\n```\n\n## Ask for Optimal Points\n\n```Python\ndef ask(self, n: int, tell_pending: bool = True) -> tuple[list[float], list[float]]:\n        \"\"\"Return 'n' points that are expected to maximally reduce the loss.\"\"\"\n        points, loss_improvements = self._ask_points_without_adding(n)\n\n        if tell_pending:\n            for p in points:\n                self.tell_pending(p)\n\n        return points, loss_improvements\n```\n\n## Parallelizing Function Components with Dask\n\n```python\ndef f(x):  # example function without caching\n    \"\"\"\n    Integer part of `x` repeats and should be reused\n    Decimal part requires a new computation\n    \"\"\"\n    return g(int(x)) + h(x % 1)\n\n\ndef g(x):\n    \"\"\"Slow but reusable function\"\"\"\n    from time import sleep\n\n    sleep(random.randrange(5))\n    return x**2\n\n\ndef h(x):\n    \"\"\"Fast function\"\"\"\n    return x**3\n```\n\n## API Documentation - Adaptive Utilities and Integrations\n\n```APIDOC\nadaptive.utils module\nadaptive.notebook_integration module\n```\n\n## Adaptive Learner Methods\n\n```Python\ndef ask(self, n: int, tell_pending: bool = True) -> tuple[list[int], list[Float]]:\n        points = list(range(self.n_requested, self.n_requested + n))\n\n        if any(p in self.data or p in self.pending_points for p in points):\n            # This means some of the points `< self.n_requested` do not exist.\n            points = list(\n                set(range(self.n_requested + n))\n                - set(self.data)\n                - set(self.pending_points)\n            )[:n]\n\n        loss_improvements = [self._loss_improvement(n) / n] * n\n        if tell_pending:\n            for p in points:\n                self.tell_pending(p)\n        return points, loss_improvements\n\ndef tell(self, n: Int, value: Real) -> None:\n        if n in self.data:\n            # The point has already been added before.\n            return\n\n        self.data[n] = value\n        self.pending_points.discard(n)\n        self.sum_f += value\n        self.sum_f_sq += value**2\n        self.npoints += 1\n\ndef tell_pending(self, n: int) -> None:\n        self.pending_points.add(n)\n\ndef remove_unfinished(self):\n\"\"\"Remove uncomputed data from the learner.\"\"\"\n        self.pending_points = set()\n```\n\n## Ask and Tell based on Loss Improvements\n\n```Python\ndef _ask_and_tell_based_on_loss_improvements(\n    self,\n    n: int\n) -> tuple[list[tuple[int, Any]], list[float]]:\n    selected = []  # tuples ((learner_index, point), loss_improvement)\n    total_points = [lrn.npoints + len(lrn.pending_points) for lrn in self.learners]\n    for _ in range(n):\n        to_select = []\n        for index, learner in enumerate(self.learners):\n            # Take the points from the cache\n            if index not in self._ask_cache:\n                self._ask_cache[index] = learner.ask(n=1, tell_pending=False)\n            points, loss_improvements = self._ask_cache[index]\n            to_select.append(\n                ((index, points[0]), (loss_improvements[0], -total_points[index]))\n            )\n\n        # Choose the optimal improvement.\n        (index, point), (loss_improvement, _) = max(to_select, key=itemgetter(1))\n        total_points[index] += 1\n        selected.append(((index, point), loss_improvement))\n        self.tell_pending((index, point))\n\n    points, loss_improvements = map(list, zip(*selected))\n    return points, loss_improvements\n```",
    "library": "adaptive_cache_config",
    "topic": "best-practices",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:31:14.324871"
}