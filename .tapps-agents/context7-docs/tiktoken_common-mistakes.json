{
  "library": "tiktoken",
  "topic": "common-mistakes",
  "documentation": {
    "content": "## Train Custom BPE Tokenizer with SimpleBytePairEncoding\n\n```python\nfrom tiktoken._educational import SimpleBytePairEncoding\n\n# Training data\ntraining_text = \"\"\"\nHello world! This is a sample text for training a tokenizer.\nMachine learning and natural language processing are fascinating.\nTokenization is an important step in text processing.\nByte pair encoding compresses text efficiently.\n\"\"\"\n\n# GPT-2 style pattern for splitting text\ngpt2_pattern = r\"'s|'t|'re|'ve|'m|'ll|'d| ?[\\p{L}]+| ?[\\p{N}]+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\n\n# Train a custom encoding\nprint(\"Training custom BPE encoding...\")\ncustom_enc = SimpleBytePairEncoding.train(\n    training_data=training_text,\n    vocab_size=400,  # Total vocabulary size (includes 256 byte tokens)\n    pat_str=gpt2_pattern\n)\n\n```\n\n## Efficient Tokenization with Tiktoken Numpy Integration\n\n```python\nimport tiktoken\n\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\n# Useful for ML pipelines where you need numpy arrays\nbatch_texts = [\"hello\", \"world\", \"test\"]\nbatch_arrays = [enc.encode_to_numpy(text) for text in batch_texts]\n\n# Can convert back to list for decoding\n# Assuming tokens_array is one of the numpy arrays from batch_arrays\ntokens_array = batch_arrays[0] # Example: taking the first one\ntokens_list = tokens_array.tolist()\ndecoded = enc.decode(tokens_list)\nprint(f\"Decoded: '{decoded}'\")\n\n# Efficient for large-scale tokenization\nlarge_text = \"This is a test. \" * 1000\ntokens_np = enc.encode_to_numpy(large_text)\nprint(f\"Large text: {len(tokens_np)} tokens in numpy array\")\n```\n\n## Tiktoken Error Handling and Edge Cases\n\n```python\nimport tiktoken\n\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\n# Handle empty strings\nempty_tokens = enc.encode(\"\")\nprint(f\"Empty string tokens: {empty_tokens}\")  # []\nassert enc.decode(empty_tokens) == \"\"\n\n# Handle unicode\nunicode_text = \"Hello \u4e16\u754c \ud83c\udf0d\"\nunicode_tokens = enc.encode(unicode_text)\nprint(f\"Unicode tokens: {unicode_tokens}\")\ndecoded_unicode = enc.decode(unicode_tokens)\nprint(f\"Decoded unicode: '{decoded_unicode}'\")\nassert decoded_unicode == unicode_text\n\n# UTF-8 decoding with different error modes\ntokens = enc.encode(\"test\")\n\n# Default: replace invalid UTF-8 with replacement character\ndecoded_replace = enc.decode(tokens, errors=\"replace\")\n\n# Strict: raise exception on invalid UTF-8\ntry:\n    decoded_strict = enc.decode(tokens, errors=\"strict\")\nexcept UnicodeDecodeError:\n    print(\"Strict mode raised error for invalid UTF-8\")\n\n# Ignore: skip invalid UTF-8\ndecoded_ignore = enc.decode(tokens, errors=\"ignore\")\n\n# Invalid token IDs raise KeyError\ntry:\n    enc.decode_single_token_bytes(9999999)\nexcept KeyError:\n    print(\"Invalid token ID handled\")\n```\n\n## BPE Tokenizer Training and Visualization (Python)\n\n```python\nfrom tiktoken._educational import *\n\n# Train a BPE tokeniser on a small amount of text\nenc = train_simple_encoding()\n\n# Visualise how the GPT-4 encoder encodes text\nenc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\nenc.encode(\"hello world aaaaaaaaaaaa\")\n```\n\n## Batch Processing for Encoding and Decoding\n\n```python\nimport tiktoken\n\nenc = tiktoken.get_encoding(\"o200k_base\")\n\n# Encode multiple texts in parallel\ntexts = [\n    \"hello world\",\n    \"goodbye world\",\n    \"python programming\",\n    \"machine learning\"\n]\n\n# Batch encode (uses multithreading internally)\nencoded_batch = enc.encode_batch(texts, num_threads=4)\nprint(f\"Encoded {len(encoded_batch)} texts\")\nfor i, tokens in enumerate(encoded_batch):\n    print(f\"Text {i}: {len(tokens)} tokens -> {tokens}\")\n\n# Batch encode ignoring special tokens (faster)\nencoded_ordinary = enc.encode_ordinary_batch(texts, num_threads=4)\n\n# Batch decode\ndecoded_batch = enc.decode_batch(encoded_batch, num_threads=4)\nfor i, text in enumerate(decoded_batch):\n    print(f\"Decoded {i}: {text}\")\n```\n\n## Handling Special Tokens in Encoding\n\n```python\nimport tiktoken\n\nenc = tiktoken.get_encoding(\"cl100k_base\")\n\n# By default, encoding special tokens raises an error\ntry:\n    enc.encode(\"<|endoftext|>\")\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n\n# Allow specific special tokens\ntokens = enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\nprint(f\"With special token: {tokens}\")  # [100257]\n\n# Allow all special tokens\ntokens = enc.encode(\"<|endoftext|>\", allowed_special=\"all\")\nprint(f\"All special: {tokens}\")  # [100257]\n\n# Treat special tokens as regular text\ntokens = enc.encode(\"<|endoftext|>\", disallowed_special=())\nprint(f\"As text: {tokens}\")  # [27, 91, 408, 1073, 728, 428, 91, 29]\n\n# Check available special tokens\nprint(f\"Special tokens: {enc.special_tokens_set}\")\n```\n\n## Visualize Tokenization Process with SimpleBytePairEncoding\n\n```python\nfrom tiktoken._educational import SimpleBytePairEncoding\nimport tiktoken\n\n# Convert a tiktoken encoding to educational format\nenc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\n\n# Encode with visualization (shows merge steps with colors)\nprint(\"Encoding 'hello world' with visualization:\")\ntokens = enc.encode(\"hello world\", visualise=\"colour\")\nprint(f\"\\nFinal tokens: {tokens}\")\n\n# Disable visualization\ntokens = enc.encode(\"hello world\", visualise=None)\nprint(f\"Tokens without visualization: {tokens}\")\n\n# Decode tokens\ndecoded = enc.decode(tokens)\nprint(f\"Decoded: {decoded}\")\n\n# See token breakdown\ntoken_bytes = enc.decode_tokens_bytes(tokens)\nprint(f\"Token bytes: {token_bytes}\")\n```\n\n## Custom Encoding Extension Setup (Python)\n\n```python\n# my_tiktoken_extension/tiktoken_ext/my_encodings.py\n# This module should contain a variable named ENCODING_CONSTRUCTORS\n# ENCODING_CONSTRUCTORS = { \"my_encoding_name\": lambda: tiktoken.Encoding(...) }\n\n# setup.py\nfrom setuptools import setup, find_namespace_packages\n\nsetup(\n    name=\"my_tiktoken_extension\",\n    packages=find_namespace_packages(include=['tiktoken_ext*']),\n    install_requires=[\"tiktoken\"],\n    ...\n)\n```\n\n## Analyze Encoding Efficiency Across Models\n\n```python\nimport tiktoken\n\n# Compare different encodings\ntext = \"The quick brown fox jumps over the lazy dog. \" * 10\n\nencodings = [\"gpt2\", \"cl100k_base\", \"o200k_base\"]\n\nfor enc_name in encodings:\n    enc = tiktoken.get_encoding(enc_name)\n    tokens = enc.encode(text)\n\n    byte_count = len(text.encode('utf-8'))\n    token_count = len(tokens)\n    bytes_per_token = byte_count / token_count\n    compression_ratio = byte_count / token_count\n\n    print(f\"\\n{enc_name}:\")\n    print(f\"  Text bytes: {byte_count}\")\n    print(f\"  Token count: {token_count}\")\n    print(f\"  Bytes per token: {bytes_per_token:.2f}\")\n    print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n\n    # Show first few tokens\n    first_tokens = tokens[:10]\n    token_texts = enc.decode_tokens_bytes(first_tokens)\n    print(f\"  First 10 tokens: {token_texts}\")\n```\n\n## List and Inspect Available Encodings with Tiktoken\n\n```python\nimport tiktoken\n\n# Get all available encoding names\nencoding_names = tiktoken.list_encoding_names()\nprint(f\"Available encodings: {encoding_names}\")\n# ['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base', 'o200k_harmony']\n\n# Load each encoding and inspect it\nfor name in encoding_names:\n    enc = tiktoken.get_encoding(name)\n    print(f\"\\nEncoding: {enc.name}\")\n    print(f\"Vocab size: {enc.n_vocab}\")\n    print(f\"Max token value: {enc.max_token_value}\")\n\n    # Test with sample text\n    sample = \"hello world\"\n    tokens = enc.encode(sample)\n    print(f\"'{sample}' -> {tokens}\")\n```",
    "library": "tiktoken",
    "topic": "best-practices",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-02-04T17:22:26.811858"
}