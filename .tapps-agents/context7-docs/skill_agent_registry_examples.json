{
  "library": "skill_agent_registry",
  "topic": "examples",
  "documentation": {
    "content": "## GenericDiffusersLoader Example\n\n```Python\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom invokeai.backend.model_manager import (\n    AnyModel,\n    BaseModelType,\n    ModelFormat,\n    ModelRepoVariant,\n    ModelType,\n    SubModelType,\n)\nfrom .. import ModelLoader, ModelLoaderRegistry\n\n\n@ModelLoaderRegistry.register(base=BaseModelType.Any, type=ModelType.CLIPVision, format=ModelFormat.Diffusers)\n@ModelLoaderRegistry.register(base=BaseModelType.Any, type=ModelType.T2IAdapter, format=ModelFormat.Diffusers)\nclass GenericDiffusersLoader(ModelLoader):\n    \"\"\"Class to load simple diffusers models.\"\"\"\n\n    def _load_model(\n        self,\n        model_path: Path,\n        model_variant: Optional[ModelRepoVariant] = None,\n        submodel_type: Optional[SubModelType] = None,\n    ) -> AnyModel:\n        model_class = self._get_hf_load_class(model_path)\n        if submodel_type is not None:\n            raise Exception(f\"There are no submodels in models of type {model_class}\")\n        variant = model_variant.value if model_variant else None\n        result: AnyModel = model_class.from_pretrained(model_path, torch_dtype=self._torch_dtype, variant=variant)  # type: ignore\n        return result\n```\n\n## Invocation Context Usage Example\n\n```Python\nclass MyInvocation(BaseInvocation):\n  ... # Other properties and methods\n  def invoke(self, context: InvocationContext) -> ImageOutput:\n      # Load an image using the ImagesInterface\n      image_pil = context.images.get_pil(self.image.image_name)\n      \n      # Perform operations on the image\n      output_image = do_something_cool(image_pil)\n      \n      # Save the processed image using the ImagesInterface\n      image_dto = context.images.save(output_image)\n      \n      # Log a message using the LoggerInterface\n      context.logger.info(f\"Did something cool, image saved!\")\n      \n      # Return the output, typically an ImageOutput object\n      return ImageOutput.build(image_dto)\n      ...\n\n```\n\n## InvokeAI Configuration Example (Memory DB)\n\n```yaml\nuse_memory_db: true\nscan_models_on_startup: true\n```\n\n## Python Event Handler Example\n\n```python\nfrom invokeai.app.services.download.download_job import DownloadJobBase\nfrom typing import Callable\n\ndef my_progress_handler(job: DownloadJobBase):\n    if job.status == \"running\":\n        print(f\"Downloading: {job.bytes}/{job.total_bytes} bytes\")\n    elif job.status == \"completed\":\n        print(\"Download complete!\")\n    elif job.status == \"error\":\n        print(f\"Download error: {job.error}\")\n\n# Assuming 'job' is an instance of DownloadJobBase or a subclass\n# job.add_event_handler(my_progress_handler)\n\n# To clear all handlers:\n# job.clear_event_handlers()\n```\n\n## Text-to-Latent Pipeline with Euler Ancestral Scheduler\n\n```bash\ncompel --prompt 'banana sushi' | compel | noise | t2l --scheduler heun --steps 3 --scheduler euler_a --link -3 conditioning positive_conditioning --link -2 conditioning negative_conditioning\n```\n\n## Text-to-Image Generation\n\n```bash\nt2i --positive_prompt 'banana sushi' --seed 42\n```\n\n## ModelManagerService Usage Example\n\n```Python\n# Retrieve the ModelManagerService instance\nmm = ApiDependencies.invoker.services.model_manager\n\n# Access the store (ModelRecordService) and retrieve a model by attribute\nconfigs = mm.store.get_model_by_attr(name='stable-diffusion-v1-5')\n```\n\n## Text-to-Latent Pipeline with DDIM Scheduler\n\n```bash\ncompel --prompt 'strawberry sushi' | compel | noise | t2l --scheduler heun --steps 3 --scheduler ddim --link -3 conditioning positive_conditioning --link -2 conditioning negative_conditioning | l2i\n```\n\n## InvokeAI Configuration File Example\n\n```yaml\n# Internal metadata - do not edit:\nschema_version: 4.0.2\n\n# Put user settings here - see https://invoke-ai.github.io/InvokeAI/features/CONFIGURATION/:\nhost: 0.0.0.0 # serve the app on your local network\nmodels_dir: D:\\\\invokeai\\\\models # store models on an external drive\nprecision: float16 # always use fp16 precision\n```\n\n## Test Model Loading and Inference\n\n```python\nimport pytest\nimport torch\n\nfrom invokeai.backend.model_management.models.base import BaseModelType, ModelType\nfrom invokeai.backend.util.test_utils import install_and_load_model\n\n@pytest.mark.slow\ndef test_model(model_installer, torch_device):\n    model_info = install_and_load_model(\n        model_installer=model_installer,\n        model_path_id_or_url=\"HF/dummy_model_id\",\n        model_name=\"dummy_model\",\n        base_model=BaseModelType.StableDiffusion1,\n        model_type=ModelType.Dummy,\n    )\n\n    dummy_input = build_dummy_input(torch_device)\n\n    with torch.no_grad(), model_info as model:\n        model.to(torch_device, dtype=torch.float32)\n        output = model(dummy_input)\n\n    # Validate output...\n\n```",
    "library": "skill_invoker",
    "topic": "examples",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:27:39.007300"
}