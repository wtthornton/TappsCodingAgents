{
  "library": "task_duration",
  "topic": null,
  "documentation": {
    "content": "## MMDetection Config System Overview\n\n```Python\n# Example config structure for Mask R-CNN\n# This is a conceptual representation, actual configs are more detailed.\nmodel = dict(\n    type='MaskRCNN',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=-1,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        norm_eval=False,\n        style='pytorch',\n        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')\n    ),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5\n    ),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_generator=dict(\n            type='AnchorGenerator',\n            octave_base_scale=8,\n            scales_per_octave=3,\n            ratios=[0.5, 1.0, 2.0],\n            aspect_ratios=[0.5, 1.0, 2.0]\n        ),\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[.0, .0, .0, .0],\n            target_stds=[1.0, 1.0, 1.0, 1.0]\n        ),\n        loss_cls=dict(\n            type='CrossEntropyLoss',\n            use_sigmoid=True,\n            loss_weight=1.0\n        ),\n        loss_bbox=dict(\n            type='L1Loss',\n            loss_weight=1.0\n        )\n    ),\n    roi_head=dict(\n        type='RoIHead',\n        bbox_head=dict(\n            type='Shared2FCBBoxHead',\n            in_channels=256,\n            fc_out_channels=1024,\n            num_classes=80,\n            bbox_coder=dict(\n                type='DeltaXYWHBBoxCoder',\n                target_means=[0., 0., 0., 0.],\n                target_stds=[0.1, 0.1, 0.1, 0.1]\n            ),\n            reg_class_agnostic=False,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0\n            ),\n            loss_bbox=dict(\n                type='L1Loss',\n                loss_weight=1.0\n            )\n        ),\n        mask_head=dict(\n            type='FCNMaskHead',\n            num_classes=80,\n            in_channels=256,\n            conv_cfg=dict(type='Conv2d'),\n            loss_mask=dict(\n                type='DiceLoss',\n                loss_weight=1.0\n            )\n        )\n    ),\n    # Model training and testing configurations\n    train_cfg=dict(\n        rpn=dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1\n            ),\n            sampler=dict(\n                type='RandomSampler',\n                num=256,\n                pos_fraction=0.5,\n                neg_pos_ratio=1,\n                neg_samples=32,\n                add_gt_as_neg=False\n            ),\n            allowed_border=-1,\n            pos_weight=-1,\n            debug=False\n        ),\n        rpn_proposal=dict(\n            nms_across_levels=False,\n            nms_pre=2000,\n            nms_post=1000,\n            max_num=1000,\n            nms_thr=0.7,\n            min_bbox_size=0\n        ),\n        rcnn=dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                match_low_quality=False,\n                ignore_iof_thr=-1\n            ),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ratio=1,\n                neg_samples=32,\n                add_gt_as_neg=False\n            ),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False\n        )\n    ),\n    test_cfg=dict(\n        rpn=dict(\n            nms_across_levels=False,\n            nms_pre=2000,\n            nms_post=1000,\n            max_num=1000,\n            nms_thr=0.7,\n            min_bbox_size=0\n        ),\n        rcnn=dict(\n            # soft-nms, modified IoU threshold for NMS\n            # nms_thr=0.5,\n            # distance_threshold=0.5\n            # Use standard NMS\n            nms_thr=0.5\n        )\n    )\n)\n\n# Dataset configuration example\ndataset_type = 'CocoDataset'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53],\n    std=[58.395, 57.12, 57.375],\n    to_rgb=True)\n\ntrain_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n]\n\nvalidation_pipeline = [\n    dict(type='LoadImageFromFile'),\n    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n    dict(type='RandomFlip', flip_ratio=0.5),\n    dict(type='Normalize', **img_norm_cfg),\n    dict(type='Pad', size_divisor=32),\n    dict(type='DefaultFormatBundle'),\n    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n]\n\n\ndata = dict(\n    samples_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file='path/to/your/annotations/train.json',\n        img_prefix='path/to/your/images/',\n        pipeline=train_pipeline),\n    val=dict(\n        type=dataset_type,\n        ann_file='path/to/your/annotations/val.json',\n        img_prefix='path/to/your/images/',\n        pipeline=validation_pipeline),\n    test=dict(\n        type=dataset_type,\n        ann_file='path/to/your/annotations/test.json',\n        img_prefix='path/to/your/images/',\n        pipeline=validation_pipeline))\n\n# Optimizer configuration example\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n\n# Learning rate scheduler configuration example\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0/3.0,\n    step=[8, 11])\n\n# Runtime configuration example\nrunner = dict(type='EpochBasedRunner', max_epochs=12)\n\n# Checkpoint saving configuration example\ncheckpoint_config = dict(\n    interval=1)\n\n# Logging configuration example\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n\n# Enable invisible CUDA devices\nvisible_cuda_devices = ['cuda:0']\n\n# Disable deterministic mode for faster training\n Cudnn_deterministic = False\n\n# Set random seed for reproducibility\nseed = 0\n\n# Set up distributed training if needed\n dist_params = dict(backend='nccl')\n\n```\n\n## Support Evaluation on Multiple Datasets\n\n```Python\nfrom mmdet.datasets import ConcatDataset\n\n# Example usage with ConcatDataset\n# dataset1 = CocoDataset(...)\n# dataset2 = LvisDataset(...)\n# combined_dataset = ConcatDataset([dataset1, dataset2])\n\n```\n\n## Download Example Cat Images\n\n```shell\ncd&&cd&&\n```\n\n## Update Guide About Model Deployment\n\n```Markdown\n## Model Deployment Guide\n\nThis guide details how to deploy your trained MMDetection models efficiently.\n\n### ONNX Export\n\nExport your model to ONNX format for broader compatibility:\n```bash\npython tools/deployment/convert_to_onnx.py <config_file> <checkpoint_file> --output-file model.onnx\n```\n```\n\n## Update get_started Guide\n\n```Markdown\n# Getting Started with MMDetection\n\n## Installation\n\n1. **Clone the repository:**\n   ```bash\n   git clone https://github.com/open-mmlab/mmdetection.git\n   cd mmdetection\n   ```\n2. **Install MMDetection:**\n   ```bash\n   pip install -v .  # install editable mode\n   ```\n```\n\n## Update Deployment Guide\n\n```Markdown\n## Deploying MMDetection Models\n\nThis guide covers deploying your trained MMDetection models to various platforms. \n\n### TorchServe Deployment\n\n1. Install TorchServe.\n2. Convert your model to TorchScript.\n3. Create a `mar` file.\n4. Serve the model using TorchServe CLI.\n```\n\n## MMDetection DataSample Properties\n\n```Python\n    @property\n    def proposals(self) -> InstanceData:\n        return self._proposals\n\n    @proposals.setter\n    def proposals(self, value: InstanceData):\n        self.set_field(value, '_proposals', dtype=InstanceData)\n\n    @proposals.deleter\n    def proposals(self):\n        del self._proposals\n\n    @property\n    def gt_instances(self) -> InstanceData:\n        return self._gt_instances\n\n    @gt_instances.setter\n    def gt_instances(self, value: InstanceData):\n        self.set_field(value, '_gt_instances', dtype=InstanceData)\n\n    @gt_instances.deleter\n    def gt_instances(self):\n        del self._gt_instances\n\n    @property\n    def pred_instances(self) -> InstanceData:\n        return self._pred_instances\n\n    @pred_instances.setter\n    def pred_instances(self, value: InstanceData):\n        self.set_field(value, '_pred_instances', dtype=InstanceData)\n\n    @pred_instances.deleter\n    def pred_instances(self):\n        del self._pred_instances\n\n    # directly add ``pred_track_instances`` in ``DetDataSample``\n    # so that the ``TrackDataSample`` does not bother to access the\n    # instance-level information.\n    @property\n    def pred_track_instances(self) -> InstanceData:\n        return self._pred_track_instances\n\n    @pred_track_instances.setter\n    def pred_track_instances(self, value: InstanceData):\n        self.set_field(value, '_pred_track_instances', dtype=InstanceData)\n\n    @pred_track_instances.deleter\n    def pred_track_instances(self):\n        del self._pred_track_instances\n\n    @property\n    def ignored_instances(self) -> InstanceData:\n        return self._ignored_instances\n\n    @ignored_instances.setter\n    def ignored_instances(self, value: InstanceData):\n        self.set_field(value, '_ignored_instances', dtype=InstanceData)\n\n    @ignored_instances.deleter\n    def ignored_instances(self):\n        del self._ignored_instances\n\n    @property\n    def gt_panoptic_seg(self) -> PixelData:\n        return self._gt_panoptic_seg\n\n    @gt_panoptic_seg.setter\n    def gt_panoptic_seg(self, value: PixelData):\n        self.set_field(value, '_gt_panoptic_seg', dtype=PixelData)\n\n    @gt_panoptic_seg.deleter\n    def gt_panoptic_seg(self):\n        del self._gt_panoptic_seg\n\n    @property\n    def pred_panoptic_seg(self) -> PixelData:\n        return self._pred_panoptic_seg\n\n    @pred_panoptic_seg.setter\n    def pred_panoptic_seg(self, value: PixelData):\n        self.set_field(value, '_pred_panoptic_seg', dtype=PixelData)\n\n    @pred_panoptic_seg.deleter\n    def pred_panoptic_seg(self):\n        del self._pred_panoptic_seg\n\n    @property\n    def gt_sem_seg(self) -> PixelData:\n        return self._gt_sem_seg\n\n    @gt_sem_seg.setter\n    def gt_sem_seg(self, value: PixelData):\n        self.set_field(value, '_gt_sem_seg', dtype=PixelData)\n\n    @gt_sem_seg.deleter\n    def gt_sem_seg(self):\n        del self._gt_sem_seg\n\n    @property\n    def pred_sem_seg(self) -> PixelData:\n        return self._pred_sem_seg\n\n    @pred_sem_seg.setter\n    def pred_sem_seg(self, value: PixelData):\n        self.set_field(value, '_pred_sem_seg', dtype=PixelData)\n\n    @pred_sem_seg.deleter\n    def pred_sem_seg(self):\n        del self._pred_sem_seg\n\n\n\nSampleList = List[DetDataSample]\nOptSampleList = Optional[SampleList]\n\n```\n\n## Reorganize Requirements and Make Albumentations Optional\n\n```Python\nReorganize requirements and make albumentations and imagecorruptions optional. (#1969)\n```\n\n## Enhance AssignResult and SamplingResult\n\n```Python\nEnhance AssignResult and SamplingResult. (#1995)\n```\n\n## Stable support of exporting PointRend to ONNX with dynamic shapes\n\n```Python\nPointRend\n```\n\n## Stable support of exporting PointRend to ONNX with dynamic shapes\n\n```Python\nONNX export\n```\n\n## Stable support of exporting PointRend to ONNX with dynamic shapes\n\n```Python\ndynamic shapes\n```",
    "library": "network_detection",
    "topic": "overview",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:28:13.304891"
}