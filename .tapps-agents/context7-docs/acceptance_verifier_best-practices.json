{
  "library": "acceptance_verifier",
  "topic": "best-practices",
  "documentation": {
    "content": "## Run Continuation Quality Evaluation (Bash)\n\n```bash\nuv run vf-eval continuation-quality\n```\n\n## Define State Structure for Rollouts (Python)\n\n```python\nfrom typing import Dict, List, Optional, Any\n\n# Assuming other necessary types like Messages, ModelResponse, etc. are imported\n# from their respective modules as shown in previous examples.\n\nclass State(dict):\n    \"\"\"\n    Dict subclass with forwarding for INPUT_FIELDS.\n    \n    Accessing state[\"prompt\"] forwards to state[\"input\"].prompt if \"input\" exists,\n    otherwise accesses directly from dict (backward compat).\n    \"\"\"\n    INPUT_FIELDS = [\"prompt\", \"answer\", \"task\", \"info\", \"example_id\"]\n    \n    # Required: input fields (always in \"input\" RolloutInput)\n    input: RolloutInput\n    \n    # Created during rollout\n    is_completed: bool\n    stop_condition: str | None\n    oai_tools: list[ChatCompletionToolParam]\n    trajectory: list[TrajectoryStep]  # Always a list (never None)\n    completion: Messages | None       # Full completion (rendered from trajectory)\n    reward: float | None               # Final reward for rollout\n    advantage: float | None            # Final advantage for rollout\n    metrics: dict[str, float] | None   # Additional metrics\n    timing: RolloutTiming | None       # Timing info\n    \n    # Custom fields (env-specific, added by subclasses)\n    # Examples:\n    # \"prompt_too_long\": bool,         # For max length handling\n    # \"sandbox_id\": str,                # For SandboxEnv\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # Initialize required fields that are always present\n        self.is_completed = False\n        self.trajectory = []\n        self.stop_condition = None\n        self.oai_tools = []\n        self.completion = None\n        self.reward = None\n        self.advantage = None\n        self.metrics = None\n        self.timing = None\n\n    def __setattr__(self, name, value):\n        # Basic forwarding for backward compatibility if 'input' is set\n        if name in self.INPUT_FIELDS and 'input' in self and hasattr(self['input'], name):\n            setattr(self['input'], name, value)\n        else:\n            super().__setattr__(name, value)\n\n    def __getattr__(self, name):\n        # Basic forwarding for backward compatibility if 'input' is set\n        if name in self.INPUT_FIELDS and 'input' in self and hasattr(self['input'], name):\n            return getattr(self['input'], name)\n        else:\n            try:\n                return super().__getattr__(name)\n            except AttributeError:\n                # Handle cases where attribute might not exist yet, e.g., during initialization\n                # or if it's meant to be set later.\n                # You might want to raise a more specific error or return a default value\n                # depending on the expected behavior.\n                raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\n```\n\n## Multi-Turn Rollout Example with Trajectories (Python)\n\n```python\n# Multi-turn environment automatically tracks trajectories\nenv = vf.load_environment(\"wordle\")\n\n# Create input from dataset row\nrow = env.dataset[0]\ninput = {\n    \"prompt\": row[\"prompt\"],\n    \"answer\": row.get(\"answer\", \"\"),\n    \"task\": row.get(\"task\", \"default\"),\n    \"info\": row.get(\"info\", {}),\n    \"example_id\": row[\"example_id\"],\n}\n\n# Run rollout\nstate = await env.rollout(input, client, model=\"gpt-4o-mini\")\n```\n\n## Single-Turn Evaluation Example (Python)\n\n```python\nimport verifiers as vf\nfrom openai import AsyncOpenAI\n\n# Load environment (works as before)\nenv = vf.load_environment(\"gsm8k\")\n\n# Run evaluation (no changes needed)\nclient = AsyncOpenAI()\nresults = await env.evaluate(client, model=\"gpt-4o-mini\", num_examples=100)\nprint(f\"Average reward: {results.metadata.avg_reward}\")\n\n# Access trajectory from results\nfor state in results.state:\n    print(f\"Trajectory steps: {len(state['trajectory'])}\")\n    for step in state[\"trajectory\"]:\n        print(f\"  Prompt: {step['prompt']}\")\n        print(f\"  Completion: {step['completion']}\")\n```\n\n## Configure Model and Sampling for Evaluation (Bash)\n\n```bash\nuv run vf-eval continuation-quality   -m gpt-4.1-mini   -n 20 -r 3 -t 1024 -T 0.7   -a '{\"key\": \"value\"}'  # env-specific args as JSON\n```\n\n## Test Order Independence with Chat Completions in Python\n\n```python\n# Set up multiple mappings\nclient.add_chat_response(\n    messages=[{\"role\": \"user\", \"content\": \"Question A\"}],\n    response=\"Answer A\"\n)\nclient.add_chat_response(\n    messages=[{\"role\": \"user\", \"content\": \"Question B\"}],\n    response=\"Answer B\"\n)\n\n# Test in different orders\nresponse_a = await client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Question A\"}]\n)\nresponse_b = await client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Question B\"}]\n)\n\n# Test reverse order\nresponse_b2 = await client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Question B\"}]\n)\nresponse_a2 = await client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Question A\"}]\n)\n\n# All responses are consistent\nassert response_a.choices[0].message.content == response_a2.choices[0].message.content\nassert response_b.choices[0].message.content == response_b2.choices[0].message.content\n```\n\n## Evaluate an Environment\n\n```bash\nuv run vf-eval environment-name -s # run and save eval results locally\n```\n\n## Dataset Info Metadata Structure\n\n```python\nInfo = dict[str, Any]\n\n# Dataset row with info dict:\n{\n    \"prompt\": \"Solve this problem\",\n    \"info\": {\n        \"answer\": \"42\",              # Required: ground truth\n        \"difficulty\": \"medium\",      # Optional metadata\n        \"source\": \"textbook\",\n        \"chapter\": 3,\n        \"requires_tool\": True\n    }\n}\n\n# Access in reward functions:\ndef reward_func(completion, answer, info=None, **kwargs):\n    difficulty = info.get(\"difficulty\", \"unknown\") if info else \"unknown\"\n    # Adjust scoring based on difficulty...\n```\n\n## Run Self-Reward Evaluation with Default Settings\n\n```bash\nuv run vf-eval self-reward -a '{\"dataset_name\": \"your/dataset\", \"model_name\": \"Qwen/Qwen3-0.6B\"}'\n```\n\n## Run Quick Evaluation with OpenAI Models\n\n```bash\nuv run vf-eval wordle -m gpt-5-nano\n```",
    "library": "acceptance_verifier",
    "topic": "overview",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T16:10:38.165227"
}