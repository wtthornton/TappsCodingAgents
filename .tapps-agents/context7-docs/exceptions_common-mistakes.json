{
  "library": "exceptions",
  "topic": "common-mistakes",
  "documentation": {
    "content": "## Run Basic Experiments with Shell Script\n\n```bash\nbash runs/run_latest.sh\n```\n\n## CoE Iterative Processing Mechanism (Mathematical Formula)\n\n```mathematica\nx^{(0)} = x\n\nx^{(t)} = \\sum_{i=1}^{K/C} g_{t,i} \\cdot \\text{FFN}_i(x^{(t-1)}) + \\mathbb{I}_r \\cdot x^{(t-1)}, \\quad t = 1, 2, ..., C\n\ny = x^{(C)}\n```\n\n## Previous MoE Output Representation (Mathematical Formula)\n\n```mathematica\ny = \\sum_{i=1}^K g_i \\cdot E_i(x)\n\ng_i = \\begin{cases}\n s_i, & s_i \\in \\text{TopK}(s_j|1 \\leq j \\leq K, N) \\\\\n 0, & \\text{otherwise}\n\\end{cases}\n\ns_i = \\text{Softmax}(u_i^\\top e_i)\n```\n\n## Load Multiple Parquet Files into Dataset (Python)\n\n```python\nfrom coe.utils.dataset.base_dataset import BaseDataset\n\n# Combine multiple data sources\ndataset = BaseDataset(\n    parquet_files=[\n        'data/metamathqa/train.parquet',\n        'data/gsm8k/train.parquet',\n        'data/additional_math/train.parquet'\n    ],\n    tokenizer='gpt2',\n    text_keys=[['problem', 'solution']],\n    max_length=1024,\n    truncation='right'\n)\n\nprint(f\"Total samples: {len(dataset)}\")\n\n# Expected output:\n# Loading Parquet files...\n# Loaded 50000 records (file 1)\n# Loaded 7473 records (file 2)\n# Loaded 25000 records (file 3)\n# Combined: 82473 total records\n# Processing text fields...\n# Total samples: 82473\n```\n\n## Save and Load HuggingFace Model Checkpoints with FSDP\n\n```python\n# Save checkpoint during training\nfrom torch.distributed.fsdp import FullStateDictConfig, StateDictType\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Assuming self.fsdp_model, self.config, and self.device_mesh are available within a class context\n# self.fsdp_model: The FSDP-wrapped model\n# self.config: Configuration object containing trainer.default_local_dir\n# self.device_mesh: PyTorch distributed device mesh object\n\ndef save_checkpoint(self, step):\n    cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n    with FSDP.state_dict_type(self.fsdp_model, StateDictType.FULL_STATE_DICT, cfg):\n        state_dict = self.fsdp_model.state_dict()\n\n    path = f\"{self.config.trainer.default_local_dir}/global_step_{step}\"\n\n    if self.device_mesh.get_rank() == 0:\n        # Assuming self.model is the base HuggingFace model and self.tokenizer is the tokenizer\n        self.model.save_pretrained(path, state_dict=state_dict)\n        self.tokenizer.save_pretrained(path)\n        print(f\"Checkpoint saved: {path}\")\n\n    torch.distributed.barrier() # Ensure all ranks synchronize before proceeding\n\n# --- Example Usage for Loading and Inference ---\n\n# Load checkpoint for inference\nmodel_path = \"output/global_step_10000\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\" # Automatically maps layers to available devices\n)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Generate text\nprompt = \"Question: What is 25 * 16? Answer:\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\") # Assuming CUDA is available\noutputs = model.generate(**inputs, max_length=100, temperature=0.7)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n```\n\n## CoE Variant with Shared Gating (Mathematical Formulation)\n\n```math\nx^{(0)} = x\n\nx^{(t)} = \\sum_{i=1}^{K/C} g_i \\cdot \\text{FFN}_i(x^{(t-1)}) + I_r \\cdot x^{(t-1)}, \\quad t = 1, 2, ..., C\n\ny = x^{(C)}\n```\n\n## Python Expert Utilization Statistics Tracking\n\n```python\nimport torch\nfrom collections import defaultdict\n```\n\n## Train CoE Model with Custom Configuration (Bash)\n\n```bash\n# Set up the environment\nexport PYTHONPATH=/path/to/coe:$PYTHONPATH\nexport CUDA_VISIBLE_DEVICES=\"0,1,2,3\"\nexport MASTER_PORT=29501\n\n# Train CoE with 2 iterations, 4 experts per token, 64 total experts\ntorchrun --master_port=$MASTER_PORT --nproc_per_node=4 main.py \\\n    model.override_config.num_experts_per_tok=4 \\\n    model.override_config.inner_iter=2 \\\n    model.override_config.n_routed_experts=63 \\\n    model.override_config.n_shared_experts=1 \\\n    model.override_config.inner_residual=true \\\n    model.override_config.outer_residual=false \\\n    model.override_config.use_igate=true \\\n    trainer.total_training_steps=10000 \\\n    trainer.experiment_name=coe-64ept-2iter-4topk \\\n    trainer.validation_interval_steps=100 \\\n    trainer.save_interval_steps=1000 \\\n    trainer.project_name=coe-experiments \\\n    trainer.logger='[\"console\",\"wandb\"]' \\\n    data.train_files=data/metamathqa/train.parquet \\\n    data.val_files=data/metamathqa/test.parquet \\\n    data.max_length=512 \\\n    data.train_batch_size=256 \\\n    data.micro_batch_size_per_gpu=64\n\n# Expected output:\n# Total training steps: 10000\n# MODEL TOTAL PARAMS: 544000000\n# Using sequence parallel size: 1\n# Step 100/10000: train/loss=1.25, train/lr=0.00028, val/loss=1.20\n# Step 1000/10000: train/loss=1.15, val/loss=1.12\n# Checkpoint saved to: ./outputs/coe-64ept-2iter-4topk/global_step_1000\n```\n\n## Track expert usage across layers and iterations\n\n```python\nfrom collections import defaultdict\nimport torch\nimport numpy as np\n\n# Assuming tokenizer, texts, model, and config are already defined and loaded\n# tokenizer = ...\n# texts = [...] # List of input texts\n# model = ... # Your CoE model\n# config = ... # Your model configuration\n\nexpert_counts = defaultdict(lambda: defaultdict(int))\n\nfor sample_idx in range(1000):\n    input_ids = tokenizer.encode(texts[sample_idx], return_tensors=\"pt\").to(\"cuda\")\n\n    with torch.no_grad():\n        _ = model(input_ids)\n\n    # Collect expert indices for each layer\n    for layer in range(config.num_hidden_layers):\n        for iteration in range(2):\n            # This assumes expert_indices are saved to disk for each layer and iteration\n            # You might need to adjust the path or how you access these indices based on your setup\n            try:\n                expert_indices = torch.load(\n                    f\"outputs/routing_logits/layer_{layer}/iter_{iteration}_topk_idx.pt\"\n                )\n\n                for expert_id in expert_indices.cpu().numpy():\n                    expert_counts[layer][(iteration, expert_id)] += 1\n            except FileNotFoundError:\n                print(f\"Warning: File not found for layer {layer}, iteration {iteration}. Skipping.\")\n                continue # Skip to next iteration if file not found\n\n# Analyze load balancing\nfor layer in range(config.num_hidden_layers):\n    # Ensure we have counts for all possible experts (0 to 62, assuming 63 experts)\n    # If an expert was never chosen, its count will be 0 due to defaultdict(int)\n    iter0_counts = [expert_counts[layer][(0, i)] for i in range(63)]\n    iter1_counts = [expert_counts[layer][(1, i)] for i in range(63)]\n\n    print(f\"Layer {layer}:\")\n    print(f\"  Iter 0 - Max: {max(iter0_counts)}, Min: {min(iter0_counts)}, \"\n          f\"Std: {np.std(iter0_counts):.2f}\")\n    print(f\"  Iter 1 - Max: {max(iter1_counts)}, Min: {min(iter1_counts)}, \"\n          f\"Std: {np.std(iter1_counts):.2f}\")\n\n```\n\n## Create Custom Dataset with Tokenization (Python)\n\n```python\n# coe/utils/dataset/base_dataset.py\nfrom coe.utils.dataset.base_dataset import BaseDataset\nfrom transformers import AutoTokenizer\n\n# Initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Create dataset with multiple text keys\ndataset = BaseDataset(\n    parquet_files=['data/metamathqa/train.parquet'],\n    tokenizer=tokenizer,\n    text_keys=[['query', 'response']],  # Combines query and response\n    max_length=512,\n    truncation='right'  # Options: 'right', 'left', 'error'\n)\n\n# Access dataset items\nsample = dataset[0]\nprint(f\"Input IDs shape: {sample['input_ids'].shape}\")\nprint(f\"Attention mask shape: {sample['attention_mask'].shape}\")\nprint(f\"Position IDs shape: {sample['position_ids'].shape}\")\nprint(f\"Loss mask shape: {sample['loss_mask'].shape}\")\n\n# Expected output:\n# Loaded 395000 records\n# Processed 395000 texts\n# Input IDs shape: torch.Size([512])\n# Attention mask shape: torch.Size([512])\n# Position IDs shape: torch.Size([512])\n# Loss mask shape: torch.Size([512])\n\n# Each item returns a dictionary with:\n# - input_ids: Token IDs (padded/truncated to max_length)\n# - attention_mask: 1 for real tokens, 0 for padding\n# - position_ids: Position indices for rotary embeddings\n# - loss_mask: Mask for computing loss (same as attention_mask)\n```",
    "library": "experts",
    "topic": "common-mistakes",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:25:33.260345"
}