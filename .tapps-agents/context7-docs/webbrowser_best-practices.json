{
  "library": "webbrowser",
  "topic": "best-practices",
  "documentation": {
    "content": "## Multimodal Support with @browser-ai/core\n\n```typescript\nimport { streamText } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\n\nconst result = streamText({\n  model: browserAI(),\n  messages: [\n    { // Image\n      role: \"user\",\n      content: [\n        { type: \"text\", text: \"What's in this image?\" },\n        { type: \"file\", mediaType: \"image/png\", data: base64ImageData },\n      ],\n    },\n    { // Audio\n      role: \"user\",\n      content: [{ type: \"file\", mediaType: \"audio/mp3\", data: audioData }],\n    },\n  ],\n});\n\nfor await (const chunk of result.textStream) {\n  console.log(chunk);\n}\n```\n\n## Unified Text Streaming with @browser-ai\n\n```typescript\nimport { streamText } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\n// or: import { transformersJS } from \"@browser-ai/transformers-js\";\n// or: import { webLLM } from \"@browser-ai/web-llm\";\n\nconst result = streamText({\n  model: browserAI(), // change model provider here\n  prompt: \"Why is the sky blue?\",\n});\n```\n\n## Tool Calling with Structured Output using ToolLoopAgent\n\n```typescript\nimport { Output, ToolLoopAgent, tool } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\nimport { z } from \"zod\";\n\nconst agent = new ToolLoopAgent({\n  model: browserAI(),\n  tools: {\n    weather: tool({\n      description: \"Get the weather in a location\",\n      inputSchema: z.object({ city: z.string() }),\n      execute: async ({ city }) => {\n        // ...\n      },\n    }),\n  },\n  output: Output.object({\n    schema: z.object({\n      summary: z.string(),\n      temperature: z.number(),\n      recommendation: z.string(),\n    }),\n  }),\n});\n\nconst { output } = await agent.generate({\n  prompt: \"What is the weather in San Francisco and what should I wear?\",\n});\n```\n\n## Track Download Progress for Browser AI Models\n\n```typescript\nimport { streamText } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\n\nconst model = browserAI();\nconst availability = await model.availability();\n\nif (availability === \"unavailable\") {\n  console.log(\"Browser doesn't support built-in AI\");\n  return;\n}\n\nif (availability === \"downloadable\") {\n  await model.createSessionWithProgress((progress) => {\n    console.log(`Download progress: ${Math.round(progress * 100)}%`);\n  });\n}\n\n// Model is ready\nconst result = streamText({\n  model,\n  prompt: 'Invent a new holiday and describe its traditions.',\n});\n```\n\n## Basic Streaming Text Generation with WebLLM\n\n```typescript\nimport { streamText, generateText, tool, stepCountIs, Output } from \"ai\";\nimport { webLLM, doesBrowserSupportWebLLM, WebWorkerMLCEngineHandler } from \"@browser-ai/web-llm\";\nimport { z } from \"zod\";\n\n// Check WebGPU support\nif (!doesBrowserSupportWebLLM()) {\n  console.log(\"Browser doesn't support WebGPU\");\n}\n\n// Basic streaming text generation\nconst result = streamText({\n  model: webLLM(\"Qwen3-0.6B-q0f16-MLC\"),\n  prompt: \"Invent a new holiday and describe its traditions.\",\n});\n\nfor await (const chunk of result.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\n## Structured JSON Output Generation with generateText\n\n```typescript\nimport { generateText } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\nimport { z } from \"zod\";\n\nconst { output } = await generateText({\n  model: browserAI(),\n  output: Output.object({\n    schema: z.object({\n      recipe: z.object({\n        name: z.string(),\n        ingredients: z.array(\n          z.object({ name: z.string(), amount: z.string() }),\n        ),\n        steps: z.array(z.string()),\n      }),\n    }),\n  }),\n  prompt: \"Generate a lasagna recipe.\",\n});\n```\n\n## Structured Output Generation with WebLLM\n\n```typescript\n// Structured output generation\nconst { output } = await generateText({\n  model: webLLM(\"Qwen3-0.6B-q0f16-MLC\"),\n  output: Output.object({\n    schema: z.object({\n      summary: z.string(),\n      keyPoints: z.array(z.string()),\n    }),\n  }),\n  prompt: \"Summarize the benefits of exercise.\",\n});\n```\n\n## Generate Text Embeddings with @browser-ai/core\n\n```typescript\nimport { embed, embedMany } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\n\n// Single embedding\nconst { embedding, usage } = await embed({\n  model: browserAI.embedding(\"embedding\"),\n  value: \"Hello, world!\",\n});\n\n// Multiple embeddings\nconst { embedding, usage } = await embedMany({\n  model: browserAI.embedding(\"embedding\"),\n  values: [\"Hello\", \"World\", \"AI\"],\n});\n```\n\n## Stream Text with Browser AI and Zod Schema\n\n```typescript\nimport { streamText } from \"ai\";\nimport { browserAI } from \"@browser-ai/core\";\nimport { z } from \"zod\";\n\nconst { partialOutputStream } = streamText({\n  model: browserAI(),\n  output: Output.object({\n    schema: z.object({\n      recipe: z.object({\n        name: z.string(),\n        ingredients: z.array(\n          z.object({ name: z.string(), amount: z.string() }),\n        ),\n        steps: z.array(z.string()),\n      }),\n    }),\n  }),\n  prompt: 'Generate a lasagna recipe.',\n});\n```\n\n## Browser AI Core with Server Fallback (TypeScript)\n\n```typescript\nimport { browserAI, doesBrowserSupportBrowserAI } from \"@browser-ai/core\";\nimport { ClientSideChatTransport, DefaultChatTransport } from \"@browser-ai/core\"; // Assuming these are exported from core\n\n// Prioritize local inference, fall back to cloud when needed\nconst { messages, sendMessage } = useChat({\n  transport: doesBrowserSupportBrowserAI()\n    ? new ClientSideChatTransport(browserAI())\n    : new DefaultChatTransport({ api: \"/api/chat\" }),\n});\n```",
    "library": "webbrowser",
    "topic": "overview",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T09:59:48.461522"
}