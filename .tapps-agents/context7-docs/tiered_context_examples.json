{
  "library": "tiered_context",
  "topic": "examples",
  "documentation": {
    "content": "## Manage Library Metadata with LibraryStore\n\n```python\nfrom deepcontext.store import LibraryStore, derive_library_id, library_id_to_collection_name\n\n# Initialize library store\nlibrary_store = LibraryStore(\n    host=\"localhost\",\n    port=6333\n)\n\n# Derive library ID from source\nlibrary_id = derive_library_id(\"vercel/next.js\", \"repo\")\nprint(f\"Library ID: {library_id}\")  # Output: /vercel/next.js\n\n# Convert library ID to collection name\ncollection_name = library_id_to_collection_name(library_id)\nprint(f\"Collection: {collection_name}\")  # Output: lib_vercel_next_js\n\n# List all libraries\nlibraries = library_store.list_libraries(limit=50)\nfor lib in libraries:\n    print(f\"- {lib.library_id} ({lib.name})\")\n    print(f\"  Collection: {lib.collection_name}\")\n    print(f\"  Source: {lib.source}\")\n    print(f\"  Chunks: {lib.chunks_count}\")\n\n# Get specific library\nlibrary = library_store.get_library(\"/vercel/next.js\")\nif library:\n    print(f\"Found library: {library.name}\")\n    print(f\"Documents: {library.documents_count}\")\n    print(f\"Chunks: {library.chunks_count}\")\n```\n\n## Python: Fetch Content from Multiple Sources\n\n```python\nfrom deepcontext import ContentFetcher\n\n# Basic usage with context manager\nwith ContentFetcher() as fetcher:\n    # Local markdown file\n    doc = fetcher.fetch(\"./docs/README.md\")\n    print(f\"Title: {doc.title}\")\n    print(f\"Content length: {len(doc.content)}\")\n\n    # GitHub file\n    doc = fetcher.fetch(\"https://github.com/vercel/next.js/blob/canary/docs/api-reference/components/image.md\")\n    print(f\"Source: {doc.source_type}\")\n\n    # Webpage\n    doc = fetcher.fetch(\"https://nextjs.org/docs/app/building-your-application/routing\")\n    print(f\"Metadata: {doc.metadata}\")\n\n# With custom timeout\nfetcher = ContentFetcher(timeout=60.0)\ndoc = fetcher.fetch(\"https://example.com/slow-page\")\nfetcher.close()\n\n# Confluence with authentication\nimport os\nos.environ[\"CONFLUENCE_EMAIL\"] = \"you@company.com\"\nos.environ[\"CONFLUENCE_TOKEN\"] = \"your_token\"\n\nwith ContentFetcher() as fetcher:\n    doc = fetcher.fetch(\"https://company.atlassian.net/wiki/spaces/DOCS/pages/123456\")\n    print(f\"Confluence page: {doc.title}\")\n    print(f\"Version: {doc.metadata.get('version')}\")\n\n```\n\n## ContentFetcher - Fetch from Multiple Sources\n\n```APIDOC\n## Python API - ContentFetcher - Fetch from Multiple Sources\n\nFetch content from local files, GitHub, webpages, and Confluence.\n\n```python\nfrom deepcontext import ContentFetcher\n\n# Basic usage with context manager\nwith ContentFetcher() as fetcher:\n    # Local markdown file\n    doc = fetcher.fetch(\"./docs/README.md\")\n    print(f\"Title: {doc.title}\")\n    print(f\"Content length: {len(doc.content)}\")\n\n    # GitHub file\n    doc = fetcher.fetch(\"https://github.com/vercel/next.js/blob/canary/docs/api-reference/components/image.md\")\n    print(f\"Source: {doc.source_type}\")\n\n    # Webpage\n    doc = fetcher.fetch(\"https://nextjs.org/docs/app/building-your-application/routing\")\n    print(f\"Metadata: {doc.metadata}\")\n\n# With custom timeout\nfetcher = ContentFetcher(timeout=60.0)\ndoc = fetcher.fetch(\"https://example.com/slow-page\")\nfetcher.close()\n\n# Confluence with authentication\nimport os\nos.environ[\"CONFLUENCE_EMAIL\"] = \"you@company.com\"\nos.environ[\"CONFLUENCE_TOKEN\"] = \"your_token\"\n\nwith ContentFetcher() as fetcher:\n    doc = fetcher.fetch(\"https://company.atlassian.net/wiki/spaces/DOCS/pages/123456\")\n    print(f\"Confluence page: {doc.title}\")\n    print(f\"Version: {doc.metadata.get('version')}\")\n```\n```\n\n## Python: Fetch Entire Websites\n\n```python\nfrom deepcontext import ContentFetcher\n\nwith ContentFetcher() as fetcher:\n    # Use sitemap automatically\n    docs = fetcher.fetch_website(\n        base_url=\"https://ui.shadcn.com/docs\",\n        max_pages=100\n    )\n    print(f\"Fetched {len(docs)} pages from sitemap\")\n\n    # With URL pattern filter (regex)\n    docs = fetcher.fetch_website(\n        base_url=\"https://nextjs.org/docs\",\n        max_pages=50,\n        url_pattern=r\"/docs/app/\",\n        use_sitemap=True\n    )\n\n    # Force crawling instead of sitemap with progress callback\n    def progress(url, current, total):\n        print(f\"[{current}/{total}] Fetching {url[:60]}...\")\n\n    docs = fetcher.fetch_website(\n        base_url=\"https://example.com/docs\",\n        max_pages=30,\n        use_sitemap=False,\n        progress_callback=progress\n    )\n\n    for doc in docs:\n        print(f\"- {doc.title} ({len(doc.content)} chars)\")\n\n```\n\n## Ingest Content via CLI\n\n```bash\n# From a local markdown file\nuv run deepcontext ingest ./docs/README.md\n\n# From a GitHub URL\nuv run deepcontext ingest https://github.com/vercel/next.js/blob/canary/contributing/core/developing.md\n\n# From a webpage\nuv run deepcontext ingest https://nextjs.org/docs/app/building-your-application/routing/middleware\n\n# From a Confluence page (requires auth)\nexport CONFLUENCE_EMAIL=you@company.com\nexport CONFLUENCE_TOKEN=your_api_token\nuv run deepcontext ingest https://company.atlassian.net/wiki/spaces/DOCS/pages/123456/Page-Title\n\n# From an entire Confluence space\nuv run deepcontext ingest-confluence https://company.atlassian.net/wiki DOCS --limit 50\n\n# From an entire website (uses sitemap or crawls links)\nuv run deepcontext ingest-website https://ui.shadcn.com/docs --max-pages 50\n```\n\n## Queue Website for Ingestion\n\n```APIDOC\n## POST /queue-website\n\n### Description\nQueues a website for background ingestion. This is the queued version of `deepcontext ingest-website`.\n\n### Method\nPOST\n\n### Endpoint\n`/queue-website <url>`\n\n### Parameters\n#### Path Parameters\n- **url** (string) - Required - The base URL of the website to ingest.\n\n#### Query Parameters\n- **max-pages** (integer) - Optional - Maximum number of pages to fetch. Defaults to 100.\n- **pattern** (string) - Optional - A URL pattern (regex) to filter pages.\n- **no-sitemap** (boolean) - Optional - If set, prevents the command from trying to use `sitemap.xml` and forces link crawling.\n- **collection** (string) - Optional - Qdrant collection name. Defaults to \"deepcontext_chunks\".\n- **host** (string) - Optional - Qdrant host. Defaults to \"localhost\".\n- **port** (integer) - Optional - Qdrant port. Defaults to 6333.\n\n### Request Example\n```bash\nuv run deepcontext queue-website https://example.com/docs\n```\n\n### Response\nPrints a **job ID** that can be used with `deepcontext jobs` and `deepcontext job`.\n(Details on success/error responses would typically be here, but are not provided in the source text.)\n```\n\n## Ingest Data from GitHub, Websites, and Confluence (Python)\n\n```python\nfrom deepcontext.retrieval import ContentFetcher\nfrom deepcontext.indexing import chunker\n\n# Initialize ContentFetcher\nwith ContentFetcher() as fetcher:\n    all_chunks = []\n\n    # Fetch GitHub repository\n    github_docs = fetcher.fetch_github_repo(\n        repo=\"vercel/next.js\",\n        path=\"docs\",\n        extensions=[ \".md\", \".mdx\" ]\n    )\n    print(f\"Fetched {len(github_docs)} files from GitHub\")\n\n    # Fetch website\n    website_docs = fetcher.fetch_website(\n        base_url=\"https://ui.shadcn.com/docs\",\n        max_pages=50\n    )\n    print(f\"Fetched {len(website_docs)} pages from website\")\n\n    # Fetch Confluence space\n    confluence_docs = fetcher.fetch_confluence_space(\n        base_url=\"https://company.atlassian.net/wiki\",\n        space_key=\"TEAM\",\n        limit=25\n    )\n    print(f\"Fetched {len(confluence_docs)} pages from Confluence\")\n\n    # Chunk all documents\n    for doc in github_docs + website_docs + confluence_docs:\n        try:\n            chunks = chunker.chunk_document(doc)\n            all_chunks.extend(chunks)\n        except Exception as e:\n            print(f\"Error chunking {doc.title}: {e}\")\n\n    print(f\"\\nCreated {len(all_chunks)} total chunks\")\n```\n\n## Filter VectorStore Searches and Manage Collections\n\n```python\nfrom deepcontext import VectorStore\nfrom deepcontext.models import SourceType\n\nstore = VectorStore(collection_name=\"docs\")\n\n# Search with filters\nresults = store.search(\n    query=\"async functions\",\n    limit=10,\n    source_type=SourceType.GITHUB,\n    language=\"typescript\"\n)\n\nprint(f\"Found {len(results)} TypeScript examples from GitHub\")\n\n# Get collection statistics\nstats = store.get_stats()\nprint(f\"Collection: {stats['collection_name']}\")\nprint(f\"Total points: {stats['points_count']}\")\nprint(f\"Indexed vectors: {stats['indexed_vectors_count']}\")\nprint(f\"Status: {stats['status']}\")\n\n# Delete chunks by source\ndeleted = store.delete_by_source(\"https://example.com/old-docs\")\nprint(f\"Deleted {deleted} chunks from old source\")\n\n# Delete by document ID\ndeleted = store.delete_by_document_id(\"abc123def456\")\nprint(f\"Deleted {deleted} chunks from document\")\n\n# Clear entire collection\nstore.clear()\nprint(\"Collection cleared and recreated\")\n\nstore.close()\n```\n\n## Perform Semantic Search and Display Results (Python)\n\n```python\nfrom deepcontext.vector_store import VectorStore # Assuming VectorStore is already initialized and 'store' is available\n\n# Perform searches\nqueries = [\n    \"how to configure middleware\",\n    \"button component example\",\n    \"deployment best practices\"\n]\n\nfor query in queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"Query: {query}\")\n    print('='*60)\n\n    # Assuming 'store' is an initialized VectorStore instance\n    results = store.search(query, limit=3)\n\n    for i, result in enumerate(results, 1):\n        print(f\"\\n{i}. {result.chunk.title} (Score: {result.score:.4f})\")\n        print(f\"   Source: {result.chunk.source}\")\n        print(f\"   {result.chunk.description[:150]}...\")\n\n# Cleanup (assuming 'store' is the VectorStore instance)\n# store.close()\n# library_store.close() # Assuming library_store is also initialized\n```\n\n## Ingest Website\n\n```APIDOC\n## POST /ingest-website\n\n### Description\nIngests all pages from a website using sitemap or link crawling. It first attempts to find and parse `sitemap.xml`, falling back to crawling links from the base URL if no sitemap is found. URLs are filtered to remain under the base path.\n\n### Method\nPOST\n\n### Endpoint\n`/ingest-website <url>`\n\n### Parameters\n#### Path Parameters\n- **url** (string) - Required - The base URL of the website to ingest.\n\n#### Query Parameters\n- **max-pages** (integer) - Optional - Maximum number of pages to fetch. Defaults to 100.\n- **pattern** (string) - Optional - A URL pattern (regex) to filter pages.\n- **no-sitemap** (boolean) - Optional - If set, prevents the command from trying to use `sitemap.xml` and forces link crawling.\n- **collection** (string) - Optional - Qdrant collection name. Defaults to \"deepcontext_chunks\".\n- **host** (string) - Optional - Qdrant host. Defaults to \"localhost\".\n- **port** (integer) - Optional - Qdrant port. Defaults to 6333.\n\n### Request Example\n```bash\n# Ingest from a website, automatically using sitemap\nuv run deepcontext ingest-website https://ui.shadcn.com/docs\n\n# Ingest with a URL pattern filter\nuv run deepcontext ingest-website https://nextjs.org/docs --pattern \"/docs/app/\"\n\n# Force crawling instead of using sitemap\nuv run deepcontext ingest-website https://example.com/docs --no-sitemap\n```\n\n### Response\n(Details on success/error responses would typically be here, but are not provided in the source text.)\n```",
    "library": "tiered_context",
    "topic": "usage",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:25:50.073421"
}