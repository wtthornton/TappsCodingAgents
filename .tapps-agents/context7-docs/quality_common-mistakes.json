{
  "library": "quality",
  "topic": "common-mistakes",
  "documentation": {
    "content": "## Streaming Source Configuration Overview\n\n```APIDOC\nstreams:\n  # Configuration for streaming sources goes here\n  # Example:\n  # my_streaming_source:\n  #   format: \"kafka\"\n  #   topic: \"my_topic\"\n  #   # ... other Kafka specific parameters ...\n  #   windowBy: \"eventTime\"\n  #   # ... other common parameters ...\n\n# Supported streaming sources:\n# - File sources\n# - Kafka sources\n\n# Kafka specific notes for streaming:\n# - startingOffsets: default is 'latest'\n# - endingOffsets: ignored (all new records processed until stopped)\n\n# Additional required parameter for all streaming sources:\n# windowBy: Source of timestamp for streaming windows and late record skipping.\n#   Applicable only for streaming jobs!\n#   Options:\n#     - processingTime: Uses current timestamp when Spark processes the record.\n#     - eventTime: Uses a 'timestamp' column (Timestamp type) from the source.\n#     - custom(columnName): Uses a user-defined column (Timestamp type) or SQL expression evaluating to Timestamp.\n```\n\n## Virtual Sources Configuration Example\n\n```HOCON\njobConfig: {\n  virtualSources: [\n    {\n      id: \"sqlVS\"\n      kind: \"sql\"\n      description: \"Filter data for specific date only\"\n      parentSources: [\"hive_source_1\"]\n      persist: \"disk_only\"\n      save: {\n        kind: \"orc\"\n        path: \"some/path/to/vs/location\"\n      }\n      query: \"select id, name, entity, description from hive_source_1 where load_date == '2023-06-30'\"\n      metadata: [\n        \"source.owner=some.preson@some.domain\"\n        \"critical.source=false\"\n      ]\n    }\n    {\n      id: \"joinVS\"\n      kind: \"join\"\n      parentSources: [\"hdfs_avro_source\", \"hdfs_orc_source\"]\n      joinBy: [\"id\"]\n      joinType: \"leftouter\"\n      persist: \"memory_only\"\n      keyFields: [\"id\", \"order_id\"]\n    }\n    {\n      id: \"filterVS\"\n      kind: \"filter\"\n      parentSources: [\"kafka_source\"]\n      expr: [\"key is not null\"]\n      keyFields: [\"orderId\", \"dttm\"]\n    }\n    {\n      id: \"selectVS\"\n      kind: \"select\"\n      parentSources: [\"table_source_1\"]\n      expr: [\n        \"count(id) as id_cnt\",\n        \"sum(amount) as total_amount\"\n      ]\n    }\n    {\n      id: \"aggVS\"\n      kind: \"aggregate\"\n      parentSources: [\"hdfs_fixed_file\"]\n      groupBy: [\"col1\"]\n      expr: [\n        \"avg(col2) as avg_col2\",\n        \"sum(col3) as sum_col3\"\n      ],\n      keyFields: [\"col1\", \"avg_col2\", \"sum_col3\"]\n    }\n  ]\n}\n```\n\n## Configure Trend Checks\n\n```APIDOC\nchecks: {\n  trend: {\n    averageBoundFull: [\n      {\n        id: \"avg_bal_check\",\n        description: \"Check that average balance stays within +/-25% of the week average\"\n        metric: \"avro_file1_avg_bal\",\n        rule: \"datetime\"\n        windowSize: \"8d\"\n        threshold: 0.25\n      }\n    ],\n    averageBoundUpper: [\n      {id: \"avg_pct_null\", metric: \"pct_of_null\", rule: \"datetime\", windowSize: \"15d\", threshold: 0.5}\n    ],\n    averageBoundLower: [\n      {id: \"avg_distinct\", metric: \"fixed_file_dist_name\", rule: \"record\", windowSize: 31, threshold: 0.3}\n    ],\n    averageBoundRange: [\n      {\n        id: \"avg_inn_match\",\n        metric: \"table_source1_inn_regex\",\n        rule: \"datetime\",\n        windowSize: \"8d\",\n        thresholdLower: 0.2,\n        thresholdUpper: 0.4\n      }\n    ],\n    topNRank: [\n      {id: \"top2_curr_match\", metric: \"filterVS_top3_currency\", targetNumber: 2, threshold: 0.1}\n    ]\n  }\n}\n```\n\n## Example Environment Variables\n\n```Environment Variables\nDQ_STORAGE_PASSOWRD\ndqMattermostToken\n```\n\n## Result Target Configuration Examples\n\n```yaml\nresults:\n  - type: file\n    resultTypes:\n      - regularMetrics\n      - composedMetrics\n    save:\n      path: \"/path/to/results/output.json\"\n      format: \"json\"\n\n  - type: hive\n    resultTypes:\n      - loadChecks\n      - checks\n    schema: \"dq_results_schema\"\n    table: \"dq_checks_table\"\n\n  - type: kafka\n    resultTypes:\n      - jobState\n    connection: \"kafka_connection_id\"\n    topic: \"dq_results_topic\"\n    options:\n      - \"compression.type=gzip\"\n```\n\n## Configure Summary Reporting Targets\n\n```APIDOC\ntargets: {\n  summary: {\n    email: {\n      attachMetricErrors: true\n      metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n      dumpSize: 10\n      recipients: [\"some.person@some.domain\"]\n    },\n    mattermost: {\n      attachMetricErrors: true\n      metrics: [\"hive_table_nulls\", \"fixed_file_dist_name\", \"table_source1_inn_regex\"]\n      dumpSize: 10\n      recipients: [\"@someUser\", \"#someChannel\"]\n    },\n    kafka: {\n      connection: \"kafka_broker\"\n      topic: \"dev.dq_results.topic\"\n    }\n  }\n}\n```\n\n## Markdown Template Parameter Substitution\n\n```Mustache\nThis {{ parameterName }} has a value of {{ parameterValue }}\n```\n\n## Configure Results Targets\n\n```APIDOC\ntargets: {\n  results: {\n    file: {\n      resultTypes: [\"checks\", \"loadChecks\"]\n      save: {\n        kind: \"delimited\"\n        path: ${basePath}\"/results/\"${referenceDate}\n        header: true\n      }\n    },\n    hive: {\n      resultTypes: [\"regularMetrics\", \"composedMetrics\", \"loadChecks\", \"checks\"],\n      schema: \"DQ_SCHEMA\",\n      table: \"DQ_TARGETS\"\n    },\n    kafka: {\n      resultTypes: [\"regularMetrics\", \"composedMetrics\", \"loadChecks\", \"checks\"],\n      connection: \"kafka_broker\"\n      topic: \"some.topic\"\n    }\n  }\n}\n```\n\n## Example Application Configuration File\n\n```HOCON\nappConfig: {\n\n  applicationName: \"Custom Data Quality Application Name\"\n  \n  dateTimeOptions: {\n    timeZone: \"GMT+3\"\n    referenceDateFormat: \"yyyy-MM-dd\"\n    executionDateFormat: \"yyyy-MM-dd-HH-mm-ss\"\n  }\n\n  enablers: {\n    allowSqlQueries: false\n    allowNotifications: true\n    aggregatedKafkaOutput: true\n  }\n\n  defaultSparkOptions: [\n    \"spark.sql.orc.enabled=true\"\n    \"spark.sql.parquet.compression.codec=snappy\"\n    \"spark.sql.autoBroadcastJoinThreshold=-1\"\n  ]\n\n  storage: {\n    dbType: \"postgres\"\n    url: \"localhost:5432/public\"\n    username: \"postgres\"\n    password: \"postgres\"\n    schema: \"dqdb\"\n    saveErrorsToStorage: true\n  }\n\n  email: {\n    host: \"smtp.some-company.domain\"\n    port: \"25\"\n    username: \"emailUser\"\n    password: \"emailPassword\"\n    address: \"some.service@some-company.domain\"\n    name: \"Data Quality Service\"\n    sslOnConnect: true\n  }\n\n  mattermost: {\n    host: \"https://some-team.mattermost.com\"\n    token: ${dqMattermostToken}\n  }\n\n  encryption: {\n    secret: \"secretmustbeatleastthirtytwocharacters\"\n    keyFields: [\"password\", \"username\", \"url\"]\n    encryptErrorData: true\n  }\n}\n\n```\n\n## Mattermost Notification Configuration\n\n```APIDOC\nMattermost Configuration:\n  Parameters:\n    host: string - Mattermost API host URL.\n    token: string - Mattermost API token (Bot account token recommended).\n  Notes:\n    The 'mattermost' section must be present in the application configuration.\n    If missing, notifications cannot be sent, and an exception may be thrown.\n```",
    "library": "quality",
    "topic": "best-practices",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T16:10:28.423580"
}