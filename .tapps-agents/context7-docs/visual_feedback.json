{
  "library": "visual_feedback",
  "topic": null,
  "documentation": {
    "content": "## System 2 EBT with Causal Replay Buffer\n\n```python\nfrom model.replay_buffer import CausalReplayBuffer\nfrom model.nlp.ebt import EBT_NLP\n\n# Initialize EBT with replay buffer for System 2 thinking\nhparams = {\n    'tokenizer': 'EleutherAI/gpt-neox-20b',\n    'embedding_dim': 768,\n    'num_transformer_blocks': 12,\n    'multiheaded_attention_heads': 12,\n    'mcmc_step_size': 500.0,\n    'mcmc_num_steps': 5,  # More steps for System 2\n    'ebt_type': 'time_embed',\n    'mcmc_step_size_learnable': True,\n    'mcmc_replay_buffer': True,\n    'mcmc_replay_buffer_size': 10000,\n    'mcmc_replay_buffer_sample_bs_percent': 0.5,  # 50% from buffer\n    'batch_size_per_device': 32,\n    'execution_mode': 'pretrain',\n    # ... other S2 params\n}\n\nmodel = EBT_NLP(hparams)\n\n# Replay buffer stores past optimization trajectories\n# During training, 50% of batch uses fresh data, 50% from buffer\nfor batch_idx, batch in enumerate(train_loader):\n    metrics = model.forward_loss_wrapper(batch, phase=\"train\")\n    # Replay buffer automatically samples and provides logits to forward()\n    # This enables the model to learn from past reasoning traces\n\nprint(f\"Replay buffer size: {model.replay_buffer.current_size}\")\n```\n\n## Load and Pretokenize RedPajama Dataset\n\n```python\nfrom data.nlp.pajama_dataloader import RedPajamaDataset\nfrom transformers import AutoTokenizer\n\n# Initialize dataset with pretokenization\nhparams = {\n    'execution_mode': 'pretrain',\n    'context_length': 256,\n    'dataset_dir': '',  # Uses HF_HOME env variable\n    'tokenizer': 'EleutherAI/gpt-neox-20b',\n    'pretokenize_dataset': True,\n    'dataset_name': 'pajama',\n    'num_workers': 12,\n    'num_gpus': 4,\n    'batch_size_per_device': 32\n}\n\ndataset = RedPajamaDataset(hparams)\n\n# Access tokenized samples\nsample = dataset[0]\nprint(f\"Input IDs shape: {sample['input_ids'].shape}\")\nprint(f\"Attention mask shape: {sample['attention_mask'].shape}\")\nprint(f\"Dataset length: {len(dataset)}\")\n\n# Dataset automatically caches preprocessed version to disk\n# Location: $HF_HOME/pajama_preprocessed/EleutherAI_gpt-neox-20b/max_length_257/\n```\n\n## Run Minimalistic NLP Training Loop\n\n```python\npython example_code/minimal_nlp_training_loop.py\n```\n\n## Install FFmpeg via APT (Linux)\n\n```bash\napt install ffmpeg\n```\n\n## Run Inference on Language Understanding Benchmarks\n\n```bash\npython train_model.py \\\n  --run_name ebt-xxs-inference-lambada \\\n  --modality \"NLP\" \\\n  --model_name \"ebt\" \\\n  --model_size \"xxs\" \\\n  --tokenizer \"EleutherAI/gpt-neox-20b\" \\\n  --normalize_initial_condition \\\n  --ebt_type \"time_embed\" \\\n  --denoising_initial_condition \"random_noise\" \\\n  --context_length 256 \\\n  --gpus \"-1\" \\\n  --batch_size_per_device 8 \\\n  --dataset_name \"lambada\" \\\n  --num_workers 12 \\\n  --wandb_project \"nlp_inference_accuracy\" \\\n  --execution_mode \"inference\" \\\n  --infer_ebt_advanced \\\n  --infer_langevin_dynamics_noise 1.0 \\\n  --infer_ebt_num_steps 2 \\\n  --only_test \\\n  --only_test_model_ckpt \"checkpoints/ebt-xxs-epoch=10.ckpt\" \\\n  --infer_max_gen_len 128 \\\n  --infer_topp 0.9 \\\n  --infer_temp 0.8 \\\n  --set_matmul_precision \"medium\"\n```\n\n## EBT Model Forward Pass with Energy Optimization (Python)\n\n```python\nfrom model.nlp.ebt import EBT_NLP\nimport torch\n\n# Initialize EBT model\nhparams = {\n    'tokenizer': 'EleutherAI/gpt-neox-20b',\n    'embedding_dim': 384,\n    'num_transformer_blocks': 6,\n    'multiheaded_attention_heads': 6,\n    'mcmc_step_size': 500.0,\n    'mcmc_step_size_lr_multiplier': 1500.0,\n    'mcmc_num_steps': 2,\n    'ebt_type': 'time_embed',\n    'normalize_initial_condition': True,\n    'denoising_initial_condition': 'random_noise',\n    'mcmc_step_size_learnable': True,\n    'no_mcmc_detach': False,\n    'weight_initialization_method': 'xavier',\n    'weight_initialization_gain': 1.0,\n    'execution_mode': 'pretrain',\n    'debug_unused_parameters': False\n}\n\nmodel = EBT_NLP(hparams)\n\n# Forward pass with energy optimization\ninput_ids = torch.randint(0, 50277, (2, 256))  # batch_size=2, seq_len=256\npredicted_distributions, predicted_energies = model(input_ids, learning=True)\n\n# predicted_distributions: list of [B*S, V] tensors for each MCMC step\n# predicted_energies: list of energy values showing optimization progress\nprint(f\"Final distribution shape: {predicted_distributions[-1].shape}\")\nprint(f\"Energy progression: {[e.mean().item() for e in predicted_energies]}\")\n```\n\n## Baseline Transformer Forward Pass (Python)\n\n```python\nfrom model.nlp.baseline_transformer import Baseline_Transformer_NLP\nimport torch\n\n\n```\n\n## Minimal PyTorch Lightning Training Loop for EBT\n\n```python\nimport torch\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom data.nlp.pajama_dataloader import RedPajamaDataset\nfrom data.nlp.collator import NLP_HF_Collator\nfrom model.nlp.baseline_transformer import Baseline_Transformer_NLP\nfrom model.nlp.ebt import EBT_NLP\n\nclass ModelWrapper(pl.LightningModule):\n    def __init__(self, hparams):\n        super().__init__()\n        self.save_hyperparameters(hparams)\n\n        model_cls = {\n            \"baseline_transformer\": Baseline_Transformer_NLP,\n            \"ebt\": EBT_NLP,\n        }[self.hparams.model_name]\n\n        self.model = model_cls(self.hparams)\n        self.dataset = RedPajamaDataset(self.hparams)\n        self.collate_fn = NLP_HF_Collator(self.hparams)\n\n    def training_step(self, batch, batch_idx):\n        metrics = self.model.forward_loss_wrapper(batch, \"train\")\n        loss = metrics[\"loss\"]\n        self.log_dict({f\"train_{k}\": v for k, v in metrics.items()}, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.model.parameters(), lr=self.hparams.lr)\n\n    def train_dataloader(self):\n        workers = torch.cuda.device_count() * self.hparams.num_workers_per_gpu\n        return DataLoader(\n            self.dataset,\n            batch_size=self.hparams.batch_size_per_device,\n            shuffle=True,\n            num_workers=workers,\n            collate_fn=self.collate_fn,\n        )\n\n# Train EBT model\nhparams = {\n    'lr': 1e-3,\n    'batch_size_per_device': 32,\n    'num_workers_per_gpu': 12,\n    'max_steps': 100000,\n    'dataset_dir': '',\n    'dataset_name': 'pajama',\n    'context_length': 256,\n    'pretokenize_dataset': True,\n    'tokenizer': 'EleutherAI/gpt-neox-20b',\n    'model_name': 'ebt',\n    'embedding_dim': 384,\n    'num_transformer_blocks': 6,\n    'multiheaded_attention_heads': 6,\n    'ffn_dim_multiplier': 1,\n    'weight_initialization_method': 'xavier',\n    'weight_initialization_gain': 1.0,\n    'execution_mode': 'pretrain',\n    'mcmc_step_size': 500.0,\n    'mcmc_step_size_lr_multiplier': 1500.0,\n    'mcmc_num_steps': 2,\n    'ebt_type': 'time_embed',\n    'normalize_initial_condition': True,\n    'denoising_initial_condition': 'random_noise',\n    'mcmc_step_size_learnable': True,\n    'no_mcmc_detach': False,\n    'debug_unused_parameters': False\n}\n\nmodel = ModelWrapper(hparams)\nlogger = WandbLogger(name=\"ebt_training\", project=\"nlp_pretrain\", entity=\"\")\n\ntrainer = pl.Trainer(\n    max_steps=hparams['max_steps'],\n    devices=-1,\n    logger=logger,\n    enable_model_summary=True,\n    enable_checkpointing=True,\n)\ntrainer.fit(model)\n```\n\n## Learning Rate Scheduling with Warmup and Cosine Annealing\n\n```python\nfrom optimization import WarmUpCosineAnnealingLR, exclude_bias_and_norm\nimport torch.optim as optim\nimport torch\n\n# Separate parameters for weight decay\nparam_groups = exclude_bias_and_norm(\n    model,\n    weight_decay=0.01,\n    skip_list=['alpha', 'langevin_dynamics_noise_std']  # Don't decay special params\n)\n\noptimizer = optim.AdamW(param_groups, lr=0.0012, betas=(0.9, 0.95))\n\n# Warmup + cosine annealing scheduler\nscheduler = WarmUpCosineAnnealingLR(\n    optimizer,\n    warm_up_steps=10000,\n    max_steps=1000000,\n    min_lr_scale=10,  # min_lr = peak_lr / 10\n    max_scheduling_steps=1000000\n)\n\n# Training loop\nfor step in range(1000000):\n    loss = model.training_step(batch, step)\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n\n    if step % 100 == 0:\n        print(f\"Step {step}, LR: {scheduler.get_last_lr()[0]:.6f}, Loss: {loss.item():.4f}\")\n```\n\n## Train EBT for Video Prediction on Kinetics-400\n\n```shell\npython train_model.py \\\n  --run_name ebt-vid-k400 \\\n  --modality \"VID\" \\\n  --model_name \"ebt_vid\" \\\n  --model_size \"s\" \\\n  --dataset_name \"k400\" \\\n  --image_dims 128 128 \\\n  --num_frames 16 \\\n  --backbone_type \"vae\" \\\n  --mcmc_step_size 1.0 \\\n  --mcmc_num_steps 3 \\\n  --batch_size_per_device 16 \\\n  --peak_learning_rate 5e-5 \\\n  --max_steps 1000000 \\\n  --gpus \"-1\" \\\n  --num_workers 8 \\\n  --wandb_project \"vid_prediction\"\n```",
    "library": "external_feedback_models",
    "topic": "overview",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:31:54.147542"
}