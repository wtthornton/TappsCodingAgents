{
  "library": "requirements_tracer",
  "topic": "common-mistakes",
  "documentation": {
    "content": "## Python: Policy Optimization with Trace\n\n```python\ndef optimize_policy(\n    env_name,\n    horizon,\n    memory_size=5,\n    n_optimization_steps=100,\n    seed=0,\n    relative=True,\n    verbose=False,\n    model=\"gpt-4-0125-preview\",\n):\n\n    @trace.bundle(trainable=True)\n    def controller(obs):\n        \"\"\"\n        A feedback controller that computes the action based on the observation.\n\n        Args:\n            obs: (dict) The observation from the environment. Each key is a string (indicating a type of observation) and the value is a list of floats.\n        Output:\n            action: (list or nd.array) A 4-dimensional vector.\n        \"\"\"\n        return [0, 0, 0, 0]\n\n    config_list = config_list_from_json(\"OAI_CONFIG_LIST\")\n    config_list = [config for config in config_list if config[\"model\"] == model]\n    optimizer = OptoPrime(controller.parameters(), config_list=config_list, memory_size=memory_size)\n\n    env = TracedEnv(env_name, seed=seed, relative=relative)\n\n    successes = []\n    returns = []\n    print(\"Optimization Starts\")\n    for i in range(n_optimization_steps):\n        env.init()\n        traj, error = rollout(env, horizon, controller)\n\n        if error is None:\n            feedback = f\"Success: {traj['success'][-1]}\\nReturn: {sum(traj['reward'])}\"\n            target = traj[\"observation\"][-1][\"observation\"]\n\n            successes.append(traj[\"success\"][-1])\n            returns.append(sum(traj[\"reward\"]))\n        else:\n            feedback = error.exception_node.create_feedback()\n            target = error.exception_node\n```\n\n## Execute Policy Optimization for Robot Control\n\n```python\nsuccesses, returns = optimize_policy(\n    env_name=\"llf-metaworld-pick-place-v2\",\n    horizon=10,\n    n_optimization_steps=30,\n    memory_size=5,\n    seed=0,\n    relative=True,\n    verbose='output',\n    model=\"gpt-4-0125-preview\"\n)\n```\n\n## Execute Policy Optimization with Trace Package\n\n```python\nsuccesses, returns = optimize_policy(\n    env_name=\"llf-metaworld-pick-place-v2\",\n    horizon=10,\n    n_optimization_steps=30,\n    memory_size=5,\n    seed=0,\n    relative=True,\n    verbose='output',\n    model=\"gpt-4-0125-preview\"\n)\n```\n\n## Prepare Instruction and Perform Optimization Steps\n\n```python\n# Add instruction from the LLFbench environment, which contains\n# information about the action space and problem background. The original instruction says\n# obsrvaiton is a json string. But here we've parsed it as a dict so we\n# update the instruction.\ninstruction = traj[\"observation\"][0][\"instruction\"].data\ninfix = \"You will get observations of the robot state \"\nprefix, suffix = instruction.split(infix)\nkeys = \", \".join(traj[\"observation\"][0][\"observation\"].data.keys())\nsuffix = suffix.replace(\"json strings.\", f\"dict, where the keys are {keys}.\")\n\n# Add an task specific explanation; as the original instruction says\n# only it's a pick-place task, which is too vague. We clarify the task.\nassert env_name in [\"llf-metaworld-pick-place-v2\", \"llf-metaworld-reach-v2\"]\nif env_name == \"llf-metaworld-pick-place-v2\":\n    hint = prefix + \"The goal of the task is to pick up a puck and put it to a goal position. \" + infix + suffix\nelse:\n    hint = prefix + infix + suffix\n\noptimizer.objective = hint + optimizer.default_objective\n\noptimizer.zero_feedback()\noptimizer.backward(target, feedback)\noptimizer.step(verbose=verbose)\n\nprint(f\"Iteration: {i}, Feedback: {feedback}, Parameter: {controller.parameter.data}\")\n\nsuccesses.append(traj[\"success\"][-1])\nreturns.append(sum(traj[\"reward\"]))\nprint(\"Final Returns:\", sum(traj[\"reward\"]))\nreturn successes, returns\n```\n\n## Define Core Functions and Setup for Optimization\n\n```python\nimport opto\nfrom opto.trace import bundle, node\nfrom opto.optimizers import OptoPrime\nfrom opto.trace.nodes import GRAPH\n\ndef blackbox(x):\n    return -x * 2\n\n@bundle()\ndef bar(x):\n    \"This is a test function, which does negative scaling.\"\n    return blackbox(x)\n\ndef foo(x):\n    y = x + 1\n    return x * y\n\n# foobar is a composition of custom function and built-in functions\n\ndef foobar(x):\n    return foo(bar(x))\n\ndef user(x):\n    if x < 50:\n        return \"The number needs to be larger.\"\n    else:\n        return \"Success.\"\n```\n\n## Optimize an LLM Agent Workflow with Trace\n\n```python\nagent = Agent(\"You are a sales assistant.\")\noptimizer = OptoPrime(agent.parameters())\n\ntry:\n    greeting = agent(\"Hola, soy Juan.\")\n    feedback = feedback_fn(greeting.data, 'es')\n    # feedback = \"Correct\" or \"Incorrect\"\nexcept ExecutionError as e:\n    greeting = e.exception_node\n    feedback = greeting.data,\n\noptimizer.zero_feedback()\noptimizer.backward(greeting, feedback)\noptimizer.step()\n```\n\n## OptoPrimeMulti.select_candidate Method\n\n```APIDOC\ndef select_candidate(self, candidates: List[Dict]) -> Dict:\n  Parameters:\n    candidates (List[Dict]): A list of candidate responses, where each candidate is a dictionary.\n  Returns:\n    Dict: The selected best candidate response from the list.\n```\n\n## Define functions for optimization example\n\n```Python\nimport opto\nfrom opto.trace import bundle, node\nfrom opto.optimizers import OptoPrime\nfrom opto.trace.nodes import GRAPH\n\ndef blackbox(x):\n    return -x * 2\n\n@bundle()\ndef bar(x):\n    \"This is a test function, which does negative scaling.\"\n    return blackbox(x)\n\ndef foo(x):\n    y = x + 1\n    return x * y\n\n# foobar is a composition of custom function and built-in functions\n\ndef foobar(x):\n    return foo(bar(x))\n\ndef user(x):\n    if x < 50:\n        return \"The number needs to be larger.\"\n    else:\n        return \"Success.\"\n```\n\n## Optimize Policy using Trace Package (Python)\n\n```python\ndef optimize_policy(\n    env_name,\n    horizon,\n    memory_size=5,\n    n_optimization_steps=100,\n    seed=0,\n    relative=True,\n    verbose=False,\n    model=\"gpt-4-0125-preview\",\n):\n\n    @trace.bundle(trainable=True)\n    def controller(obs):\n        \"\"\"\n        A feedback controller that computes the action based on the observation.\n\n        Args:\n            obs: (dict) The observation from the environment. Each key is a string (indicating a type of observation) and the value is a list of floats.\n        Output:\n            action: (list or nd.array) A 4-dimensional vector.\n        \"\"\"\n        return [0, 0, 0, 0]\n\n    config_list = config_list_from_json(\"OAI_CONFIG_LIST\")\n    config_list = [config for config in config_list if config[\"model\"] == model]\n    optimizer = OptoPrime(controller.parameters(), config_list=config_list, memory_size=memory_size)\n\n    env = TracedEnv(env_name, seed=seed, relative=relative)\n\n    successes = []\n    returns = []\n    print(\"Optimization Starts\")\n    for i in range(n_optimization_steps):\n        env.init()\n        traj, error = rollout(env, horizon, controller)\n\n        if error is None:\n            feedback = f\"Success: {traj['success'][-1]}\\nReturn: {sum(traj['reward'])}\"\n            target = traj[\"observation\"][-1][\"observation\"]\n\n            successes.append(traj[\"success\"][-1])\n            returns.append(sum(traj[\"reward\"]))\n        else:\n            feedback = error.exception_node.create_feedback()\n            target = error.exception_node\n\n        # Add instruction from the LLFbench environment, which contains\n        # information about the action space and problem background. The original instruction says\n        # obsrvaiton is a json string. But here we've parsed it as a dict so we\n        # update the instruction.\n        instruction = traj[\"observation\"][0][\"instruction\"].data\n        infix = \"You will get observations of the robot state \"\n        prefix, suffix = instruction.split(infix)\n        keys = \", \".join(traj[\"observation\"][0][\"observation\"].data.keys())\n        suffix = suffix.replace(\"json strings.\", f\"dict, where the keys are {keys}.\")\n\n        # Add an task specific explanation; as the original instruction says\n        # only it's a pick-place task, which is too vague. We clarify the task.\n        assert env_name in [\"llf-metaworld-pick-place-v2\", \"llf-metaworld-reach-v2\"]\n        if env_name == \"llf-metaworld-pick-place-v2\":\n            hint = prefix + \"The goal of the task is to pick up a puck and put it to a goal position. \" + infix + suffix\n        else:\n            hint = prefix + infix + suffix\n\n        optimizer.objective = hint + optimizer.default_objective\n\n        optimizer.zero_feedback()\n        optimizer.backward(target, feedback)\n        optimizer.step(verbose=verbose)\n\n        print(f\"Iteration: {i}, Feedback: {feedback}, Parameter: {controller.parameter.data}\")\n\n    successes.append(traj[\"success\"][-1])\n    returns.append(sum(traj[\"reward\"]))\n    print(\"Final Returns:\", sum(traj[\"reward\"]))\n    return successes, returns\n```\n\n## Visualize Optimization Success and Returns\n\n```python\n# plot successes, returns\nimport matplotlib.pyplot as plt\nplt.plot(successes)\nplt.xlabel(\"Optimization Steps\")\nplt.ylabel(\"Success\")\nplt.title(\"Successes\")\nplt.show()\n\nplt.plot(returns)\nplt.xlabel(\"Optimization Steps\")\nplt.ylabel(\"Return\")\nplt.title(\"Returns\")\nplt.show()\n```",
    "library": "requirements_tracer",
    "topic": "best-practices",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-29T22:14:31.580892"
}