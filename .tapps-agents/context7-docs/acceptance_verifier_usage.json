{
  "library": "acceptance_verifier",
  "topic": "usage",
  "documentation": {
    "content": "## ToolRubric: Tracking Tool Usage (Python)\n\n```python\nimport verifiers as vf\n\n# Define tools (type hints + docstrings omitted for brevity)\ndef calculate(expr: str) -> float: ...\ndef search_web(query: str, max_results: int = 5) -> list[dict]: ...\n\n# Initialize with tools to track\ntool_rubric = vf.ToolRubric(tools=[calculate, search_web])\n\n# Metrics exposed (names):\n# - total_tool_calls\n# - calculate_calls\n# - search_web_calls\n\n# Optional: turn counts into rewards by setting weights\n# Index 0 corresponds to total_tool_calls; subsequent indices follow the tools order\ntool_rubric.reward_weights[0] = -0.1   # penalize excessive tool calls\ntool_rubric.reward_weights[2] = 0.2    # reward using search_web specifically\n```\n\n## Bash CLI Evaluation and TUI Usage\n\n```bash\nvf-install my-environment-module # from ./environments/my_environment_module \nvf-eval my-environment-module -m gpt-5 -n 10 -r 5 -s \nvf-tui \n```\n\n## Mock OpenAI Client Usage in Tests\n\n```python\nfrom tests.mock_openai_client import MockOpenAIClient\n\ndef test_with_mock(mock_client):\n    env = vf.SingleTurnEnv(client=mock_client)\n    # Test without real API calls\n```\n\n## Define Reward for Tool Usage in Python\n\n```python\nimport verifiers as vf\nfrom openai import AsyncOpenAI\n\n# Assume dataset, search_wikipedia, calculate are defined elsewhere\n\ndef tool_usage_reward(state: vf.State) -> float:\n    \"\"\"Reward based on correct tool usage and final answer.\"\"\"\n    trajectory = state.get(\"trajectory\", [])\n    used_tools = any(\"tool_calls\" in step.get(\"completion\", [{}])[-1]\n                     for step in trajectory if step.get(\"completion\"))\n    return 1.0 if used_tools else 0.5\n\n# Create rubric with tool-specific rewards\nrubric = vf.ToolRubric(\n    funcs=[tool_usage_reward],\n    weights=[1.0]\n)\n\n# Create ToolEnv\nvf_env = vf.ToolEnv(\n    dataset=dataset,\n    tools=[search_wikipedia, calculate],\n    rubric=rubric,\n    max_turns=10,\n    system_prompt=\"Use the available tools to answer the question accurately.\"\n)\n\n# Generate rollouts\nclient = AsyncOpenAI(api_key=\"your-api-key\")\n# results = await vf_env.generate(\n#     inputs=dataset,\n#     client=client,\n#     model=\"gpt-4.1-mini\",\n#     max_concurrent=16\n# )\n```\n\n## Verifiers: Creating Tool-Using Environments\n\n```APIDOC\n## Verifiers: Creating Tool-Using Environments\n\n### Description\nCreate environments that enable LLM agents to use external tools for complex tasks. This involves defining available tools and structuring the dataset accordingly.\n\n### Method\nN/A (Python library usage)\n\n### Endpoint\nN/A (Python library usage)\n\n### Parameters\n#### Path Parameters\nNone\n\n#### Query Parameters\nNone\n\n#### Request Body\nNone\n\n### Request Example\n```python\nimport verifiers as vf\nfrom datasets import Dataset\n\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Search Wikipedia for information about a query.\"\"\"\n    # Implementation would call actual Wikipedia API\n    return f\"Wikipedia results for: {query}\"\n\ndef calculate(expression: str) -> float:\n    \"\"\"Evaluate a mathematical expression safely.\"\"\"\n    import ast\n    import operator\n\n    # Safe evaluation of arithmetic expressions\n    ops = {\n        ast.Add: operator.add,\n        ast.Sub: operator.sub,\n        ast.Mult: operator.mul,\n        ast.Div: operator.truediv,\n    }\n\n    def eval_expr(node):\n        if isinstance(node, ast.Num):\n            return node.n\n        elif isinstance(node, ast.BinOp):\n            return ops[type(node.op)](eval_expr(node.left), eval_expr(node.right))\n        raise ValueError(f\"Unsupported operation: {node}\")\n\n    return eval_expr(ast.parse(expression, mode='eval').body)\n\n# Dataset with questions requiring tool use\ndataset = Dataset.from_dict({\n    \"question\": [\n        \"What is the population of France multiplied by 2?\",\n        \"Find the GDP of Germany and add 1000 to it.\"\n    ],\n    \"answer\": [\"134000000\", \"4001000\"]  # Ground truth answers\n})\n\n# Example of creating a ToolEnv (specific class not shown in provided text but implied)\n# tool_env = vf.ToolEnv(\n#     dataset=dataset,\n#     tools=[search_wikipedia, calculate],\n#     system_prompt=\"You have access to Wikipedia search and a calculator.\",\n#     rubric=..., # Define rubric for tool use and final answer\n#     parser=...\n# )\n```\n\n### Response\n#### Success Response (200)\nN/A (This section describes environment creation, not a direct API response)\n\n#### Response Example\nN/A\n```\n\n## Install 'envs' extras for Verifiers\n\n```bash\nuv add 'verifiers[envs]'\n```\n\n## Install 'envs' extras for Verifiers\n\n```bash\nuv sync --extra envs\n```\n\n## Install Verifiers - Basic\n\n```bash\nuv add verifiers\n```\n\n## Configure Model and Sampling for Evaluation (Bash)\n\n```bash\nuv run vf-eval tool-test \\\n  -m gpt-4.1-mini \\\n  -n 20 -r 3 -t 1024 -T 0.7 \\\n  -a '{\"num_train_examples\": 1000, \"num_eval_examples\": 100}'\n```\n\n## Configure Wordle Evaluation with Model and Sampling (Bash)\n\n```bash\nuv run vf-eval wordle \\\n  -m gpt-4.1-mini \\\n  -n 20 -r 3 -t 1024 -T 0.7 \\\n  -a '{\"num_train_examples\": 2000, \"num_eval_examples\": 20}'\n```\n\n## Create Tool-Using Environment - Python\n\n```python\nimport verifiers as vf\nfrom datasets import Dataset\n\ndef search_wikipedia(query: str) -> str:\n    \"\"\"Search Wikipedia for information about a query.\"\"\"\n    # Implementation would call actual Wikipedia API\n    return f\"Wikipedia results for: {query}\"\n\ndef calculate(expression: str) -> float:\n    \"\"\"Evaluate a mathematical expression safely.\"\"\"\n    import ast\n    import operator\n\n    # Safe evaluation of arithmetic expressions\n    ops = {\n        ast.Add: operator.add,\n        ast.Sub: operator.sub,\n        ast.Mult: operator.mul,\n        ast.Div: operator.truediv,\n    }\n\n    def eval_expr(node):\n        if isinstance(node, ast.Num):\n            return node.n\n        elif isinstance(node, ast.BinOp):\n            return ops[type(node.op)](eval_expr(node.left), eval_expr(node.right))\n        raise ValueError(f\"Unsupported operation: {node}\")\n\n    return eval_expr(ast.parse(expression, mode='eval').body)\n\n# Dataset with questions requiring tool use\ndataset = Dataset.from_dict({\n    \"question\": [\n        \"What is the population of France multiplied by 2?\",\n        \"Find the GDP of Germany and add 1000 to it.\"\n    ],\n    \"answer\": [\"134000000\", \"4001000\"]  # Ground truth answers\n})\n```\n",
    "library": "acceptance_verifier",
    "topic": "usage",
    "source": "api"
  },
  "source": "api",
  "saved_at": "2026-01-30T16:10:42.036155"
}