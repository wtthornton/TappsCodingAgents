{
  "library": "long_duration_support",
  "topic": "common-mistakes",
  "documentation": {
    "content": "## External Promises for Callback Patterns in Python\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, fn\nfrom duron.contrib.storage import MemoryLogStorage\n\n@fn()\nasync def wait_for_webhook(ctx: Context) -> dict:\n    # Create external promise - returns ID for external resolution\n    promise_id, future = await ctx.create_promise(\n        dict,\n        metadata={\"type\": \"webhook_callback\"}\n    )\n\n    print(f\"Waiting for webhook with ID: {promise_id}\")\n\n    # Wait for external resolution\n    result = await future\n    return result\n\nasync def main():\n    log_storage = MemoryLogStorage()\n\n    async with wait_for_webhook.invoke(log_storage) as job:\n        await job.start()\n\n        # Simulate webhook arriving after delay\n        async def simulate_webhook():\n            await asyncio.sleep(0.5)\n            webhook_data = {\n                \"event\": \"payment.completed\",\n                \"amount\": 100,\n                \"currency\": \"USD\"\n            }\n            # Find and complete the promise (in real scenario, ID would be tracked)\n            promises = [e for e in await log_storage.entries()\n                       if e.get(\"type\") == \"promise/create\"]\n            if promises:\n                promise_id = promises[0][\"id\"]\n                await job.complete_promise(promise_id, result=webhook_data)\n\n        webhook_simulator = asyncio.create_task(simulate_webhook())\n        result = await job.wait()\n        await webhook_simulator\n        print(result)  # {\"event\": \"payment.completed\", \"amount\": 100, \"currency\": \"USD\"}\n\nasyncio.run(main())\n```\n\n## Deterministic Time and Random Values in Python\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, fn\nfrom duron.contrib.storage import FileLogStorage\n\n@fn()\nasync def deterministic_workflow(ctx: Context) -> dict:\n    # Time is captured during execution and replayed consistently\n    start_time = ctx.time()\n    start_ns = ctx.time_ns()\n\n    # Random values are deterministic based on execution path\n    random_values = [ctx.random().randint(1, 100) for _ in range(5)]\n\n    await ctx.run(lambda: asyncio.sleep(0.1))\n\n    end_time = ctx.time()\n\n    return {\n        \"start_time\": start_time,\n        \"start_ns\": start_ns,\n        \"end_time\": end_time,\n        \"random_values\": random_values,\n        \"duration\": end_time - start_time\n    }\n\nasync def main():\n    log_storage = FileLogStorage(Path(\"logs/deterministic.json\"))\n\n    # First execution\n    async with deterministic_workflow.invoke(log_storage) as job:\n        await job.start()\n        result1 = await job.wait()\n        print(\"First run:\", result1)\n\n    # Resume - will produce IDENTICAL results\n    async with deterministic_workflow.invoke(log_storage) as job:\n        await job.resume()\n        result2 = await job.wait()\n        print(\"Second run:\", result2)\n\n    # Verify determinism\n    assert result1 == result2, \"Results must be identical\"\n    print(\"\u2713 Execution is deterministic across runs\")\n\nasyncio.run(main())\n```\n\n## Signals for Interruption Handling in Python\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, Signal, SignalInterrupt, fn\nfrom duron.contrib.storage import MemoryLogStorage\n\n@fn()\nasync def interruptible_task(ctx: Context) -> str:\n    signal, writer = await ctx.create_signal(\n        str,\n        metadata={\"name\": \"cancel_signal\"}\n    )\n\n    try:\n        async with signal:\n            # Long-running work that can be interrupted\n            for i in range(100):\n                await ctx.run(lambda: asyncio.sleep(0.1))\n                print(f\"Processing step {i}\")\n    except SignalInterrupt as e:\n        # Signal was triggered - handle gracefully\n        return f\"Interrupted with reason: {e.value}\"\n\n    return \"Completed normally\"\n\nasync def main():\n    log_storage = MemoryLogStorage()\n\n    async with interruptible_task.invoke(log_storage) as job:\n        await job.start()\n\n        # Trigger signal after delay\n        async def trigger_cancel():\n            await asyncio.sleep(0.5)\n            # Send signal to all matching streams\n            await job.send_stream(\n                lambda meta: meta.get(\"name\") == \"cancel_signal\",\n                \"User requested cancellation\"\n            )\n            await job.close_stream(\n                lambda meta: meta.get(\"name\") == \"cancel_signal\"\n            )\n\n        canceller = asyncio.create_task(trigger_cancel())\n        result = await job.wait()\n        await canceller\n        print(result)  # Interrupted with reason: User requested cancellation\n\nasyncio.run(main())\n```\n\n## Metadata Tracking for Operations in Python\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, RunOptions, fn\nfrom duron.contrib.storage import MemoryLogStorage\n\n@fn()\nasync def traced_workflow(ctx: Context, user_id: int) -> dict:\n    # Add metadata to track operation types\n    user_data = await ctx.run(\n        fetch_user_data,\n        RunOptions(metadata={\n            \"type\": \"api_call\",\n            \"service\": \"user_service\",\n            \"user_id\": user_id\n        }),\n        user_id\n    )\n\n    recommendations = await ctx.run(\n        generate_recommendations,\n        RunOptions(metadata={\n            \"type\": \"ml_inference\",\n            \"model\": \"recommendation_v2\"\n        }),\n        user_data\n    )\n\n    return recommendations\n\nasync def fetch_user_data(user_id: int) -> dict:\n    await asyncio.sleep(0.1)\n    return {\"id\": user_id, \"preferences\": [\"tech\", \"science\"]}\n\nasync def generate_recommendations(user_data: dict) -> dict:\n    await asyncio.sleep(0.2)\n    return {\"items\": [101, 102, 103], \"confidence\": 0.95}\n\nasync def main():\n    log_storage = MemoryLogStorage()\n\n    async with traced_workflow.invoke(log_storage) as job:\n        await job.start(12345)\n        result = await job.wait()\n        print(result)  # {\"items\": [101, 102, 103], \"confidence\": 0.95}\n\n        # Inspect logged operations with metadata\n        for entry in await log_storage.entries():\n            if \"metadata\" in entry:\n                print(f\"Operation: {entry['type']}, Metadata: {entry['metadata']}\")\n\nasyncio.run(main())\n```\n\n## Resume Interrupted Duron Task\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, fn\nfrom duron.contrib.storage import FileLogStorage\n\n@fn()\nasync def long_running_task(ctx: Context, iterations: int) -> int:\n    total = 0\n    for i in range(iterations):\n        # Each iteration is logged - if interrupted, resumes from last completed iteration\n        result = await ctx.run(expensive_operation, i)\n        total += result\n    return total\n\nasync def expensive_operation(n: int) -> int:\n    await asyncio.sleep(0.5)  # Simulated work\n    return n * 2\n\n# First run - gets interrupted\nasync def run_with_timeout():\n    log_storage = FileLogStorage(Path(\"logs/interrupted.json\"))\n    async with long_running_task.invoke(log_storage) as job:\n        await job.start(100)\n        try:\n            result = await asyncio.wait_for(job.wait(), timeout=2.0)\n        except asyncio.TimeoutError:\n            print(\"Task interrupted after 2 seconds\")\n\n# Resume from checkpoint\nasync def resume_task():\n    log_storage = FileLogStorage(Path(\"logs/interrupted.json\"))\n    async with long_running_task.invoke(log_storage) as job:\n        await job.resume()  # Replays logged operations, continues from interruption\n        result = await job.wait()\n        print(f\"Task completed: {result}\")  # Continues where it left off\n\nasyncio.run(run_with_timeout())\nasyncio.run(resume_task())\n```\n\n## Custom Serialization with Pydantic Codec in Duron\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom typing import Any, cast\nfrom pydantic import BaseModel, TypeAdapter\nfrom duron import Context, fn\nfrom duron.codec import Codec, JSONValue\nfrom duron.contrib.storage import FileLogStorage\nfrom duron.typing import TypeHint\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n\nclass PydanticCodec(Codec):\n    def encode_json(self, result: object) -> JSONValue:\n        return cast(JSONValue, TypeAdapter(type(result)).dump_python(\n            result, mode=\"json\", exclude_none=True\n        ))\n\n    def decode_json(self, encoded: JSONValue, expected_type: TypeHint[Any]) -> object:\n        return TypeAdapter(expected_type).validate_python(encoded)\n\n@fn(codec=PydanticCodec())\nasync def process_user(ctx: Context, user_id: int) -> User:\n    # Pydantic models are automatically serialized/deserialized\n    user = await ctx.run(fetch_user, user_id)\n    user.name = user.name.upper()\n    return user\n\nasync def fetch_user(user_id: int) -> User:\n    return User(id=user_id, name=\"Alice\", email=\"alice@example.com\")\n\nasync def main():\n    log_storage = FileLogStorage(Path(\"logs/users.json\"))\n    async with process_user.invoke(log_storage) as job:\n        await job.start(42)\n        user = await job.wait()\n        print(f\"{user.name} <{user.email}>\")  # ALICE <alice@example.com>\n\nasyncio.run(main())\n\n```\n\n## External Streams for Real-time Data in Duron\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, fn\nfrom duron.contrib.storage import MemoryLogStorage\n\n@fn()\nasync def consume_external_data(ctx: Context) -> int:\n    # Create stream that external code can write to\n    stream, writer = await ctx.create_stream(\n        int,\n        external=True,\n        metadata={\"name\": \"sensor_data\"}\n    )\n\n    total = 0\n    async for value in stream:\n        total += value\n        if value == -1:  # Termination signal\n            break\n    return total\n\nasync def main():\n    log_storage = MemoryLogStorage()\n\n    async with consume_external_data.invoke(log_storage) as job:\n        await job.start()\n\n        # External process feeds data into the stream\n        async def feed_data():\n            await asyncio.sleep(0.1)  # Wait for stream creation\n            for i in [10, 20, 30, 40, 50, -1]:\n                # Send data to streams matching predicate\n                await job.send_stream(\n                    lambda meta: meta.get(\"name\") == \"sensor_data\",\n                    i\n                )\n                await asyncio.sleep(0.05)\n\n        feeder = asyncio.create_task(feed_data())\n        result = await job.wait()\n        await feeder\n        print(result)  # 149 (10+20+30+40+50-1)\n\nasyncio.run(main())\n\n```\n\n## Resumable Streaming with Checkpoints in Duron\n\n```python\nimport asyncio\nfrom collections.abc import AsyncGenerator\nfrom pathlib import Path\nfrom duron import Context, RunOptions, fn, op\nfrom duron.contrib.storage import FileLogStorage\n\n@fn()\nasync def generate_and_sum(ctx: Context, target: int) -> str:\n    @op(\n        checkpoint=True,\n        action_type=str,\n        initial=lambda: \"\",\n        reducer=lambda state, chunk: state + chunk\n    )\n    async def generate_chunks(state: str) -> AsyncGenerator[str, str]:\n        # state is restored from checkpoint on resume\n        while len(state) < target:\n            chunk = f\"[{len(state)} ]\"\n            state = yield chunk  # Checkpoint after each yield\n\n    final_state = await ctx.run(generate_chunks, RunOptions(\n        metadata={\"type\": \"chunk_generator\"}\n    ))\n    return final_state\n\nasync def main():\n    log_storage = FileLogStorage(Path(\"logs/streaming.json\"))\n\n    # First run - completes successfully\n    async with generate_and_sum.invoke(log_storage) as job:\n        await job.start(25)\n        result = await job.wait()\n        print(result)  # [0][3][6][9][12][15][18][21][24]\n\n    # Resume will replay checkpoints instantly\n    async with generate_and_sum.invoke(log_storage) as job:\n        await job.resume()\n        result = await job.wait()\n        print(result)  # Same result, replayed from log\n\nasyncio.run(main())\n\n```\n\n## Declare Durable Async Function with Duron\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, fn\nfrom duron.contrib.storage import FileLogStorage\n\n@fn()\nasync def fetch_and_process(ctx: Context, url: str) -> dict:\n    # All operations run through ctx are logged and replayable\n    response = await ctx.run(fetch_data, url)\n    processed = await ctx.run(process_data, response)\n    return processed\n\nasync def fetch_data(url: str) -> str:\n    await asyncio.sleep(1)  # Simulated network delay\n    return f\"data from {url}\"\n\nasync def process_data(data: str) -> dict:\n    return {\"result\": data.upper(), \"length\": len(data)}\n\n# Execute with persistent storage\nasync def main():\n    log_storage = FileLogStorage(Path(\"logs/session1.json\"))\n    async with fetch_and_process.invoke(log_storage) as job:\n        await job.start(\"https://example.com\")\n        result = await job.wait()\n        print(result)  # {\"result\": \"DATA FROM HTTPS://EXAMPLE.COM\", \"length\": 24}\n\nasyncio.run(main())\n```\n\n## Watch Duron Streams for Output Monitoring in Python\n\n```python\nimport asyncio\nfrom pathlib import Path\nfrom duron import Context, fn\nfrom duron.contrib.storage import MemoryLogStorage\n\n@fn()\nasync def emit_progress(ctx: Context) -> int:\n    # Create internal stream for progress reporting\n    stream, writer = await ctx.create_stream(\n        dict,\n        metadata={\"name\": \"progress\"}\n    )\n\n    total = 0\n    for i in range(10):\n        await asyncio.sleep(0.1)\n        total += i\n        await writer.send({\"step\": i, \"total\": total})\n\n    await writer.close()\n    return total\n\nasync def main():\n    log_storage = MemoryLogStorage()\n\n    async with emit_progress.invoke(log_storage) as job:\n        # Watch stream before starting job\n        progress_stream = job.watch_stream(\n            lambda meta: meta.get(\"name\") == \"progress\"\n        )\n\n        await job.start()\n\n        # Monitor progress from external task\n        async def monitor():\n            async for event in progress_stream:\n                print(f\"Progress: step {event['step']}, total {event['total']}\")\n\n        monitor_task = asyncio.create_task(monitor())\n        result = await job.wait()\n        await monitor_task\n        print(f\"Final result: {result}\")  # Final result: 45\n\nasyncio.run(main())\n```",
    "library": "task_duration",
    "topic": "common-mistakes",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:32:05.494841"
}