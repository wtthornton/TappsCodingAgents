{
  "library": "llm_communicator",
  "topic": "best-practices",
  "documentation": {
    "content": "## Define a Stateful Toolbox Class for Key-Value Memory\n\n```Python\nimport llm\n\nclass Memory(llm.Toolbox):\n    _memory = None\n\n    def _get_memory(self):\n        if self._memory is None:\n            self._memory = {}\n        return self._memory\n\n    def set(self, key: str, value: str):\n        \"Set something as a key\"\n        self._get_memory()[key] = value\n\n    def get(self, key: str):\n        \"Get something from a key\"\n        return self._get_memory().get(key) or \"\"\n\n    def append(self, key: str, value: str):\n        \"Append something as a key\"\n        memory = self._get_memory()\n        memory[key] = (memory.get(key) or \"\") + \"\\n\" + value\n\n    def keys(self):\n        \"Return a list of keys\"\n        return list(self._get_memory().keys())\n```\n\n## llm Default Model Configuration API Methods\n\n```APIDOC\nllm.get_default_model() -> Model\nllm.set_default_model(alias: str) -> None\nllm.get_default_embedding_model(alias: str) -> EmbeddingModel\nllm.set_default_embedding_model() -> None\n```\n\n## Group Plugin Tools with llm.Toolbox Classes\n\n```APIDOC\nllm.Toolbox: A class for grouping related plugin-provided tools that share state or configuration.\n```\n\n## Display detailed options for LLM models using `llm models --options`\n\n```bash\nllm models --options\n```\n\n## Display detailed options for LLM models using `llm models --options`\n\n```APIDOC\nOpenAI Chat: gpt-4o (aliases: 4o)\n  Options:\n    temperature: float\n      What sampling temperature to use, between 0 and 2. Higher values like\n      0.8 will make the output more random, while lower values like 0.2 will\n      make it more focused and deterministic.\n    max_tokens: int\n      Maximum number of tokens to generate.\n    top_p: float\n      An alternative to sampling with temperature, called nucleus sampling,\n      where the model considers the results of the tokens with top_p\n      probability mass. So 0.1 means only the tokens comprising the top 10%\n      probability mass are considered. Recommended to use top_p or\n      temperature but not both.\n    frequency_penalty: float\n      Number between -2.0 and 2.0. Positive values penalize new tokens based\n      on their existing frequency in the text so far, decreasing the model's\n      likelihood to repeat the same line verbatim.\n    presence_penalty: float\n      Number between -2.0 and 2.0. Positive values penalize new tokens based\n      on whether they appear in the text so far, increasing the model's\n      likelihood to talk about new topics.\n    stop: str\n      A string where the API will stop generating further tokens.\n    logit_bias: dict, str\n      Modify the likelihood of specified tokens appearing in the completion.\n      Pass a JSON string like '{\\\"1712\\\":-100, \\\"892\\\":-100, \\\"1489\\\":-100}'\n    seed: int\n      Integer seed to attempt to sample deterministically\n    json_object: boolean\n      Output a valid JSON object {...}. Prompt must mention JSON.\n  Attachment types:\n    application/pdf, image/gif, image/jpeg, image/png, image/webp\n  Features:\n  - streaming\n  - schemas\n  - tools\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\nOpenAI Chat: chatgpt-4o-latest (aliases: chatgpt-4o)\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n    presence_penalty: float\n    stop: str\n    logit_bias: dict, str\n    seed: int\n    json_object: boolean\n  Attachment types:\n    application/pdf, image/gif, image/jpeg, image/png, image/webp\n  Features:\n  - streaming\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\nOpenAI Chat: gpt-4o-mini (aliases: 4o-mini)\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n    presence_penalty: float\n    stop: str\n    logit_bias: dict, str\n    seed: int\n    json_object: boolean\n  Attachment types:\n    application/pdf, image/gif, image/jpeg, image/png, image/webp\n  Features:\n  - streaming\n  - schemas\n  - tools\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\nOpenAI Chat: gpt-4o-audio-preview\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n    presence_penalty: float\n    stop: str\n    logit_bias: dict, str\n    seed: int\n    json_object: boolean\n  Attachment types:\n    audio/mpeg, audio/wav\n  Features:\n  - streaming\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\nOpenAI Chat: gpt-4o-audio-preview-2024-12-17\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n    presence_penalty: float\n    stop: str\n    logit_bias: dict, str\n    seed: int\n    json_object: boolean\n  Attachment types:\n    audio/mpeg, audio/wav\n  Features:\n  - streaming\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\nOpenAI Chat: gpt-4o-audio-preview-2024-10-01\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n    presence_penalty: float\n    stop: str\n    logit_bias: dict, str\n    seed: int\n    json_object: boolean\n  Attachment types:\n    audio/mpeg, audio/wav\n  Features:\n  - streaming\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\nOpenAI Chat: gpt-4o-mini-audio-preview\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n```\n\n## Chaining Prompts in LLM Conversations with Tools\n\n```python\nprint(conversation.chain(\n    \"Convert panda to uppercase and reverse it\"\n).text())\nprint(conversation.chain(\n    \"Same with pangolin\"\n).text())\n```\n\n## Approve Tool Calls Interactively with LLM\n\n```bash\nllm --functions '\ndef multiply(x: int, y: int) -> int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return x * y\n' 'what is 34234 * 213345' --ta\n```\n\n## OpenAI Chat Model: o3-mini API Reference\n\n```APIDOC\nOpenAI Chat: o3-mini\n  Options:\n    temperature: float\n    max_tokens: int\n    top_p: float\n    frequency_penalty: float\n    presence_penalty: float\n    stop: str\n    logit_bias: dict, str\n    seed: int\n    json_object: boolean\n    reasoning_effort: str\n  Features:\n  - streaming\n  - schemas\n  - tools\n  - async\n  Keys:\n    key: openai\n    env_var: OPENAI_API_KEY\n```\n\n## Search Stored Fragments using llm\n\n```bash\nllm fragments -q pytest -q asyncio\n```\n\n## List all available LLM models using `llm models`\n\n```bash\nllm models\n```\n\n## Interactive Session with LLM Memory Tool\n\n```bash\nChatting with gpt-4.1-mini\nType 'exit' or 'quit' to exit\nType '!multi' to enter multiple lines, then '!end' to finish\nType '!edit' to open your default editor and modify the prompt\nType '!fragment <my_fragment> [<another_fragment> ...]' to insert one or more fragments\n> Remember my name is Henry\n\nTool call: Memory_set({'key': 'user_name', 'value': 'Henry'})\n  null\n\nGot it, Henry! I'll remember your name. How can I assist you today?\n> what keys are there?\n\nTool call: Memory_keys({})\n  [\n    \"user_name\"\n  ]\n\nCurrently, there is one key stored: \"user_name\". Would you like to add or retrieve any information?\n> read it\n\nTool call: Memory_get({'key': 'user_name'})\n  Henry\n\nThe value stored under the key \"user_name\" is Henry. Is there anything else you'd like to do?\n> add Barrett to it\n\nTool call: Memory_append({'key': 'user_name', 'value': 'Barrett'})\n  null\n\nI have added \"Barrett\" to the key \"user_name\". If you want, I can now show you the updated value.\n> show value\n\nTool call: Memory_get({'key': 'user_name'})\n  Henry\n  Barrett\n\nThe value stored under the key \"user_name\" is now:\nHenry\nBarrett\n\nIs there anything else you would like to do?\n```\n\nContext7 cache for llm_communicator: cached at 2026-01-31T02:32:04.496542+00:00Z; consider refreshing if docs changed.",
    "library": "llm_communicator",
    "topic": "best-practices",
    "source": "cache"
  },
  "source": "cache",
  "saved_at": "2026-01-30T18:32:04.578481"
}