{
  "library": "external_feedback_models",
  "topic": "examples",
  "documentation": {
    "content": "## Initialize Baseline Transformer NLP Model\n\n```python\nhparams = {\n    'tokenizer': 'EleutherAI/gpt-neox-20b',\n    'embedding_dim': 384,\n    'num_transformer_blocks': 6,\n    'multiheaded_attention_heads': 6,\n    'weight_initialization_method': 'xavier',\n    'weight_initialization_gain': 1.0,\n    'execution_mode': 'pretrain'\n}\n\nmodel = Baseline_Transformer_NLP(hparams)\n\n# Forward pass returns log probabilities\ninput_ids = torch.randint(0, 50277, (2, 256))  # batch_size=2, seq_len=256\nlog_probs = model(input_ids, learning=True)\n\n# log_probs: [B*S, V] log probabilities over vocabulary\nprint(f\"Output shape: {log_probs.shape}\")  # torch.Size([512, 50277])\n```\n\n## Train EBT for Video Prediction on Kinetics-400\n\n```shell\npython train_model.py \\\n  --run_name ebt-vid-k400 \\\n  --modality \"VID\" \\\n  --model_name \"ebt_vid\" \\\n  --model_size \"s\" \\\n  --dataset_name \"k400\" \\\n  --image_dims 128 128 \\\n  --num_frames 16 \\\n  --backbone_type \"vae\" \\\n  --mcmc_step_size 1.0 \\\n  --mcmc_num_steps 3 \\\n  --batch_size_per_device 16 \\\n  --peak_learning_rate 5e-5 \\\n  --max_steps 1000000 \\\n  --gpus \"-1\" \\\n  --num_workers 8 \\\n  --wandb_project \"vid_prediction\"\n```\n\n## Load and Pretokenize RedPajama Dataset\n\n```python\nfrom data.nlp.pajama_dataloader import RedPajamaDataset\nfrom transformers import AutoTokenizer\n\n# Initialize dataset with pretokenization\nhparams = {\n    'execution_mode': 'pretrain',\n    'context_length': 256,\n    'dataset_dir': '',  # Uses HF_HOME env variable\n    'tokenizer': 'EleutherAI/gpt-neox-20b',\n    'pretokenize_dataset': True,\n    'dataset_name': 'pajama',\n    'num_workers': 12,\n    'num_gpus': 4,\n    'batch_size_per_device': 32\n}\n\ndataset = RedPajamaDataset(hparams)\n\n# Access tokenized samples\nsample = dataset[0]\nprint(f\"Input IDs shape: {sample['input_ids'].shape}\")\nprint(f\"Attention mask shape: {sample['attention_mask'].shape}\")\nprint(f\"Dataset length: {len(dataset)}\")\n\n# Dataset automatically caches preprocessed version to disk\n# Location: $HF_HOME/pajama_preprocessed/EleutherAI_gpt-neox-20b/max_length_257/\n```\n\n## Run Inference on Language Understanding Benchmarks\n\n```bash\npython train_model.py \\\n  --run_name ebt-xxs-inference-lambada \\\n  --modality \"NLP\" \\\n  --model_name \"ebt\" \\\n  --model_size \"xxs\" \\\n  --tokenizer \"EleutherAI/gpt-neox-20b\" \\\n  --normalize_initial_condition \\\n  --ebt_type \"time_embed\" \\\n  --denoising_initial_condition \"random_noise\" \\\n  --context_length 256 \\\n  --gpus \"-1\" \\\n  --batch_size_per_device 8 \\\n  --dataset_name \"lambada\" \\\n  --num_workers 12 \\\n  --wandb_project \"nlp_inference_accuracy\" \\\n  --execution_mode \"inference\" \\\n  --infer_ebt_advanced \\\n  --infer_langevin_dynamics_noise 1.0 \\\n  --infer_ebt_num_steps 2 \\\n  --only_test \\\n  --only_test_model_ckpt \"checkpoints/ebt-xxs-epoch=10.ckpt\" \\\n  --infer_max_gen_len 128 \\\n  --infer_topp 0.9 \\\n  --infer_temp 0.8 \\\n  --set_matmul_precision \"medium\"\n```\n\n## Unzip SSv2 Labels Archive\n\n```bash\nunzip 20bn-something-something-download-package-labels.zip\n```\n\n## Run Minimalistic NLP Training Loop\n\n```python\npython example_code/minimal_nlp_training_loop.py\n```\n\n## Install FFmpeg via APT (Linux)\n\n```bash\napt install ffmpeg\n```\n\n## Learning Rate Scheduling with Warmup and Cosine Annealing\n\n```python\nfrom optimization import WarmUpCosineAnnealingLR, exclude_bias_and_norm\nimport torch.optim as optim\nimport torch\n\n# Separate parameters for weight decay\nparam_groups = exclude_bias_and_norm(\n    model,\n    weight_decay=0.01,\n    skip_list=['alpha', 'langevin_dynamics_noise_std']  # Don't decay special params\n)\n\noptimizer = optim.AdamW(param_groups, lr=0.0012, betas=(0.9, 0.95))\n\n# Warmup + cosine annealing scheduler\nscheduler = WarmUpCosineAnnealingLR(\n    optimizer,\n    warm_up_steps=10000,\n    max_steps=1000000,\n    min_lr_scale=10,  # min_lr = peak_lr / 10\n    max_scheduling_steps=1000000\n)\n\n# Training loop\nfor step in range(1000000):\n    loss = model.training_step(batch, step)\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n\n    if step % 100 == 0:\n        print(f\"Step {step}, LR: {scheduler.get_last_lr()[0]:.6f}, Loss: {loss.item():.4f}\")\n```\n\n## Train Baseline Transformer Model (Bash)\n\n```bash\npython train_model.py \\\n  --run_name baseline-xxs-bs_256 \\\n  --modality \"NLP\" \\\n  --model_name \"baseline_transformer\" \\\n  --model_size \"xxs\" \\\n  --pretokenize_dataset \\\n  --tokenizer \"EleutherAI/gpt-neox-20b\" \\\n  --context_length 256 \\\n  --gpus \"-1\" \\\n  --peak_learning_rate 0.0012 \\\n  --batch_size_per_device 32 \\\n  --accumulate_grad_batches 2 \\\n  --gradient_clip_val 1.0 \\\n  --weight_decay 0.01 \\\n  --max_steps 1000000 \\\n  --warm_up_steps 10000 \\\n  --dataset_name \"pajama\" \\\n  --num_workers 12 \\\n  --wandb_project \"nlp_pretrain\"\n```\n\n## Configure Model Size Presets\n\n```python\nfrom model.model_utils import model_sizes\n\n# Available model sizes with automatic parameter configuration\n# model_sizes = {\n#     \"xxs\": {'num_transformer_blocks': 6, 'multiheaded_attention_heads': 6, 'embedding_dim': 384},\n#     \"xs\": {'num_transformer_blocks': 12, 'multiheaded_attention_heads': 12, 'embedding_dim': 768},\n#     \"s\": {'num_transformer_blocks': 24, 'multiheaded_attention_heads': 16, 'embedding_dim': 1024},\n#     \"m\": {'num_transformer_blocks': 32, 'multiheaded_attention_heads': 32, 'embedding_dim': 2048},\n#     \"l\": {'num_transformer_blocks': 48, 'multiheaded_attention_heads': 48, 'embedding_dim': 4096}\n# }\n\n# Use in training script\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--model_size', type=str, default='xxs')\nargs = parser.parse_args(['--model_size', 's'])\n\nif args.model_size:\n    args.num_transformer_blocks = model_sizes[args.model_size]['num_transformer_blocks']\n    args.multiheaded_attention_heads = model_sizes[args.model_size]['multiheaded_attention_heads']\n    args.embedding_dim = model_sizes[args.model_size]['embedding_dim']\n    print(f\"Model: {args.num_transformer_blocks} layers, {args.embedding_dim} dim\")\n    # Output: Model: 24 layers, 1024 dim\n```",
    "library": "external_feedback_models",
    "topic": "usage",
    "source": "fuzzy_match"
  },
  "source": "fuzzy_match",
  "saved_at": "2026-01-30T18:28:14.151961"
}